"""
ETL process for JobInsight data

Lu·ªìng ETL:
1. D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c load t·ª´ raw_jobs v√†o staging_jobs b·∫±ng SQL
2. Stored procedures x·ª≠ l√Ω c∆° b·∫£n salary, deadline ƒë√£ ƒë∆∞·ª£c th·ª±c thi
3. Script n√†y x·ª≠ l√Ω th√™m c√°c tr∆∞·ªùng ph·ª©c t·∫°p b·∫±ng pandas v√† c·∫≠p nh·∫≠t l·∫°i staging_jobs

- Input: staging_jobs table (ƒë√£ c√≥ d·ªØ li·ªáu c∆° b·∫£n v√† m·ªôt s·ªë tr∆∞·ªùng ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·∫±ng SQL)
- Output: staging_jobs table (c·∫≠p nh·∫≠t th√™m c√°c tr∆∞·ªùng ƒë√£ x·ª≠ l√Ω b·∫±ng pandas)
"""

import os
import json
import logging
import sys
import time
from datetime import datetime
from contextlib import contextmanager
import pandas as pd
import numpy as np
import re
from typing import List, Dict, Any, Optional, Tuple
from bs4 import BeautifulSoup

# Import for performance monitoring
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    logger.warning("psutil not available - performance monitoring will be limited")

# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† logging
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))

# ƒê·∫£m b·∫£o th∆∞ m·ª•c logs t·ªìn t·∫°i
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "processing.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Import modules t·ª´ utils
from src.utils.config import DB_CONFIG, RAW_JOBS_TABLE, DWH_STAGING_SCHEMA, STAGING_JOBS_TABLE, RAW_BATCH_SIZE
from src.utils.db import get_connection, execute_query, table_exists, get_dataframe, execute_stored_procedure, execute_sql_file
from src.processing.data_processing import clean_title, clean_company_name, extract_location_info, refine_location

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c SQL
SQL_DIR = os.path.join(PROJECT_ROOT, "sql")
if not os.path.exists(SQL_DIR):
    SQL_DIR = os.path.join(os.getcwd(), "sql")
    os.makedirs(SQL_DIR, exist_ok=True)

@contextmanager
def performance_monitor(phase_name):
    """
    Context manager ƒë·ªÉ monitor performance cho t·ª´ng phase

    Args:
        phase_name (str): T√™n phase ƒë·ªÉ tracking
    """
    # Start metrics
    start_time = time.time()
    start_memory = 0
    start_cpu = 0

    if PSUTIL_AVAILABLE:
        try:
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            start_cpu = process.cpu_percent()
        except:
            pass

    try:
        yield
    finally:
        # End metrics
        duration = time.time() - start_time
        end_memory = start_memory
        end_cpu = start_cpu

        if PSUTIL_AVAILABLE:
            try:
                process = psutil.Process()
                end_memory = process.memory_info().rss / 1024 / 1024  # MB
                end_cpu = process.cpu_percent()
            except:
                pass

        # Log performance metrics
        logger.info(f"üìä {phase_name} Performance:")
        logger.info(f"  ‚è±Ô∏è  Duration: {duration*1000:.1f}ms")
        if PSUTIL_AVAILABLE and end_memory > 0:
            logger.info(f"  üß† Memory: {end_memory:.1f}MB (Œî{end_memory-start_memory:+.1f}MB)")
            logger.info(f"  ‚ö° CPU: {end_cpu:.1f}%")
        else:
            logger.info(f"  üìà Resource monitoring not available")

def setup_database_schema():
    """Thi·∫øt l·∫≠p schema v√† b·∫£ng"""
    try:
        # Ki·ªÉm tra v√† t·∫°o schema n·∫øu ch∆∞a t·ªìn t·∫°i
        with get_connection() as conn:
            with conn.cursor() as cursor:
                # Ki·ªÉm tra schema ƒë√£ t·ªìn t·∫°i ch∆∞a
                cursor.execute("""
                SELECT EXISTS(
                    SELECT 1 FROM information_schema.schemata 
                    WHERE schema_name = %s
                );
                """, (DWH_STAGING_SCHEMA,))
                schema_exists = cursor.fetchone()[0]
                
                if not schema_exists:
                    logger.info(f"Schema {DWH_STAGING_SCHEMA} ch∆∞a t·ªìn t·∫°i, ƒëang t·∫°o m·ªõi...")
                    cursor.execute(f"CREATE SCHEMA IF NOT EXISTS {DWH_STAGING_SCHEMA};")
                    conn.commit()
                    logger.info(f"ƒê√£ t·∫°o schema {DWH_STAGING_SCHEMA}")
                else:
                    logger.info(f"Schema {DWH_STAGING_SCHEMA} ƒë√£ t·ªìn t·∫°i")

        # Th·ª±c thi file schema_staging.sql ƒë·ªÉ thi·∫øt l·∫≠p b·∫£ng v√† ch·ªâ m·ª•c
        schema_staging = os.path.join(SQL_DIR, "schema_staging.sql")
        if os.path.exists(schema_staging):
            if not execute_sql_file(schema_staging):
                logger.error("Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema v√† b·∫£ng!")
                return False
        else:
            logger.error(f"Kh√¥ng t√¨m th·∫•y file schema: {schema_staging}")
            return False
        
        logger.info("ƒê√£ thi·∫øt l·∫≠p schema v√† b·∫£ng database th√†nh c√¥ng!")
        
        # Th·ª±c thi file insert_raw_to_staging.sql ƒë·ªÉ ch√®n d·ªØ li·ªáu t·ª´ raw_jobs v√†o staging_jobs
        insert_staging = os.path.join(SQL_DIR, "insert_raw_to_staging.sql")
        if os.path.exists(insert_staging):
            logger.info("ƒêang ch√®n d·ªØ li·ªáu t·ª´ raw_jobs v√†o staging_jobs...")
            if not execute_sql_file(insert_staging):
                logger.error("Kh√¥ng th·ªÉ ch√®n d·ªØ li·ªáu t·ª´ raw_jobs v√†o staging_jobs!")
                return False
            logger.info("ƒê√£ ch√®n d·ªØ li·ªáu t·ª´ raw_jobs v√†o staging_jobs th√†nh c√¥ng!")
        else:
            logger.error(f"Kh√¥ng t√¨m th·∫•y file insert: {insert_staging}")
            return False
        
        # ƒê·∫øm s·ªë b·∫£n ghi ƒë√£ ch√®n
        with get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(f"SELECT COUNT(*) FROM {DWH_STAGING_SCHEMA}.staging_jobs")
                record_count = cursor.fetchone()[0]
                logger.info(f"T√¨m th·∫•y {record_count} b·∫£n ghi trong b·∫£ng staging_jobs")
                
                # Ki·ªÉm tra s·ªë l∆∞·ª£ng b·∫£n ghi trong raw_jobs
                cursor.execute(f"SELECT COUNT(*) FROM public.raw_jobs")
                raw_count = cursor.fetchone()[0]
                logger.info(f"S·ªë b·∫£n ghi trong raw_jobs: {raw_count}")
                
                # Th·ªëng k√™ d·ªØ li·ªáu
                cursor.execute(f"SELECT COUNT(DISTINCT company_name) FROM {DWH_STAGING_SCHEMA}.staging_jobs")
                company_count = cursor.fetchone()[0]
                
                cursor.execute(f"SELECT COUNT(DISTINCT location) FROM {DWH_STAGING_SCHEMA}.staging_jobs")
                location_count = cursor.fetchone()[0]
                
                cursor.execute(f"SELECT COUNT(*), (COUNT(*) * 100.0 / {record_count}) FROM {DWH_STAGING_SCHEMA}.staging_jobs WHERE verified_employer = TRUE")
                verified_count, verified_percent = cursor.fetchone()
                
                logger.info("Th·ªëng k√™ d·ªØ li·ªáu:")
                logger.info(f"- S·ªë c√¥ng ty: {company_count}")
                logger.info(f"- S·ªë ƒë·ªãa ƒëi·ªÉm: {location_count}")
                logger.info(f"- S·ªë nh√† tuy·ªÉn d·ª•ng ƒë√£ x√°c th·ª±c: {verified_count} ({verified_percent:.1f}%)")
        
        return True
    except Exception as e:
        logger.error(f"L·ªói khi thi·∫øt l·∫≠p schema database: {str(e)}")
        return False

def run_stored_procedures():
    """Th·ª±c thi stored procedures - OPTIMIZED VERSION"""
    try:
        logger.info("ƒêang th·ª±c thi stored procedures...")

        with get_connection() as conn:
            cursor = conn.cursor()

            # Ki·ªÉm tra function ƒë√£ t·ªìn t·∫°i ch∆∞a ƒë·ªÉ tr√°nh re-create
            cursor.execute("""
                SELECT EXISTS(
                    SELECT 1 FROM pg_proc p
                    JOIN pg_namespace n ON p.pronamespace = n.oid
                    WHERE n.nspname = 'jobinsight_staging'
                    AND p.proname = 'normalize_salary'
                );
            """)
            function_exists = cursor.fetchone()[0]

            if not function_exists:
                logger.info("T·∫°o stored procedures l·∫ßn ƒë·∫ßu...")
                stored_procs_file = os.path.join(SQL_DIR, "stored_procedures.sql")
                if os.path.exists(stored_procs_file):
                    if not execute_sql_file(stored_procs_file):
                        logger.error("Kh√¥ng th·ªÉ t·∫°o stored procedures!")
                        return False
                else:
                    logger.error(f"Kh√¥ng t√¨m th·∫•y file stored procedures: {stored_procs_file}")
                    return False
            else:
                logger.info("Stored procedures ƒë√£ t·ªìn t·∫°i, b·ªè qua t·∫°o m·ªõi")

            # Ch·∫°y tr·ª±c ti·∫øp SQL thay v√¨ stored procedure ƒë·ªÉ tƒÉng performance
            logger.info("C·∫≠p nh·∫≠t th·ªùi gian c√≤n l·∫°i...")
            cursor.execute("""
                UPDATE jobinsight_staging.staging_jobs
                SET time_remaining = CASE
                    WHEN due_date > CURRENT_TIMESTAMP THEN
                        'C√≤n ' || EXTRACT(day FROM (due_date - CURRENT_TIMESTAMP))::int || ' ng√†y ƒë·ªÉ ·ª©ng tuy·ªÉn'
                    WHEN due_date > CURRENT_TIMESTAMP - INTERVAL '1 day' AND due_date <= CURRENT_TIMESTAMP THEN
                        'ƒê√£ h·∫øt th·ªùi gian ·ª©ng tuy·ªÉn'
                    ELSE 'ƒê√£ h·∫øt th·ªùi gian ·ª©ng tuy·ªÉn'
                END
                WHERE time_remaining IS NULL OR time_remaining = '';
            """)

            updated_rows = cursor.rowcount
            conn.commit()
            logger.info(f"ƒê√£ c·∫≠p nh·∫≠t th√†nh c√¥ng th·ªùi gian c√≤n l·∫°i cho {updated_rows} b·∫£n ghi")

            return True

    except Exception as e:
        logger.error(f"L·ªói khi th·ª±c thi stored procedures: {e}")
        return False

def load_staging_data(limit=None, offset=0, query_filter=None):
    """
    Load d·ªØ li·ªáu t·ª´ b·∫£ng staging_jobs v√†o DataFrame
    
    Args:
        limit: Gi·ªõi h·∫°n s·ªë b·∫£n ghi c·∫ßn l·∫•y
        offset: V·ªã tr√≠ b·∫Øt ƒë·∫ßu
        query_filter: ƒêi·ªÅu ki·ªán WHERE ƒë·ªÉ l·ªçc d·ªØ li·ªáu
        
    Returns:
        DataFrame ch·ª©a d·ªØ li·ªáu t·ª´ staging_jobs
    """
    try:
        # S·ª≠a l·∫°i t√™n b·∫£ng ƒë·ªÉ tr√°nh l·∫∑p l·∫°i schema
        table_name = f"{STAGING_JOBS_TABLE}" 
        logger.info(f"ƒêang t·∫£i d·ªØ li·ªáu t·ª´ b·∫£ng {table_name}...")
        
        # X√¢y d·ª±ng query
        query = f"SELECT * FROM {table_name}"
        
        # Th√™m ƒëi·ªÅu ki·ªán l·ªçc n·∫øu c√≥
        if query_filter:
            query += f" {query_filter}"
            
        # Th√™m limit v√† offset
        if limit is not None:
            query += f" LIMIT {limit}"
        if offset > 0:
            query += f" OFFSET {offset}"
        
        # L·∫•y d·ªØ li·ªáu
        df = get_dataframe(query)
        logger.info(f"ƒê√£ t·∫£i th√†nh c√¥ng {len(df)} b·∫£n ghi t·ª´ b·∫£ng {table_name}")
        return df
    except Exception as e:
        logger.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu t·ª´ {table_name}: {e}")
        raise

def save_back_to_staging(df):
    """
    L∆∞u DataFrame ƒë√£ x·ª≠ l√Ω tr·ªü l·∫°i b·∫£ng staging_jobs - OPTIMIZED VERSION

    Args:
        df: DataFrame ƒë√£ x·ª≠ l√Ω

    Returns:
        bool: K·∫øt qu·∫£ th·ª±c hi·ªán
    """
    try:
        table_name = f"{DWH_STAGING_SCHEMA}.staging_jobs"
        logger.info(f"ƒêang l∆∞u {len(df)} b·∫£n ghi v√†o b·∫£ng {table_name}...")

        # T·∫°o m·ªôt b·∫£n sao ƒë·ªÉ tr√°nh s·ª≠a ƒë·ªïi df g·ªëc
        df_to_save = df.copy()

        # X·ª≠ l√Ω c√°c c·ªôt ki·ªÉu JSONB m·ªôt c√°ch hi·ªáu qu·∫£ h∆°n
        json_columns = ['skills', 'location_pairs', 'raw_data']
        for col in json_columns:
            if col in df_to_save.columns:
                # Vectorized JSON processing thay v√¨ apply()
                df_to_save[col] = df_to_save[col].map(
                    lambda x: json.dumps(x) if isinstance(x, (list, dict)) else
                             (None if pd.isna(x) else str(x))
                )

        # S·ª≠ d·ª•ng bulk operations thay v√¨ temporary table
        from sqlalchemy import create_engine, text
        engine = create_engine(f"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}")

        with engine.begin() as conn:
            # S·ª≠ d·ª•ng pandas to_sql v·ªõi method='multi' cho performance t·ªët h∆°n
            temp_table = f"temp_staging_{int(datetime.now().timestamp())}"

            # T·∫°o temp table v·ªõi c√πng structure
            conn.execute(text(f"CREATE TEMP TABLE {temp_table} (LIKE {table_name})"))

            # Bulk insert v√†o temp table
            df_to_save.to_sql(
                temp_table,
                conn,
                if_exists='append',
                index=False,
                method='multi',  # Faster bulk insert
                chunksize=1000   # Process in chunks
            )

            # Single efficient upsert query
            update_columns = [c for c in df_to_save.columns if c != 'job_id']
            update_stmt = ", ".join([f"{col} = EXCLUDED.{col}" for col in update_columns])

            upsert_query = f"""
            INSERT INTO {table_name}
            SELECT * FROM {temp_table}
            ON CONFLICT (job_id)
            DO UPDATE SET {update_stmt}
            """

            result = conn.execute(text(upsert_query))
            logger.info(f"Upsert completed: {result.rowcount} rows affected")

        logger.info(f"ƒê√£ c·∫≠p nh·∫≠t th√†nh c√¥ng {len(df)} b·∫£n ghi v√†o b·∫£ng {table_name}")
        return True

    except Exception as e:
        logger.error(f"L·ªói khi l∆∞u d·ªØ li·ªáu v√†o b·∫£ng {table_name}: {e}")
        return False

# H√†m x·ª≠ l√Ω d·ªØ li·ªáu
def process_staging_data(df):
    """
    X·ª≠ l√Ω t·∫•t c·∫£ c√°c tr∆∞·ªùng d·ªØ li·ªáu c·∫ßn thi·∫øt b·∫±ng pandas
    
    Args:
        df: DataFrame t·ª´ staging_jobs
        
    Returns:
        DataFrame ƒë√£ x·ª≠ l√Ω
    """
    # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh ·∫£nh h∆∞·ªüng ƒë·∫øn d·ªØ li·ªáu g·ªëc
    processed_df = df.copy()
    
    # 1. X·ª≠ l√Ω location
    logger.info("ƒêang x·ª≠ l√Ω location...")
    if 'location_detail' in processed_df.columns:
        processed_df['location_pairs'] = processed_df['location_detail'].apply(extract_location_info)
    
    if 'location' in processed_df.columns and 'location_pairs' in processed_df.columns:
        processed_df['location'] = processed_df.apply(refine_location, axis=1)
    
    # 2. X·ª≠ l√Ω title
    logger.info("ƒêang x·ª≠ l√Ω title...")
    if 'title' in processed_df.columns:
        processed_df['title_clean'] = processed_df['title'].apply(clean_title)
    
    # 3. X·ª≠ l√Ω company_name
    logger.info("ƒêang x·ª≠ l√Ω company_name...")
    if 'company_name' in processed_df.columns:
        processed_df['company_name_standardized'] = processed_df['company_name'].apply(clean_company_name)
    
    logger.info(f"ƒê√£ ho√†n th√†nh x·ª≠ l√Ω chi ti·∫øt cho {len(processed_df)} b·∫£n ghi")
    return processed_df

def verify_etl_integrity(source_count, target_count, threshold=0.98):
    """
    Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu sau ETL.
    
    Args:
        source_count (int): S·ªë b·∫£n ghi ngu·ªìn
        target_count (int): S·ªë b·∫£n ghi ƒë√≠ch
        threshold (float): Ng∆∞·ª°ng ch·∫•p nh·∫≠n (% d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng)
        
    Returns:
        bool: True n·∫øu t·ª∑ l·ªá d·ªØ li·ªáu chuy·ªÉn ƒë·ªïi ƒë·∫°t threshold
    """
    if source_count == 0:
        logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ngu·ªìn ƒë·ªÉ x·ª≠ l√Ω")
        return True
        
    success_rate = target_count / source_count
    logger.info(f"T·ª∑ l·ªá d·ªØ li·ªáu x·ª≠ l√Ω th√†nh c√¥ng: {success_rate:.2%} ({target_count}/{source_count})")
    
    if success_rate < threshold:
        logger.error(f"T·ª∑ l·ªá d·ªØ li·ªáu x·ª≠ l√Ω ({success_rate:.2%}) th·∫•p h∆°n ng∆∞·ª°ng {threshold:.2%}")
        return False
        
    return True

def run_etl(batch_size=None, only_unprocessed=False, verbose=False):
    """
    Th·ª±c hi·ªán quy tr√¨nh ETL ho√†n ch·ªânh
    
    Args:
        batch_size (int, optional): S·ªë b·∫£n ghi x·ª≠ l√Ω m·ªói batch, m·∫∑c ƒë·ªãnh l√† None (x·ª≠ l√Ω t·∫•t c·∫£)
        only_unprocessed (bool, optional): Ch·ªâ x·ª≠ l√Ω c√°c b·∫£n ghi ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω, m·∫∑c ƒë·ªãnh l√† False
        verbose (bool, optional): Hi·ªÉn th·ªã th√™m th√¥ng tin chi ti·∫øt, m·∫∑c ƒë·ªãnh l√† False
    
    Returns:
        dict: K·∫øt qu·∫£ th·ª±c hi·ªán v·ªõi c√°c th√¥ng tin chi ti·∫øt
    """
    try:
        # Thi·∫øt l·∫≠p logging level n·∫øu verbose
        if verbose:
            logger.setLevel(logging.DEBUG)
            for handler in logger.handlers:
                handler.setLevel(logging.DEBUG)
        
        logger.info(f"B·∫Øt ƒë·∫ßu ETL t·ª´ raw_jobs sang staging_jobs (batch_size={batch_size}, only_unprocessed={only_unprocessed})...")
        start_time = datetime.now()

        # 1. Thi·∫øt l·∫≠p schema v√† b·∫£ng n·∫øu c·∫ßn
        with performance_monitor("Schema Setup"):
            if not setup_database_schema():
                logger.error("Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema v√† b·∫£ng!")
                return {"success": False, "error": "Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema v√† b·∫£ng!"}

        # 2. Ch·∫°y stored procedures ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu c∆° b·∫£n
        with performance_monitor("Stored Procedures"):
            if not run_stored_procedures():
                logger.warning("C√≥ l·ªói khi th·ª±c thi stored procedures!")
                # V·∫´n ti·∫øp t·ª•c v√¨ c√≥ th·ªÉ m·ªôt s·ªë SP ƒë√£ ch·∫°y th√†nh c√¥ng
        
        # 3. Load d·ªØ li·ªáu t·ª´ staging ƒë·ªÉ x·ª≠ l√Ω th√™m b·∫±ng pandas
        try:
            # N·∫øu only_unprocessed l√† True, ch·ªâ l·∫•y c√°c b·∫£n ghi ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω
            query_filter = None
            if only_unprocessed:
                query_filter = "WHERE processed IS NULL OR processed = FALSE"

            with performance_monitor("Data Loading"):
                staging_df = load_staging_data(limit=batch_size, query_filter=query_filter)
                source_count = len(staging_df)

            if source_count == 0:
                logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu trong b·∫£ng staging_jobs ƒë·ªÉ x·ª≠ l√Ω!")
                return {
                    "success": True,
                    "message": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω",
                    "stats": {
                        "total_records": 0,
                        "processed_records": 0,
                        "success_count": 0,
                        "failure_count": 0,
                        "success_rate": 100.0,
                        "duration_seconds": (datetime.now() - start_time).total_seconds(),
                        "batch_count": 0
                    }
                }

            # 4. X·ª≠ l√Ω chi ti·∫øt b·∫±ng pandas
            with performance_monitor("Data Processing"):
                processed_df = process_staging_data(staging_df)
                processed_count = len(processed_df)

                # Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu sau b∆∞·ªõc x·ª≠ l√Ω
                if not verify_etl_integrity(source_count, processed_count):
                    logger.warning("Ph√°t hi·ªán m·∫•t m√°t d·ªØ li·ªáu trong qu√° tr√¨nh x·ª≠ l√Ω!")
                    # V·∫´n ti·∫øp t·ª•c nh∆∞ng ƒë√£ c·∫£nh b√°o

            # 5. L∆∞u k·∫øt qu·∫£ tr·ªü l·∫°i b·∫£ng staging
            with performance_monitor("Data Saving"):
                if not save_back_to_staging(processed_df):
                    logger.error("Kh√¥ng th·ªÉ l∆∞u k·∫øt qu·∫£ v√†o b·∫£ng staging!")
                    return {
                        "success": False,
                        "error": "Kh√¥ng th·ªÉ l∆∞u k·∫øt qu·∫£ v√†o b·∫£ng staging",
                        "stats": {
                            "total_records": source_count,
                            "processed_records": processed_count,
                            "duration_seconds": (datetime.now() - start_time).total_seconds()
                        }
                    }
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu staging: {e}")
            return {
                "success": False,
                "error": f"L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu staging: {str(e)}"
            }
        
        # T√≠nh th·ªùi gian ch·∫°y
        duration = (datetime.now() - start_time).total_seconds()
        
        # Th·ªëng k√™ ETL
        etl_stats = {
            "total_records": source_count,
            "processed_records": processed_count,
            "success_count": processed_count,  # Gi·∫£ s·ª≠ t·∫•t c·∫£ ƒë·ªÅu th√†nh c√¥ng
            "failure_count": source_count - processed_count,
            "success_rate": processed_count / max(1, source_count) * 100,
            "duration_seconds": duration,
            "batch_count": 1  # M·∫∑c ƒë·ªãnh l√† 1 batch
        }
        
        logger.info(f"Quy tr√¨nh ETL ƒë√£ ho√†n th√†nh th√†nh c√¥ng trong {duration:.2f} gi√¢y!")
        logger.info(f"Th·ªëng k√™ ETL: {json.dumps(etl_stats)}")
        
        return {
            "success": True,
            "message": "ETL raw_to_staging ho√†n th√†nh th√†nh c√¥ng",
            "stats": etl_stats
        }
    except Exception as e:
        logger.error(f"L·ªói trong quy tr√¨nh ETL: {e}")
        return {
            "success": False,
            "error": f"L·ªói trong quy tr√¨nh ETL: {str(e)}"
        }

# H√†m main
if __name__ == "__main__":
    try:
        import argparse
        
        # T·∫°o parser
        parser = argparse.ArgumentParser(description="Job Data ETL Process")
        parser.add_argument("--limit", type=int, help="Gi·ªõi h·∫°n s·ªë b·∫£n ghi x·ª≠ l√Ω")
        parser.add_argument("--verbose", "-v", action="store_true", help="Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt")
        
        args = parser.parse_args()
        
        # Thi·∫øt l·∫≠p logging level
        if args.verbose:
            logger.setLevel(logging.DEBUG)
            for handler in logger.handlers:
                handler.setLevel(logging.DEBUG)
        
        # Th·ª±c thi ETL
        success = run_etl()
        
        if success:
            print("‚úÖ Quy tr√¨nh ETL ƒë√£ ho√†n th√†nh th√†nh c√¥ng!")
            sys.exit(0)
        else:
            print("‚ùå Quy tr√¨nh ETL th·∫•t b·∫°i!")
            sys.exit(1)
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
        sys.exit(1)

