#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Module x·ª≠ l√Ω fact tables v√† bridge tables
"""
import pandas as pd
import logging
import duckdb
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any

# Thi·∫øt l·∫≠p logging
logger = logging.getLogger(__name__)

from .etl_utils import batch_insert_records, lookup_dimension_key, lookup_location_key
from src.common.decorators import retry

try:
    from src.processing.data_prepare import (
        generate_daily_fact_records, calculate_load_month, parse_job_location
    )
except ImportError:
    # Fallback cho tr∆∞·ªùng h·ª£p ch·∫°y tr·ª±c ti·∫øp script n√†y
    from processing.data_prepare import (
        generate_daily_fact_records, calculate_load_month, parse_job_location
    )

class FactHandler:
    """
    Class x·ª≠ l√Ω fact tables v√† bridge tables
    """
    
    def __init__(self, duck_conn: duckdb.DuckDBPyConnection):
        """
        Kh·ªüi t·∫°o FactHandler
        
        Args:
            duck_conn: K·∫øt n·ªëi DuckDB
        """
        self.duck_conn = duck_conn
    
    @retry(max_tries=3, delay_seconds=2, backoff_factor=2, exceptions=[Exception])
    def generate_fact_records(self, staging_records: pd.DataFrame) -> Tuple[List[Dict], List[Dict]]:
        """
        T·∫°o fact records v√† bridge records t·ª´ staging data
        
        Args:
            staging_records: DataFrame ch·ª©a c√°c b·∫£n ghi t·ª´ staging
            
        Returns:
            Tuple[List[Dict], List[Dict]]: (fact_records, bridge_records)
        """
        fact_records = []
        bridge_records = []
        
        # L·ªçc c√°c b·∫£n ghi c√≥ ƒë·ªß th√¥ng tin
        if staging_records.empty:
            logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu staging ƒë·ªÉ x·ª≠ l√Ω!")
            return [], []
            
        # L·∫•y th·ªùi gian hi·ªán t·∫°i cho crawled_at v√† load_month
        crawled_at = datetime.now()
        load_month = crawled_at.strftime('%Y-%m')
        
        # Danh s√°ch c√°c ng√†y c·∫ßn t·∫°o fact record (h√¥m nay v√† v√†i ng√†y t·ªõi)
        # Do jobs th∆∞·ªùng hi·ªÉn th·ªã trong nhi·ªÅu ng√†y
        today = datetime.now().date()
        dates_to_create = [today + timedelta(days=i) for i in range(5)]  # H√¥m nay v√† 4 ng√†y ti·∫øp theo
        
        # T·∫°o m·ªôt b·∫£n sao ƒë·ªÉ tr√°nh SettingWithCopyWarning
        staging_df = staging_records.copy()
        
        # ƒê·∫£m b·∫£o c√°c c·ªôt c·∫ßn thi·∫øt t·ªìn t·∫°i
        for col in ['job_id', 'title_clean', 'company_name_standardized', 'due_date']:
            if col not in staging_df.columns:
                logger.error(f"C·ªôt {col} kh√¥ng t·ªìn t·∫°i trong d·ªØ li·ªáu staging!")
                return [], []
                
        # X·ª≠ l√Ω t·ª´ng b·∫£n ghi staging
        job_skipped = 0
        bridge_skipped = 0
        
        # L·∫•y to√†n b·ªô DimJob current
        dim_jobs = self.duck_conn.execute("""
            SELECT job_id, job_sk FROM DimJob WHERE is_current = TRUE
        """).fetchdf()
        job_id_to_sk = dict(zip(dim_jobs['job_id'], dim_jobs['job_sk']))
        
        # L·∫•y to√†n b·ªô DimCompany current
        dim_companies = self.duck_conn.execute("""
            SELECT company_name_standardized, company_sk FROM DimCompany WHERE is_current = TRUE
        """).fetchdf()
        company_to_sk = dict(zip(dim_companies['company_name_standardized'], dim_companies['company_sk']))
        
        # L·∫•y to√†n b·ªô DimLocation current ƒë·ªÉ cache
        dim_locations = self.duck_conn.execute("""
            SELECT location_sk, province, city, district 
            FROM DimLocation 
            WHERE is_current = TRUE
        """).fetchdf()
        
        # T·∫°o cache cho location lookups
        location_cache = {}
        for _, loc in dim_locations.iterrows():
            # T·∫°o key t·ª´ province, city, district
            cache_key = f"{loc['province'] or 'None'}:{loc['city']}:{loc['district'] or 'None'}"
            location_cache[cache_key] = loc['location_sk']
        
        # OPTIMIZED: Vectorized operations instead of iterrows()
        # Map surrogate keys using vectorized operations
        staging_df['job_sk'] = staging_df['job_id'].astype(str).map(job_id_to_sk)
        staging_df['company_sk'] = staging_df['company_name_standardized'].map(company_to_sk)

        # Filter out records without valid surrogate keys
        valid_records = staging_df.dropna(subset=['job_sk', 'company_sk'])
        job_skipped = len(staging_df) - len(valid_records)

        if job_skipped > 0:
            logger.warning(f"B·ªè qua {job_skipped} records do thi·∫øu job_sk ho·∫∑c company_sk")

        # Process valid records in batches
        for _, job in valid_records.iterrows():
                
            # Parse posted_time v√† due_date
            posted_time = None
            if hasattr(job, 'posted_time') and pd.notna(job.posted_time):
                try:
                    posted_time = pd.to_datetime(job.posted_time)
                except:
                    pass
                    
            due_date = None
            if hasattr(job, 'due_date') and pd.notna(job.due_date):
                try:
                    due_date = pd.to_datetime(job.due_date)
                except:
                    pass
                    
            # OPTIMIZED: Batch process fact records for all dates
            job_sk = int(job['job_sk'])
            company_sk = int(job['company_sk'])

            # Parse dates once
            posted_time = None
            if pd.notna(job.get('posted_time')):
                try:
                    posted_time = pd.to_datetime(job['posted_time'])
                except:
                    pass

            due_date = None
            if pd.notna(job.get('due_date')):
                try:
                    due_date = pd.to_datetime(job['due_date'])
                except:
                    pass

            # Batch create fact records for all dates
            batch_fact_records = self._batch_create_fact_records(
                job, job_sk, company_sk, dates_to_create,
                due_date, posted_time, crawled_at, load_month
            )

            # Process successful fact records
            for fact_record in batch_fact_records:
                fact_records.append(fact_record)

                # Process location bridges for this fact
                bridges = self._process_location_bridges(job, fact_record['fact_id'], location_cache)
                if bridges:
                    bridge_records.extend(bridges)
                else:
                    bridge_skipped += 1
                
        # Enhanced summary logging
        logger.info(f"üìä FACT GENERATION SUMMARY:")
        logger.info(f"  - Input staging records: {len(staging_df)}")
        logger.info(f"  - Valid records (with job_sk & company_sk): {len(valid_records)}")
        logger.info(f"  - Expected fact records (valid √ó 5 dates): {len(valid_records) * 5}")
        logger.info(f"  - Actually created fact records: {len(fact_records)}")
        logger.info(f"  - Created bridge records: {len(bridge_records)}")
        logger.info(f"  - Skipped jobs: {job_skipped}")
        logger.info(f"  - Skipped bridges: {bridge_skipped}")

        # Calculate success rate
        expected_facts = len(valid_records) * 5
        success_rate = (len(fact_records) / expected_facts * 100) if expected_facts > 0 else 0
        logger.info(f"  - Success rate: {success_rate:.1f}% ({len(fact_records)}/{expected_facts})")

        if len(fact_records) == 0 and expected_facts > 0:
            logger.warning("üö® ZERO FACT RECORDS CREATED - This indicates a critical issue!")
        elif success_rate < 95.0:  # Only warn if success rate < 95%
            logger.warning(f"‚ö†Ô∏è LOW SUCCESS RATE - Created {len(fact_records)}/{expected_facts} expected records ({success_rate:.1f}%)")
        elif len(fact_records) < expected_facts:
            logger.info(f"‚ÑπÔ∏è MINOR GAPS - Created {len(fact_records)}/{expected_facts} expected records ({success_rate:.1f}%) - within acceptable range")
        
        return fact_records, bridge_records

    def _batch_create_fact_records(
        self,
        job: pd.Series,
        job_sk: int,
        company_sk: int,
        dates_to_create: List[datetime],
        due_date: Optional[datetime],
        posted_time: Optional[datetime],
        crawled_at: datetime,
        load_month: str
    ) -> List[Dict]:
        """
        Batch create fact records for multiple dates - OPTIMIZED

        Returns:
            List of successfully created fact records
        """
        batch_records = []

        try:
            # Prepare batch data
            batch_data = []
            for date_id in dates_to_create:
                batch_data.append((
                    job_sk, company_sk, date_id,
                    job.get('salary_min') if pd.notna(job.get('salary_min')) else None,
                    job.get('salary_max') if pd.notna(job.get('salary_max')) else None,
                    job.get('salary_type') if pd.notna(job.get('salary_type')) else None,
                    due_date,
                    job.get('time_remaining') if pd.notna(job.get('time_remaining')) else None,
                    job.get('verified_employer', False),
                    posted_time,
                    crawled_at,
                    load_month
                ))

            # Batch UPSERT with RETURNING fact_id - FIXED VERSION (no load_month update)
            upsert_query = """
                INSERT INTO FactJobPostingDaily (
                    job_sk, company_sk, date_id,
                    salary_min, salary_max, salary_type,
                    due_date, time_remaining, verified_employer,
                    posted_time, crawled_at, load_month
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT (job_sk, date_id)
                DO UPDATE SET
                    salary_min = EXCLUDED.salary_min,
                    salary_max = EXCLUDED.salary_max,
                    salary_type = EXCLUDED.salary_type,
                    due_date = EXCLUDED.due_date,
                    time_remaining = EXCLUDED.time_remaining,
                    verified_employer = EXCLUDED.verified_employer,
                    posted_time = EXCLUDED.posted_time,
                    crawled_at = EXCLUDED.crawled_at
                RETURNING fact_id, job_sk, company_sk, date_id
            """

            # REMOVED: Excessive debug logging - functionality is stable

            # Execute batch upsert with detailed logging
            for i, data in enumerate(batch_data):
                try:
                    # Check if record already exists before UPSERT
                    check_existing = self.duck_conn.execute(
                        "SELECT fact_id FROM FactJobPostingDaily WHERE job_sk = ? AND date_id = ?",
                        [data[0], data[2]]  # job_sk, date_id
                    ).fetchone()

                    existing_fact_id = check_existing[0] if check_existing else None

                    # Execute UPSERT with enhanced error handling
                    try:
                        result = self.duck_conn.execute(upsert_query, data).fetchone()
                    except Exception as upsert_error:
                        logger.error(f"üö® UPSERT execution failed for job_sk={data[0]}, date={dates_to_create[i]}: {upsert_error}")
                        logger.error(f"   Query: {upsert_query}")
                        logger.error(f"   Data: {data}")
                        continue

                    # REMOVED: Excessive UPSERT logging - only log failures
                    if not result:  # Only log failures for debugging
                        if existing_fact_id:
                            logger.debug(f"üîÑ UPSERT UPDATE failed: job_sk={data[0]}, date={dates_to_create[i]}")
                        else:
                            logger.debug(f"üÜï UPSERT INSERT failed: job_sk={data[0]}, date={dates_to_create[i]}")

                    # Handle result - try to get fact_id even if RETURNING is empty
                    if result:
                        # RETURNING worked - use returned fact_id
                        fact_id = result[0]
                        logger.info(f"‚úÖ RETURNING success: fact_id={fact_id}")
                    elif existing_fact_id:
                        # RETURNING failed but record existed - use existing fact_id
                        fact_id = existing_fact_id
                        logger.info(f"üîÑ Using existing fact_id: {fact_id}")
                    else:
                        # Neither RETURNING nor existing - query for new fact_id
                        new_check = self.duck_conn.execute(
                            "SELECT fact_id FROM FactJobPostingDaily WHERE job_sk = ? AND date_id = ?",
                            [data[0], data[2]]
                        ).fetchone()
                        if new_check:
                            fact_id = new_check[0]
                            logger.info(f"üîç Found new fact_id: {fact_id}")
                        else:
                            # ENHANCED: Better error handling for missing fact_id
                            logger.warning(f"‚ùå No fact_id found after UPSERT: job_sk={data[0]}, date={dates_to_create[i]}")
                            logger.warning(f"   UPSERT data: {data}")
                            logger.warning(f"   This indicates a potential constraint violation or data issue")
                            continue

                    # Add to batch_records
                    batch_records.append({
                        'fact_id': fact_id,
                        'job_sk': data[0],
                        'company_sk': data[1],
                        'date_id': dates_to_create[i]
                    })

                except Exception as e:
                    logger.warning(f"Failed to upsert fact record for job_sk={job_sk}, date={dates_to_create[i]}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Batch create fact records failed for job_sk={job_sk}: {e}")

        return batch_records

    @retry(max_tries=3, delay_seconds=2, backoff_factor=2, exceptions=[Exception])
    def _process_single_fact_record(
        self,
        job: pd.Series,
        job_sk: int,
        company_sk: int,
        date_id: datetime,
        due_date: Optional[datetime],
        posted_time: Optional[datetime],
        crawled_at: datetime,
        load_month: str
    ) -> Optional[int]:
        """
        X·ª≠ l√Ω m·ªôt fact record duy nh·∫•t cho m·ªôt job v√† m·ªôt ng√†y
        
        Args:
            job: B·∫£n ghi job t·ª´ staging
            job_sk: Surrogate key c·ªßa job
            company_sk: Surrogate key c·ªßa company
            date_id: Ng√†y c·∫ßn t·∫°o fact record
            due_date: Ng√†y h·∫øt h·∫°n job
            posted_time: Th·ªùi ƒëi·ªÉm ƒëƒÉng job
            crawled_at: Th·ªùi ƒëi·ªÉm crawl d·ªØ li·ªáu
            load_month: Th√°ng load d·ªØ li·ªáu (YYYY-MM)
            
        Returns:
            fact_id n·∫øu th√†nh c√¥ng, None n·∫øu th·∫•t b·∫°i
        """
        try:
            # Ki·ªÉm tra xem ƒë√£ c√≥ fact record cho job_sk v√† date_id n√†y ch∆∞a
            check_query = """
                SELECT fact_id FROM FactJobPostingDaily
                WHERE job_sk = ? AND date_id = ?
            """
            
            existing = self.duck_conn.execute(check_query, [job_sk, date_id]).fetchone()
            
            if existing:
                # ƒê√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t th√¥ng tin
                fact_id = existing[0]
                
                # S·ª≠ d·ª•ng transaction ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n khi update
                try:
                    # B·∫Øt ƒë·∫ßu transaction
                    self.duck_conn.execute("BEGIN TRANSACTION")
                    
                    # Ki·ªÉm tra l·∫°i xem fact_id c√≥ t·ªìn t·∫°i kh√¥ng
                    check_fact_id = self.duck_conn.execute(
                        "SELECT 1 FROM FactJobPostingDaily WHERE fact_id = ?", 
                        [fact_id]
                    ).fetchone()
                    
                    if not check_fact_id:
                        # fact_id kh√¥ng t·ªìn t·∫°i, c√≥ th·ªÉ ƒë√£ b·ªã x√≥a
                        logger.warning(f"fact_id {fact_id} kh√¥ng t·ªìn t·∫°i trong b·∫£ng FactJobPostingDaily, th·ª≠ t·∫°o m·ªõi...")
                        self.duck_conn.execute("ROLLBACK")
                        
                        # Chuy·ªÉn sang t·∫°o m·ªõi v·ªõi fact_id m·ªõi
                        return self._create_new_fact_record(job, job_sk, company_sk, date_id, due_date, posted_time, crawled_at, load_month)
                    
                    # C·∫≠p nh·∫≠t c√°c th√¥ng tin c√≥ th·ªÉ thay ƒë·ªïi
                    update_query = """
                        UPDATE FactJobPostingDaily
                        SET 
                            company_sk = ?,
                            salary_min = ?,
                            salary_max = ?,
                            salary_type = ?,
                            due_date = ?,
                            time_remaining = ?,
                            crawled_at = ?,
                            load_month = ?
                        WHERE fact_id = ?
                    """
                    
                    update_values = [
                        company_sk,
                        job.salary_min if pd.notna(job.salary_min) else None,
                        job.salary_max if pd.notna(job.salary_max) else None,
                        job.salary_type if pd.notna(job.salary_type) else None,
                        due_date,
                        job.time_remaining if pd.notna(job.time_remaining) else None,
                        crawled_at,
                        load_month,
                        fact_id
                    ]
                    
                    try:
                        self.duck_conn.execute(update_query, update_values)
                        logger.debug(f"Updated existing fact record: job_sk={job_sk}, date_id={date_id}, fact_id={fact_id}")
                        self.duck_conn.execute("COMMIT")
                        return fact_id  # Tr·∫£ v·ªÅ fact_id sau khi update th√†nh c√¥ng
                    except Exception as e:
                        self.duck_conn.execute("ROLLBACK")
                        logger.error(f"L·ªói khi update fact record cho job_id={job.job_id}, date={date_id}: {e}")
                        
                        # Th·ª≠ t·∫°o m·ªõi v·ªõi fact_id m·ªõi
                        logger.info(f"Th·ª≠ t·∫°o m·ªõi fact record cho job_id={job.job_id}, date={date_id} v·ªõi fact_id m·ªõi...")
                        return self._create_new_fact_record(job, job_sk, company_sk, date_id, due_date, posted_time, crawled_at, load_month)
                        
                except Exception as e:
                    try:
                        self.duck_conn.execute("ROLLBACK")
                    except:
                        pass
                    logger.error(f"L·ªói transaction khi update fact record cho job_id={job.job_id}, date={date_id}: {e}")
                    return None
            else:
                # Ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi
                return self._create_new_fact_record(job, job_sk, company_sk, date_id, due_date, posted_time, crawled_at, load_month)
            
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω fact record cho job_id={job.job_id}, date={date_id}: {e}")
            return None
    
    @retry(max_tries=3, delay_seconds=2, backoff_factor=2, exceptions=[Exception])
    def _process_location_bridges(self, job: pd.Series, fact_id: int, location_cache: Dict) -> List[Dict]:
        """
        X·ª≠ l√Ω location bridges cho m·ªôt fact record
        
        Args:
            job: B·∫£n ghi job t·ª´ staging
            fact_id: ID c·ªßa fact record
            location_cache: Cache cho location lookups
            
        Returns:
            List c√°c bridge records
        """
        bridge_records = []
        
        try:
            # X√≥a bridge records c≈© cho fact_id n√†y
            self.duck_conn.execute("DELETE FROM FactJobLocationBridge WHERE fact_id = ?", [fact_id])
            
            # L·∫•y location string
            location_str = None
            
            # ∆Øu ti√™n s·ª≠ d·ª•ng location_pairs n·∫øu c√≥
            if hasattr(job, 'location_pairs'):
                try:
                    location_pairs_value = getattr(job, 'location_pairs')
                    if location_pairs_value is not None and str(location_pairs_value).lower() not in ['nan', 'none', '']:
                        location_str = str(location_pairs_value)
                except:
                    pass
                    
            # Fallback v·ªÅ location n·∫øu kh√¥ng c√≥ location_pairs
            if not location_str and hasattr(job, 'location'):
                try:
                    location_value = getattr(job, 'location')
                    if location_value is not None and str(location_value).lower() not in ['nan', 'none', '']:
                        location_str = str(location_value)
                except:
                    pass
            
            if location_str:
                # Parse location string th√†nh c√°c tuple (province, city, district)
                parsed_locations = parse_job_location(location_str)
                
                location_sks_added = set()  # Tr√°nh duplicate locations cho c√πng 1 fact_id
                
                for province, city, district in parsed_locations:
                    # S·ª≠ d·ª•ng cache thay v√¨ truy v·∫•n database
                    cache_key = f"{province or 'None'}:{city}:{district or 'None'}"
                    location_sk = location_cache.get(cache_key)
                    
                    if location_sk and location_sk not in location_sks_added:
                        bridge_records.append({'fact_id': fact_id, 'location_sk': location_sk})
                        location_sks_added.add(location_sk)
                    elif not location_sk:
                        # N·∫øu kh√¥ng t√¨m th·∫•y trong cache, th·ª≠ t√¨m Unknown
                        unknown_key = f"None:Unknown:None"
                        unknown_location_sk = location_cache.get(unknown_key)
                        
                        # N·∫øu kh√¥ng c√≥ trong cache, truy v·∫•n database
                        if not unknown_location_sk:
                            unknown_location_sk = lookup_location_key(self.duck_conn, None, 'Unknown', None)
                            # C·∫≠p nh·∫≠t cache
                            if unknown_location_sk:
                                location_cache[unknown_key] = unknown_location_sk
                        
                        if unknown_location_sk and unknown_location_sk not in location_sks_added:
                            bridge_records.append({'fact_id': fact_id, 'location_sk': unknown_location_sk})
                            location_sks_added.add(unknown_location_sk)
            else:
                # Kh√¥ng c√≥ location, s·ª≠ d·ª•ng Unknown
                unknown_key = f"None:Unknown:None"
                unknown_location_sk = location_cache.get(unknown_key)
                
                # N·∫øu kh√¥ng c√≥ trong cache, truy v·∫•n database
                if not unknown_location_sk:
                    unknown_location_sk = lookup_location_key(self.duck_conn, None, 'Unknown', None)
                    # C·∫≠p nh·∫≠t cache
                    if unknown_location_sk:
                        location_cache[unknown_key] = unknown_location_sk
                
                if unknown_location_sk:
                    bridge_records.append({'fact_id': fact_id, 'location_sk': unknown_location_sk})
            
            # Batch insert bridge records
            if bridge_records:
                batch_insert_records(
                    self.duck_conn, 
                    'FactJobLocationBridge', 
                    bridge_records, 
                    on_conflict="ON CONFLICT (fact_id, location_sk) DO NOTHING"
                )
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω location bridges cho fact_id={fact_id}: {e}")
        
        return bridge_records
    
    def cleanup_duplicate_fact_records(self):
        """
        D·ªçn d·∫πp c√°c fact records b·ªã tr√πng l·∫∑p (job_sk + date_id)
        
        Returns:
            Dict th√¥ng tin k·∫øt qu·∫£
        """
        logger.info("B·∫Øt ƒë·∫ßu d·ªçn d·∫πp duplicate fact records...")
        
        try:
            # T·∫°o transaction ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
            self.duck_conn.execute("BEGIN TRANSACTION")
            
            try:
                # FIXED: DuckDB doesn't support PRAGMA foreign_keys
                # Skip foreign key disabling as DuckDB handles constraints differently
                logger.debug("DuckDB kh√¥ng c·∫ßn t·∫Øt foreign keys - b·ªè qua b∆∞·ªõc n√†y")
                
                # T·∫°o b·∫£ng backup
                logger.info("T·∫°o b·∫£ng backup...")
                self.duck_conn.execute("DROP TABLE IF EXISTS temp_fact_backup")
                self.duck_conn.execute("CREATE TEMP TABLE temp_fact_backup AS SELECT * FROM FactJobPostingDaily")
                
                # T√¨m c√°c b·∫£n ghi tr√πng l·∫∑p v√† l∆∞u v√†o b·∫£ng t·∫°m ƒë·ªÉ tr√°nh c√°c v·∫•n ƒë·ªÅ v·ªõi cursor
                logger.info("T√¨m duplicate fact records...")
                self.duck_conn.execute("""
                    DROP TABLE IF EXISTS temp_duplicates;
                    CREATE TEMP TABLE temp_duplicates AS
                    WITH duplicates AS (
                        SELECT job_sk, date_id, COUNT(*) as count, MIN(fact_id) as min_fact_id
                        FROM FactJobPostingDaily
                        GROUP BY job_sk, date_id
                        HAVING COUNT(*) > 1
                    )
                    SELECT f.fact_id, f.job_sk, f.date_id, d.min_fact_id
                    FROM FactJobPostingDaily f
                    JOIN duplicates d ON f.job_sk = d.job_sk AND f.date_id = d.date_id
                    WHERE f.fact_id != d.min_fact_id
                    ORDER BY f.job_sk, f.date_id
                """)
                
                # ƒê·∫øm s·ªë l∆∞·ª£ng b·∫£n ghi tr√πng l·∫∑p
                duplicate_count = self.duck_conn.execute("SELECT COUNT(*) FROM temp_duplicates").fetchone()[0]
                
                if duplicate_count > 0:
                    logger.warning(f"T√¨m th·∫•y {duplicate_count} duplicate fact records")
                    
                    # T·∫°o b·∫£ng backup cho bridge
                    self.duck_conn.execute("DROP TABLE IF EXISTS temp_bridge_backup")
                    self.duck_conn.execute("CREATE TEMP TABLE temp_bridge_backup AS SELECT * FROM FactJobLocationBridge")
                    
                    # C·∫≠p nh·∫≠t c√°c bridge records ƒë·ªÉ tr·ªè ƒë·∫øn fact_id m·ªõi
                    logger.info("C·∫≠p nh·∫≠t bridge records...")
                    self.duck_conn.execute("""
                        -- Chuy·ªÉn c√°c bridge records t·ª´ fact_id c≈© sang fact_id m·ªõi
                        INSERT INTO FactJobLocationBridge (fact_id, location_sk)
                        SELECT DISTINCT d.min_fact_id, b.location_sk
                        FROM FactJobLocationBridge b
                        JOIN temp_duplicates d ON b.fact_id = d.fact_id
                        WHERE NOT EXISTS (
                            SELECT 1 FROM FactJobLocationBridge b2
                            WHERE b2.fact_id = d.min_fact_id AND b2.location_sk = b.location_sk
                        );
                        
                        -- X√≥a c√°c bridge records tr·ªè ƒë·∫øn fact_id c≈©
                        DELETE FROM FactJobLocationBridge
                        WHERE fact_id IN (SELECT fact_id FROM temp_duplicates);
                    """)
                    
                    # X√≥a c√°c fact records tr√πng l·∫∑p
                    logger.info("X√≥a duplicate fact records...")
                    self.duck_conn.execute("""
                        DELETE FROM FactJobPostingDaily
                        WHERE fact_id IN (SELECT fact_id FROM temp_duplicates)
                    """)
                    
                else:
                    logger.info("Kh√¥ng t√¨m th·∫•y duplicate fact records")
                
                # Ki·ªÉm tra k·∫øt qu·∫£
                check_after = self.duck_conn.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM FactJobPostingDaily) as total_facts,
                        (SELECT COUNT(*) FROM (SELECT DISTINCT job_sk, date_id FROM FactJobPostingDaily)) as unique_combinations
                """).fetchone()
                
                # X√≥a c√°c b·∫£ng t·∫°m
                self.duck_conn.execute("DROP TABLE IF EXISTS temp_duplicates")
                
                # B·∫≠t l·∫°i foreign key constraints
                try:
                    self.duck_conn.execute("PRAGMA foreign_keys = ON")
                except:
                    pass
                
                # Commit transaction
                self.duck_conn.execute("COMMIT")
                
                logger.info(f"Sau d·ªçn d·∫πp: {check_after[0]} fact records, {check_after[1]} unique combinations")
                
                # Ki·ªÉm tra xem c√≤n duplicate records kh√¥ng
                if check_after[0] != check_after[1]:
                    logger.warning(f"V·∫´n c√≤n {check_after[0] - check_after[1]} duplicate records sau khi d·ªçn d·∫πp!")
                
                return {
                    "success": True,
                    "duplicate_count": duplicate_count,
                    "remaining_facts": check_after[0],
                    "unique_combinations": check_after[1],
                    "is_clean": check_after[0] == check_after[1]
                }
                
            except Exception as e:
                # Rollback transaction n·∫øu c√≥ l·ªói
                self.duck_conn.execute("ROLLBACK")
                logger.error(f"L·ªói khi d·ªçn d·∫πp duplicate fact records, ƒë√£ rollback: {e}")
                return {
                    "success": False,
                    "error": str(e)
                }
                
        except Exception as e:
            logger.error(f"L·ªói nghi√™m tr·ªçng khi d·ªçn d·∫πp duplicate fact records: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def _create_new_fact_record(
        self,
        job: pd.Series,
        job_sk: int,
        company_sk: int,
        date_id: datetime,
        due_date: Optional[datetime],
        posted_time: Optional[datetime],
        crawled_at: datetime,
        load_month: str
    ) -> Optional[int]:
        """
        T·∫°o m·ªõi m·ªôt fact record v·ªõi proper sequence-based fact_id

        Args:
            job: B·∫£n ghi job t·ª´ staging
            job_sk: Surrogate key c·ªßa job
            company_sk: Surrogate key c·ªßa company
            date_id: Ng√†y c·∫ßn t·∫°o fact record
            due_date: Ng√†y h·∫øt h·∫°n job
            posted_time: Th·ªùi ƒëi·ªÉm ƒëƒÉng job
            crawled_at: Th·ªùi ƒëi·ªÉm crawl d·ªØ li·ªáu
            load_month: Th√°ng load d·ªØ li·ªáu (YYYY-MM)

        Returns:
            fact_id n·∫øu th√†nh c√¥ng, None n·∫øu th·∫•t b·∫°i
        """
        try:
            # Ki·ªÉm tra xem ƒë√£ c√≥ fact record v·ªõi job_sk v√† date_id n√†y ch∆∞a
            double_check = self.duck_conn.execute(
                "SELECT fact_id FROM FactJobPostingDaily WHERE job_sk = ? AND date_id = ?",
                [job_sk, date_id]
            ).fetchone()

            if double_check:
                logger.debug(f"Fact record ƒë√£ t·ªìn t·∫°i: job_sk={job_sk}, date_id={date_id}, fact_id={double_check[0]}")
                return double_check[0]
                
            # FIXED: Use proper UPSERT with ON CONFLICT to handle duplicates (no load_month update)
            upsert_query = """
                INSERT INTO FactJobPostingDaily (
                    job_sk, company_sk, date_id,
                    salary_min, salary_max, salary_type,
                    due_date, time_remaining, verified_employer,
                    posted_time, crawled_at, load_month
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT (job_sk, date_id)
                DO UPDATE SET
                    salary_min = EXCLUDED.salary_min,
                    salary_max = EXCLUDED.salary_max,
                    salary_type = EXCLUDED.salary_type,
                    due_date = EXCLUDED.due_date,
                    time_remaining = EXCLUDED.time_remaining,
                    verified_employer = EXCLUDED.verified_employer,
                    posted_time = EXCLUDED.posted_time,
                    crawled_at = EXCLUDED.crawled_at
                RETURNING fact_id
            """

            # REMOVED: Debug logging - functionality is stable

            upsert_values = [
                job_sk, company_sk, date_id,
                job.salary_min if pd.notna(job.salary_min) else None,
                job.salary_max if pd.notna(job.salary_max) else None,
                job.salary_type if pd.notna(job.salary_type) else None,
                due_date,
                job.time_remaining if pd.notna(job.time_remaining) else None,
                job.verified_employer if pd.notna(job.verified_employer) else False,
                posted_time,
                crawled_at,
                load_month
            ]

            # Check if record exists before UPSERT
            pre_check = self.duck_conn.execute(
                "SELECT fact_id FROM FactJobPostingDaily WHERE job_sk = ? AND date_id = ?",
                [job_sk, date_id]
            ).fetchone()
            existing_fact_id = pre_check[0] if pre_check else None

            # Execute UPSERT
            result = self.duck_conn.execute(upsert_query, upsert_values).fetchone()

            # Handle UPSERT result with fallback logic
            if result:
                # RETURNING worked
                fact_id = result[0]
                logger.debug(f"‚úÖ RETURNING success: fact_id={fact_id}, job_sk={job_sk}, date_id={date_id}")
                return fact_id
            elif existing_fact_id:
                # RETURNING failed but record existed - use existing fact_id
                logger.debug(f"üîÑ Using existing fact_id: {existing_fact_id}, job_sk={job_sk}, date_id={date_id}")
                return existing_fact_id
            else:
                # Query for fact_id after UPSERT
                post_check = self.duck_conn.execute(
                    "SELECT fact_id FROM FactJobPostingDaily WHERE job_sk = ? AND date_id = ?",
                    [job_sk, date_id]
                ).fetchone()
                if post_check:
                    fact_id = post_check[0]
                    logger.debug(f"üîç Found fact_id after UPSERT: {fact_id}, job_sk={job_sk}, date_id={date_id}")
                    return fact_id
                else:
                    logger.error(f"‚ùå No fact_id found after UPSERT: job_sk={job_sk}, date_id={date_id}")
                    return None
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o fact record cho job_id={job.job_id}, date={date_id}: {e}")
            return None