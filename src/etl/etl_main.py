#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ETL module ch√≠nh cho vi·ªác chuy·ªÉn d·ªØ li·ªáu t·ª´ Staging sang Data Warehouse (DuckDB)
Phi√™n b·∫£n c·∫£i ti·∫øn v·ªõi c·∫•u tr√∫c module r√µ r√†ng
"""
import pandas as pd
import logging
import os
import sys
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import duckdb

# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† logging
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))

# ƒê·∫£m b·∫£o th∆∞ m·ª•c logs t·ªìn t·∫°i
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

# ƒê·∫£m b·∫£o th∆∞ m·ª•c backup t·ªìn t·∫°i
BACKUP_DIR = os.path.join(PROJECT_ROOT, "data", "duck_db", "backup")
os.makedirs(BACKUP_DIR, exist_ok=True)

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "etl.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Import c√°c module c·∫ßn thi·∫øt
from src.utils.config import DUCKDB_PATH, DWH_STAGING_SCHEMA, STAGING_JOBS_TABLE
from src.utils.db import get_dataframe, execute_query
from src.common.decorators import retry

from .etl_utils import get_duckdb_connection, setup_duckdb_schema, batch_insert_records
from .dimension_handler import DimensionHandler
from .fact_handler import FactHandler
from .partitioning import PartitionManager

try:
    from src.processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range
    )
except ImportError:
    # Fallback cho tr∆∞·ªùng h·ª£p ch·∫°y tr·ª±c ti·∫øp script n√†y
    from processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range
    )

def backup_dwh_database():
    """
    T·∫°o b·∫£n backup c·ªßa DuckDB tr∆∞·ªõc khi ch·∫°y ETL
    
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file backup ho·∫∑c None n·∫øu kh√¥ng th·ªÉ backup
    """
    if not os.path.exists(DUCKDB_PATH):
        logger.info(f"Kh√¥ng t√¨m th·∫•y DuckDB ƒë·ªÉ backup: {DUCKDB_PATH}")
        return None
        
    try:
        # T·∫°o t√™n file backup v·ªõi timestamp v√† process ID ƒë·ªÉ tr√°nh xung ƒë·ªôt
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        pid = os.getpid()
        backup_filename = f"jobinsight_warehouse_{timestamp}_{pid}.duckdb.bak"
        backup_path = os.path.join(BACKUP_DIR, backup_filename)
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c backup t·ªìn t·∫°i
        os.makedirs(BACKUP_DIR, exist_ok=True)
        
        # ƒê√≥ng t·∫•t c·∫£ k·∫øt n·ªëi ƒë·∫øn DuckDB tr∆∞·ªõc khi backup
        # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ tr√°nh file b·ªã lock
        try:
            # FIXED: DuckDB doesn't support shutdown_transactions pragma
            # Simply ensure any existing connections are properly closed
            temp_conn = duckdb.connect(DUCKDB_PATH)
            # Just test connection and close - no need for shutdown_transactions
            temp_conn.execute("SELECT 1")
            temp_conn.close()
            logger.info("ƒê√£ ki·ªÉm tra v√† ƒë√≥ng k·∫øt n·ªëi DuckDB")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ ki·ªÉm tra k·∫øt n·ªëi DuckDB: {e}")
        
        # Copy file DuckDB hi·ªán t·∫°i sang file backup
        shutil.copy2(DUCKDB_PATH, backup_path)
        
        # Ki·ªÉm tra xem file backup c√≥ t·ªìn t·∫°i kh√¥ng
        if os.path.exists(backup_path):
            backup_size = os.path.getsize(backup_path) / (1024 * 1024)  # MB
            logger.info(f"‚úÖ ƒê√£ t·∫°o backup DuckDB: {backup_path} ({backup_size:.2f} MB)")
            
            # D·ªçn d·∫πp c√°c backup c≈© (gi·ªØ l·∫°i 5 backup g·∫ßn nh·∫•t)
            cleanup_old_backups()
            
            return backup_path
        else:
            logger.warning(f"‚ùå Kh√¥ng th·ªÉ t·∫°o backup DuckDB")
            return None
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·∫°o backup DuckDB: {e}")
        return None

def cleanup_old_backups(keep_count=5):
    """
    D·ªçn d·∫πp c√°c file backup c≈©, ch·ªâ gi·ªØ l·∫°i s·ªë l∆∞·ª£ng file m·ªõi nh·∫•t
    
    Args:
        keep_count: S·ªë l∆∞·ª£ng file backup m·ªõi nh·∫•t c·∫ßn gi·ªØ l·∫°i
    """
    try:
        # L·∫•y danh s√°ch t·∫•t c·∫£ c√°c file backup
        backup_files = [os.path.join(BACKUP_DIR, f) for f in os.listdir(BACKUP_DIR) 
                       if f.endswith('.duckdb.bak')]
        
        # S·∫Øp x·∫øp theo th·ªùi gian s·ª≠a ƒë·ªïi (m·ªõi nh·∫•t ƒë·∫ßu ti√™n)
        backup_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        # X√≥a c√°c file c≈©
        if len(backup_files) > keep_count:
            for old_file in backup_files[keep_count:]:
                os.remove(old_file)
                logger.info(f"ƒê√£ x√≥a backup c≈©: {os.path.basename(old_file)}")
    except Exception as e:
        logger.warning(f"L·ªói khi d·ªçn d·∫πp backup c≈©: {e}")

def restore_dwh_from_backup(backup_path):
    """
    Kh√¥i ph·ª•c DuckDB t·ª´ file backup
    
    Args:
        backup_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file backup
        
    Returns:
        bool: True n·∫øu kh√¥i ph·ª•c th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    if not os.path.exists(backup_path):
        logger.error(f"Kh√¥ng t√¨m th·∫•y file backup: {backup_path}")
        return False
        
    try:
        # ƒê√≥ng t·∫•t c·∫£ k·∫øt n·ªëi ƒë·∫øn DuckDB tr∆∞·ªõc khi kh√¥i ph·ª•c
        try:
            # T·∫°o k·∫øt n·ªëi m·ªõi v√† ƒë√≥ng t·∫•t c·∫£ c√°c k·∫øt n·ªëi hi·ªán c√≥
            temp_conn = duckdb.connect(DUCKDB_PATH)
            temp_conn.execute("PRAGMA shutdown_transactions")
            temp_conn.close()
            logger.info("ƒê√£ ƒë√≥ng t·∫•t c·∫£ k·∫øt n·ªëi hi·ªán c√≥ ƒë·∫øn DuckDB")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ ƒë√≥ng k·∫øt n·ªëi hi·ªán c√≥: {e}")
        
        # T·∫°o backup c·ªßa file hi·ªán t·∫°i tr∆∞·ªõc khi kh√¥i ph·ª•c (ph√≤ng tr∆∞·ªùng h·ª£p kh√¥i ph·ª•c th·∫•t b·∫°i)
        if os.path.exists(DUCKDB_PATH):
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            pid = os.getpid()
            pre_restore_backup = os.path.join(BACKUP_DIR, f"pre_restore_{timestamp}_{pid}.duckdb.bak")
            shutil.copy2(DUCKDB_PATH, pre_restore_backup)
            logger.info(f"ƒê√£ t·∫°o backup tr∆∞·ªõc khi kh√¥i ph·ª•c: {pre_restore_backup}")
        
        # Copy file backup sang file DuckDB
        shutil.copy2(backup_path, DUCKDB_PATH)
        
        # Ki·ªÉm tra xem file DuckDB c√≥ t·ªìn t·∫°i kh√¥ng
        if os.path.exists(DUCKDB_PATH):
            logger.info(f"‚úÖ ƒê√£ kh√¥i ph·ª•c DuckDB t·ª´ backup: {backup_path}")
            
            # Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa database sau khi kh√¥i ph·ª•c
            try:
                test_conn = duckdb.connect(DUCKDB_PATH)
                test_conn.execute("SELECT 1")  # Th·ª≠ th·ª±c hi·ªán m·ªôt truy v·∫•n ƒë∆°n gi·∫£n
                test_conn.close()
                logger.info("‚úÖ Ki·ªÉm tra t√≠nh to√†n v·∫πn database th√†nh c√¥ng")
            except Exception as e:
                logger.error(f"‚ùå Database kh√¥ng ho·∫°t ƒë·ªông sau khi kh√¥i ph·ª•c: {e}")
                
                # Th·ª≠ kh√¥i ph·ª•c t·ª´ pre-restore backup n·∫øu c√≥
                if os.path.exists(pre_restore_backup):
                    logger.warning("Th·ª≠ kh√¥i ph·ª•c t·ª´ pre-restore backup...")
                    shutil.copy2(pre_restore_backup, DUCKDB_PATH)
                    logger.info("ƒê√£ kh√¥i ph·ª•c t·ª´ pre-restore backup")
                
                return False
            
            return True
        else:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ kh√¥i ph·ª•c DuckDB t·ª´ backup")
            return False
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi kh√¥i ph·ª•c DuckDB t·ª´ backup: {e}")
        return False

@retry(max_tries=3, delay_seconds=2, backoff_factor=2, exceptions=[Exception])
def get_staging_batch(last_etl_date: datetime) -> pd.DataFrame:
    """
    L·∫•y batch d·ªØ li·ªáu t·ª´ staging jobs k·ªÉ t·ª´ l·∫ßn ETL g·∫ßn nh·∫•t
    
    Args:
        last_etl_date: Timestamp c·ªßa l·∫ßn ETL g·∫ßn nh·∫•t
        
    Returns:
        DataFrame ch·ª©a b·∫£n ghi c·∫ßn x·ª≠ l√Ω
    """
    try:
        logger.info(f"Truy v·∫•n d·ªØ li·ªáu t·ª´ b·∫£ng {STAGING_JOBS_TABLE}")
        
        query = f"""
            SELECT *
            FROM {STAGING_JOBS_TABLE}
            WHERE crawled_at >= %s
            OR
            (crawled_at IS NOT NULL AND %s IS NULL)
        """
        
        # Ki·ªÉm tra xem b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
        table_exists_query = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{DWH_STAGING_SCHEMA}' 
                AND table_name = 'staging_jobs'
            );
        """
        
        # Th·ª±c hi·ªán ki·ªÉm tra
        exists = execute_query(table_exists_query, fetch=True)
        
        if not exists or not exists[0].get('exists', False):
            logger.error(f"B·∫£ng {STAGING_JOBS_TABLE} kh√¥ng t·ªìn t·∫°i!")
            # Tr·∫£ v·ªÅ DataFrame r·ªóng n·∫øu b·∫£ng kh√¥ng t·ªìn t·∫°i
            return pd.DataFrame()
        
        df = get_dataframe(query, params=(last_etl_date, last_etl_date))
        logger.info(f"ƒê√£ l·∫•y {len(df)} b·∫£n ghi t·ª´ staging (t·ª´ {last_etl_date})")
        
        # REMOVED: Debug column logging - not needed in production
        
        return df
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y d·ªØ li·ªáu t·ª´ staging: {e}", exc_info=True)
        # Tr·∫£ v·ªÅ DataFrame r·ªóng trong tr∆∞·ªùng h·ª£p l·ªói
        return pd.DataFrame()

def verify_etl_integrity(staging_count: int, fact_count: int, threshold: float = 0.9) -> bool:
    """
    Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa qu√° tr√¨nh ETL Staging to DWH
    
    Args:
        staging_count: S·ªë b·∫£n ghi staging ƒë·∫ßu v√†o
        fact_count: S·ªë b·∫£n ghi fact ƒë√£ t·∫°o
        threshold: Ng∆∞·ª°ng ch·∫•p nh·∫≠n (% d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng)
        
    Returns:
        bool: True n·∫øu t·ª∑ l·ªá d·ªØ li·ªáu chuy·ªÉn ƒë·ªïi ƒë·∫°t threshold
    """
    if staging_count == 0:
        logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ngu·ªìn ƒë·ªÉ x·ª≠ l√Ω")
        return True
    
    # M·ªói staging record c√≥ th·ªÉ t·∫°o ra nhi·ªÅu fact record (m·ªói ng√†y m·ªôt record)
    # N√™n ki·ªÉm tra xem c√≥ fact records ƒë∆∞·ª£c t·∫°o kh√¥ng, kh√¥ng so s√°nh s·ªë l∆∞·ª£ng
    if fact_count == 0:
        logger.error("Kh√¥ng c√≥ fact record n√†o ƒë∆∞·ª£c t·∫°o t·ª´ staging data!")
        return False
    
    logger.info(f"ƒê√£ t·∫°o {fact_count} fact records t·ª´ {staging_count} staging records")
    return True

@retry(max_tries=3, delay_seconds=2, backoff_factor=1.5, exceptions=[Exception])
def process_dimension_batch(dim_handler, staging_batch, dim_name, prepare_function, natural_key=None, surrogate_key=None, compare_columns=None):
    """
    X·ª≠ l√Ω m·ªôt dimension batch v·ªõi retry
    
    Args:
        dim_handler: DimensionHandler instance
        staging_batch: DataFrame staging data
        dim_name: T√™n dimension table
        prepare_function: H√†m chu·∫©n b·ªã d·ªØ li·ªáu
        natural_key: Natural key column (None cho DimLocation)
        surrogate_key: Surrogate key column (None cho DimLocation)
        compare_columns: Columns ƒë·ªÉ so s√°nh thay ƒë·ªïi (None cho DimLocation)
        
    Returns:
        Dict th·ªëng k√™ k·∫øt qu·∫£
    """
    logger.info(f"X·ª≠ l√Ω dimension {dim_name} v·ªõi retry...")
    
    try:
        # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho DimLocation
        if dim_name == 'DimLocation':
            stats = dim_handler.process_location_dimension(staging_batch, prepare_function)
        else:
            # X·ª≠ l√Ω th√¥ng th∆∞·ªùng cho c√°c dimension kh√°c
            stats = dim_handler.process_dimension_with_scd2(
                staging_batch,
                dim_name,
                prepare_function,
                natural_key,
                surrogate_key,
                compare_columns
            )
        logger.info(f"ƒê√£ x·ª≠ l√Ω {dim_name}: {stats}")
        return stats
    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω dimension {dim_name}: {e}", exc_info=True)
        # Re-raise ƒë·ªÉ retry decorator c√≥ th·ªÉ x·ª≠ l√Ω
        raise

@retry(max_tries=3, delay_seconds=2, backoff_factor=2, exceptions=[Exception])
def generate_fact_records_with_retry(fact_handler, staging_batch):
    """
    T·∫°o fact records v·ªõi retry
    
    Args:
        fact_handler: FactHandler instance
        staging_batch: DataFrame staging data
        
    Returns:
        Tuple ch·ª©a (fact_records, bridge_records)
    """
    logger.info("T·∫°o fact records v·ªõi retry...")
    
    try:
        fact_records, bridge_records = fact_handler.generate_fact_records(staging_batch)
        logger.info(f"ƒê√£ t·∫°o {len(fact_records)} fact records v√† {len(bridge_records)} bridge records")
        return fact_records, bridge_records
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o fact records: {e}", exc_info=True)
        # Re-raise ƒë·ªÉ retry decorator c√≥ th·ªÉ x·ª≠ l√Ω
        raise

def run_staging_to_dwh_etl(last_etl_date: Optional[datetime] = None) -> Dict[str, Any]:
    """
    Ch·∫°y ETL t·ª´ staging sang DWH
    
    Args:
        last_etl_date: Ng√†y ch·∫°y ETL g·∫ßn nh·∫•t, l·∫•y d·ªØ li·ªáu t·ª´ ng√†y n√†y ƒë·∫øn hi·ªán t·∫°i
        
    Returns:
        Dict th√¥ng tin v·ªÅ k·∫øt qu·∫£ ETL
    """
    start_time = datetime.now()
    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu ETL Staging to DWH...")
    
    # T·∫°o backup tr∆∞·ªõc khi ch·∫°y ETL
    logger.info("üì¶ T·∫°o backup database tr∆∞·ªõc khi ch·∫°y ETL...")
    backup_path = backup_dwh_database()
    if backup_path:
        logger.info(f"‚úÖ Backup th√†nh c√¥ng: {backup_path}")
    else:
        logger.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o backup database")
    
    try:
        # 1. L·∫•y d·ªØ li·ªáu t·ª´ staging
        if last_etl_date is None:
            last_etl_date = datetime.now() - timedelta(days=7)
            
        logger.info(f"üì• L·∫•y d·ªØ li·ªáu t·ª´ staging t·ª´ {last_etl_date}...")
        staging_batch = get_staging_batch(last_etl_date)
        
        if staging_batch.empty:
            logger.info("‚ÑπÔ∏è Kh√¥ng c√≥ b·∫£n ghi n√†o ƒë·ªÉ x·ª≠ l√Ω t·ª´ staging")
            return {
                "success": True,
                "message": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω",
                "source_count": 0,
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
            
        logger.info(f"‚úÖ ƒê√£ l·∫•y {len(staging_batch)} b·∫£n ghi t·ª´ staging")
        
        # 2. Ki·ªÉm tra file DuckDB
        if os.path.exists(DUCKDB_PATH):
            logger.info(f"üìÅ S·ª≠ d·ª•ng DuckDB hi·ªán c√≥: {DUCKDB_PATH}")
            file_size_mb = os.path.getsize(DUCKDB_PATH) / (1024 * 1024)
            logger.info(f"üìä K√≠ch th∆∞·ªõc file DuckDB: {file_size_mb:.2f} MB")
        else:
            logger.info(f"üÜï T·∫°o DuckDB m·ªõi: {DUCKDB_PATH}")
        
        # 3. Thi·∫øt l·∫≠p schema v√† b·∫£ng (gi·ªØ nguy√™n d·ªØ li·ªáu c≈©)
        logger.info("üîß Thi·∫øt l·∫≠p schema v√† b·∫£ng DuckDB...")
        if not setup_duckdb_schema():
            logger.error("‚ùå Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema DuckDB")
            if backup_path:
                logger.warning("üîÑ Kh√¥i ph·ª•c t·ª´ backup do l·ªói thi·∫øt l·∫≠p schema...")
                restore_dwh_from_backup(backup_path)
                
            return {
                "success": False,
                "message": "Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema DuckDB",
                "source_count": len(staging_batch),
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
        
        # 4. K·∫øt n·ªëi DuckDB v√† th·ª±c hi·ªán ETL v·ªõi SCD Type 2
        logger.info("üîå K·∫øt n·ªëi DuckDB v√† b·∫Øt ƒë·∫ßu x·ª≠ l√Ω ETL...")
        with get_duckdb_connection(DUCKDB_PATH) as duck_conn:
            try:
                # Kh·ªüi t·∫°o c√°c handler
                logger.info("üîß Kh·ªüi t·∫°o c√°c handler x·ª≠ l√Ω...")
                dim_handler = DimensionHandler(duck_conn)
                fact_handler = FactHandler(duck_conn)
                partition_manager = PartitionManager(duck_conn)
                
                # Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu ban ƒë·∫ßu
                logger.info("üîç Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu ban ƒë·∫ßu...")
                initial_integrity_check = duck_conn.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM FactJobPostingDaily) as total_facts,
                        (SELECT COUNT(*) FROM (SELECT DISTINCT job_sk, date_id FROM FactJobPostingDaily)) as unique_combinations
                """).fetchone()
                
                if initial_integrity_check and initial_integrity_check[0] != initial_integrity_check[1]:
                    logger.warning(f"‚ö†Ô∏è Ph√°t hi·ªán v·∫•n ƒë·ªÅ tr√πng l·∫∑p d·ªØ li·ªáu: {initial_integrity_check[0]} facts, {initial_integrity_check[1]} unique combinations")
                else:
                    logger.info(f"‚úÖ D·ªØ li·ªáu ban ƒë·∫ßu kh√¥ng c√≥ v·∫•n ƒë·ªÅ tr√πng l·∫∑p: {initial_integrity_check[0] if initial_integrity_check else 0} facts")
                
                # D·ªçn d·∫πp duplicate records hi·ªán c√≥
                logger.info("üßπ D·ªçn d·∫πp duplicate records hi·ªán c√≥...")
                cleanup_start = datetime.now()
                cleanup_result = fact_handler.cleanup_duplicate_fact_records()
                cleanup_duration = (datetime.now() - cleanup_start).total_seconds()
                logger.info(f"‚úÖ D·ªçn d·∫πp ho√†n t·∫•t trong {cleanup_duration:.2f} gi√¢y. K·∫øt qu·∫£: {cleanup_result}")
                
                # X·ª≠ l√Ω dimension tables
                logger.info("üîÑ X·ª≠ l√Ω dimension tables v·ªõi SCD Type 2...")
                
                # DimJob
                logger.info("‚è≥ X·ª≠ l√Ω DimJob...")
                job_start = datetime.now()
                job_stats = process_dimension_batch(dim_handler, staging_batch, 'DimJob', prepare_dim_job, 'job_id', 'job_sk', ['title_clean', 'job_url', 'skills', 'last_update', 'logo_url'])
                job_duration = (datetime.now() - job_start).total_seconds()
                logger.info(f"‚úÖ DimJob x·ª≠ l√Ω ho√†n t·∫•t trong {job_duration:.2f} gi√¢y. K·∫øt qu·∫£: {job_stats}")
                
                # DimCompany
                logger.info("‚è≥ X·ª≠ l√Ω DimCompany...")
                company_start = datetime.now()
                company_stats = process_dimension_batch(dim_handler, staging_batch, 'DimCompany', prepare_dim_company, 'company_name_standardized', 'company_sk', ['company_url', 'verified_employer'])
                company_duration = (datetime.now() - company_start).total_seconds()
                logger.info(f"‚úÖ DimCompany x·ª≠ l√Ω ho√†n t·∫•t trong {company_duration:.2f} gi√¢y. K·∫øt qu·∫£: {company_stats}")
                
                # DimLocation (x·ª≠ l√Ω ƒë·∫∑c bi·ªát v√¨ c√≥ composite key)
                logger.info("‚è≥ X·ª≠ l√Ω DimLocation...")
                location_start = datetime.now()
                location_stats = process_dimension_batch(dim_handler, staging_batch, 'DimLocation', prepare_dim_location, None, None, None)
                location_duration = (datetime.now() - location_start).total_seconds()
                logger.info(f"‚úÖ DimLocation x·ª≠ l√Ω ho√†n t·∫•t trong {location_duration:.2f} gi√¢y. K·∫øt qu·∫£: {location_stats}")
                
                # X·ª≠ l√Ω DimDate
                logger.info("‚è≥ X·ª≠ l√Ω DimDate...")
                date_start = datetime.now()
                
                # T√¨m ng√†y b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c t·ª´ staging data
                min_date = datetime.now().date() - timedelta(days=30)  # M·∫∑c ƒë·ªãnh 30 ng√†y tr∆∞·ªõc
                max_date = datetime.now().date() + timedelta(days=270)  # M·∫∑c ƒë·ªãnh 270 ng√†y sau
                
                # T·∫°o date range
                date_range_df = generate_date_range(min_date, max_date)
                
                # Insert v√†o DimDate
                date_records = []
                for _, date_record in date_range_df.iterrows():
                    date_records.append(date_record.to_dict())
                
                date_count = batch_insert_records(
                    duck_conn, 
                    'DimDate', 
                    date_records, 
                    on_conflict='ON CONFLICT (date_id) DO NOTHING'
                )
                
                date_duration = (datetime.now() - date_start).total_seconds()
                logger.info(f"‚úÖ DimDate x·ª≠ l√Ω ho√†n t·∫•t trong {date_duration:.2f} gi√¢y. ƒê√£ th√™m {date_count} ng√†y m·ªõi.")
                
                # X·ª≠ l√Ω fact table
                logger.info("‚è≥ X·ª≠ l√Ω FactJobPostingDaily v√† FactJobLocationBridge...")
                fact_start = datetime.now()
                fact_records, bridge_records = generate_fact_records_with_retry(fact_handler, staging_batch)
                fact_duration = (datetime.now() - fact_start).total_seconds()
                logger.info(f"‚úÖ Fact tables x·ª≠ l√Ω ho√†n t·∫•t trong {fact_duration:.2f} gi√¢y. ƒê√£ t·∫°o {len(fact_records)} fact records v√† {len(bridge_records)} bridge records.")
                
                # REMOVED: processed_ids not used - staging marking handled elsewhere
                
                # T√≥m t·∫Øt k·∫øt qu·∫£
                dim_stats = {
                    'DimJob': job_stats,
                    'DimCompany': company_stats,
                    'DimLocation': location_stats,
                    'DimDate': {'inserted': date_count, 'updated': 0, 'unchanged': 0}
                }
                
                # Qu·∫£n l√Ω partition v√† export sang Parquet
                logger.info("üìä Qu·∫£n l√Ω partition v√† export sang Parquet")
                partition_start = datetime.now()
                partition_result = partition_manager.manage_partitions('FactJobPostingDaily', 'load_month')
                partition_duration = (datetime.now() - partition_start).total_seconds()
                logger.info(f"‚úÖ Partition x·ª≠ l√Ω ho√†n t·∫•t trong {partition_duration:.2f} gi√¢y. K·∫øt qu·∫£: {partition_result}")
                
                # T·ªïng k·∫øt
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                logger.info("============================================================")
                logger.info(f"üìä T·ªîNG K·∫æT ETL STAGING TO DWH")
                logger.info("============================================================")
                
                # Log th·ªëng k√™ dimension
                for dim_name, stats in dim_stats.items():
                    logger.info(f"{dim_name:<15} - Insert: {stats['inserted']:5d}, Update: {stats['updated']:5d}, Unchanged: {stats['unchanged']:5d}")
                
                logger.info("------------------------------------------------------------")
                total_inserted = sum(stats['inserted'] for stats in dim_stats.values())
                total_updated = sum(stats['updated'] for stats in dim_stats.values())
                total_unchanged = sum(stats['unchanged'] for stats in dim_stats.values())
                logger.info(f"T·ªîNG DIM        - Insert: {total_inserted:5d}, Update: {total_updated:5d}, Unchanged: {total_unchanged:5d}")
                
                # Log th·ªëng k√™ partition
                if partition_result['success']:
                    logger.info(f"PARTITION       - Th√†nh c√¥ng: {partition_result['message']}")
                else:
                    logger.warning(f"PARTITION       - Th·∫•t b·∫°i: {partition_result['message']}")
                
                # Validation ETL
                logger.info("üîç B·∫Øt ƒë·∫ßu validation ETL...")
                validation_start = datetime.now()
                try:
                    from src.utils.etl_validator import generate_etl_report, log_validation_results
                    validation_result = generate_etl_report(duck_conn)
                    log_validation_results(validation_result)
                    validation_duration = (datetime.now() - validation_start).total_seconds()
                    logger.info(f"‚úÖ Validation ho√†n t·∫•t trong {validation_duration:.2f} gi√¢y.")
                except ImportError as e:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ import etl_validator - b·ªè qua validation: {e}")
                except Exception as e:
                    logger.error(f"‚ùå L·ªói khi th·ª±c hi·ªán validation: {str(e)}")
                
                # K·∫øt th√∫c
                logger.info("============================================================")
                logger.info(f"‚úÖ ETL HO√ÄN TH√ÄNH TRONG {duration:.2f} GI√ÇY!")
                logger.info(f"üïí Th·ªùi gian b·∫Øt ƒë·∫ßu: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
                logger.info(f"üïí Th·ªùi gian k·∫øt th√∫c: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
                logger.info("============================================================")
                
                return {
                    "success": True,
                    "message": f"ETL ho√†n th√†nh th√†nh c√¥ng trong {duration:.2f} gi√¢y",
                    "source_count": len(staging_batch),
                    "fact_count": len(fact_records),
                    "dim_stats": dim_stats,
                    "partition_success": partition_result['success'],
                    "duration_seconds": duration,
                    "start_time": start_time.isoformat(),
                    "end_time": end_time.isoformat()
                }
            except Exception as e:
                logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh ETL: {e}", exc_info=True)
                if backup_path:
                    logger.warning("üîÑ Kh√¥i ph·ª•c t·ª´ backup do l·ªói trong qu√° tr√¨nh ETL...")
                    restore_result = restore_dwh_from_backup(backup_path)
                    logger.info(f"K·∫øt qu·∫£ kh√¥i ph·ª•c: {'‚úÖ Th√†nh c√¥ng' if restore_result else '‚ùå Th·∫•t b·∫°i'}")
                
                return {
                    "success": False,
                    "message": f"L·ªói trong qu√° tr√¨nh ETL: {str(e)}",
                    "source_count": len(staging_batch),
                    "fact_count": 0,
                    "duration_seconds": (datetime.now() - start_time).total_seconds(),
                    "error": str(e),
                    "error_type": type(e).__name__
                }
    except Exception as e:
        logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh ETL: {e}", exc_info=True)
        if backup_path:
            logger.warning("üîÑ Kh√¥i ph·ª•c t·ª´ backup do l·ªói nghi√™m tr·ªçng...")
            restore_result = restore_dwh_from_backup(backup_path)
            logger.info(f"K·∫øt qu·∫£ kh√¥i ph·ª•c: {'‚úÖ Th√†nh c√¥ng' if restore_result else '‚ùå Th·∫•t b·∫°i'}")
            
        return {
            "success": False,
            "message": f"L·ªói nghi√™m tr·ªçng: {str(e)}",
            "source_count": 0,
            "fact_count": 0,
            "duration_seconds": (datetime.now() - start_time).total_seconds(),
            "error": str(e),
            "error_type": type(e).__name__
        }

def run_incremental_etl(last_etl_date: Optional[datetime] = None, batch_size: int = 1000) -> Dict[str, Any]:
    """
    Ch·∫°y ETL t·ª´ staging sang DWH theo batch ƒë·ªÉ gi·∫£m √°p l·ª±c l√™n h·ªá th·ªëng
    
    Args:
        last_etl_date: Ng√†y ch·∫°y ETL g·∫ßn nh·∫•t, l·∫•y d·ªØ li·ªáu t·ª´ ng√†y n√†y ƒë·∫øn hi·ªán t·∫°i
        batch_size: S·ªë b·∫£n ghi t·ªëi ƒëa x·ª≠ l√Ω trong m·ªói batch
        
    Returns:
        Dict th√¥ng tin v·ªÅ k·∫øt qu·∫£ ETL
    """
    start_time = datetime.now()
    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu Incremental ETL Staging to DWH v·ªõi batch_size={batch_size}...")
    
    # T·∫°o backup tr∆∞·ªõc khi ch·∫°y ETL
    backup_path = backup_dwh_database()
    
    try:
        # 1. L·∫•y d·ªØ li·ªáu t·ª´ staging
        if last_etl_date is None:
            last_etl_date = datetime.now() - timedelta(days=7)
            
        logger.info(f"L·∫•y d·ªØ li·ªáu t·ª´ staging t·ª´ {last_etl_date}...")
        full_staging_batch = get_staging_batch(last_etl_date)
        
        if full_staging_batch.empty:
            logger.info("Kh√¥ng c√≥ b·∫£n ghi n√†o ƒë·ªÉ x·ª≠ l√Ω t·ª´ staging")
            return {
                "success": True,
                "message": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω",
                "source_count": 0,
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
            
        total_records = len(full_staging_batch)
        logger.info(f"ƒê√£ l·∫•y t·ªïng c·ªông {total_records} b·∫£n ghi t·ª´ staging")
        
        # Chia th√†nh c√°c batch
        num_batches = (total_records + batch_size - 1) // batch_size  # Ceiling division
        logger.info(f"Chia th√†nh {num_batches} batch, m·ªói batch {batch_size} b·∫£n ghi")
        
        # Th·ªëng k√™ t·ªïng h·ª£p
        total_stats = {
            "source_count": total_records,
            "processed_count": 0,
            "fact_count": 0,
            "dim_stats": {
                "DimJob": {"inserted": 0, "updated": 0, "unchanged": 0},
                "DimCompany": {"inserted": 0, "updated": 0, "unchanged": 0},
                "DimLocation": {"inserted": 0, "updated": 0, "unchanged": 0},
                "DimDate": {"inserted": 0, "updated": 0, "unchanged": 0}
            },
            "batch_results": []
        }
        
        # X·ª≠ l√Ω t·ª´ng batch
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_records)
            
            logger.info(f"X·ª≠ l√Ω batch {batch_idx+1}/{num_batches} (records {start_idx+1}-{end_idx}/{total_records})...")
            
            # L·∫•y batch hi·ªán t·∫°i
            current_batch = full_staging_batch.iloc[start_idx:end_idx].copy()
            
            # Ch·∫°y ETL cho batch n√†y
            try:
                batch_result = run_staging_to_dwh_etl_batch(current_batch)
                
                # C·∫≠p nh·∫≠t th·ªëng k√™ t·ªïng h·ª£p
                total_stats["processed_count"] += len(current_batch)
                total_stats["fact_count"] += batch_result.get("fact_count", 0)
                
                # C·∫≠p nh·∫≠t dim_stats
                for dim_name, stats in batch_result.get("dim_stats", {}).items():
                    if dim_name in total_stats["dim_stats"]:
                        for key in ["inserted", "updated", "unchanged"]:
                            total_stats["dim_stats"][dim_name][key] += stats.get(key, 0)
                
                # L∆∞u k·∫øt qu·∫£ batch
                total_stats["batch_results"].append({
                    "batch_idx": batch_idx,
                    "records": len(current_batch),
                    "success": batch_result.get("success", False),
                    "fact_count": batch_result.get("fact_count", 0)
                })
                
                logger.info(f"Batch {batch_idx+1}/{num_batches} ho√†n th√†nh: {len(current_batch)} records, {batch_result.get('fact_count', 0)} facts")
                
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω batch {batch_idx+1}/{num_batches}: {e}", exc_info=True)
                # Ti·∫øp t·ª•c v·ªõi batch ti·∫øp theo
                total_stats["batch_results"].append({
                    "batch_idx": batch_idx,
                    "records": len(current_batch),
                    "success": False,
                    "error": str(e)
                })
        
        # Qu·∫£n l√Ω partition v√† export sang Parquet
        logger.info("üìä Qu·∫£n l√Ω partition v√† export sang Parquet cho t·∫•t c·∫£ d·ªØ li·ªáu...")
        with get_duckdb_connection(DUCKDB_PATH) as duck_conn:
            partition_manager = PartitionManager(duck_conn)
            partition_result = partition_manager.manage_partitions('FactJobPostingDaily', 'load_month')
            total_stats["partition_success"] = partition_result.get("success", False)
        
        # T·ªïng k·∫øt
        duration = (datetime.now() - start_time).total_seconds()
        logger.info("============================================================")
        logger.info(f"üìä T·ªîNG K·∫æT INCREMENTAL ETL STAGING TO DWH")
        logger.info("============================================================")
        logger.info(f"T·ªïng s·ªë b·∫£n ghi: {total_records}")
        logger.info(f"ƒê√£ x·ª≠ l√Ω: {total_stats['processed_count']}")
        logger.info(f"T·∫°o facts: {total_stats['fact_count']}")
        
        # Log th·ªëng k√™ dimension
        for dim_name, stats in total_stats["dim_stats"].items():
            logger.info(f"{dim_name:<15} - Insert: {stats['inserted']:5d}, Update: {stats['updated']:5d}, Unchanged: {stats['unchanged']:5d}")
        
        # Log th·ªëng k√™ batch
        success_batches = sum(1 for batch in total_stats["batch_results"] if batch["success"])
        logger.info(f"Batches th√†nh c√¥ng: {success_batches}/{num_batches}")
        
        # Log th·ªëng k√™ partition
        if total_stats["partition_success"]:
            logger.info(f"PARTITION       - Th√†nh c√¥ng: {partition_result.get('message', '')}")
        else:
            logger.warning(f"PARTITION       - Th·∫•t b·∫°i: {partition_result.get('message', '')}")
        
        logger.info("============================================================")
        logger.info(f"‚úÖ INCREMENTAL ETL HO√ÄN TH√ÄNH TRONG {duration:.2f} GI√ÇY!")
        logger.info("============================================================")
        
        return {
            "success": total_stats["processed_count"] > 0,
            "message": f"Incremental ETL ho√†n th√†nh trong {duration:.2f} gi√¢y",
            "source_count": total_records,
            "processed_count": total_stats["processed_count"],
            "fact_count": total_stats["fact_count"],
            "dim_stats": total_stats["dim_stats"],
            "batch_results": total_stats["batch_results"],
            "partition_success": total_stats["partition_success"],
            "duration_seconds": duration
        }
        
    except Exception as e:
        logger.error(f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh Incremental ETL: {e}", exc_info=True)
        if backup_path:
            logger.warning("Kh√¥i ph·ª•c t·ª´ backup do l·ªói nghi√™m tr·ªçng...")
            restore_dwh_from_backup(backup_path)
            
        return {
            "success": False,
            "message": f"L·ªói nghi√™m tr·ªçng trong Incremental ETL: {str(e)}",
            "source_count": 0,
            "processed_count": 0,
            "fact_count": 0,
            "duration_seconds": (datetime.now() - start_time).total_seconds()
        }

def run_staging_to_dwh_etl_batch(staging_batch: pd.DataFrame) -> Dict[str, Any]:
    """
    Ch·∫°y ETL t·ª´ staging sang DWH cho m·ªôt batch d·ªØ li·ªáu
    
    Args:
        staging_batch: DataFrame ch·ª©a d·ªØ li·ªáu staging c·∫ßn x·ª≠ l√Ω
        
    Returns:
        Dict th√¥ng tin v·ªÅ k·∫øt qu·∫£ ETL
    """
    start_time = datetime.now()
    logger.info(f"B·∫Øt ƒë·∫ßu ETL batch v·ªõi {len(staging_batch)} b·∫£n ghi...")
    
    try:
        # REMOVED: Debug DuckDB file logging - not needed
        
        # Thi·∫øt l·∫≠p schema v√† b·∫£ng (gi·ªØ nguy√™n d·ªØ li·ªáu c≈©)
        if not setup_duckdb_schema():
            logger.error("Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema DuckDB")
            return {
                "success": False,
                "message": "Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema DuckDB",
                "source_count": len(staging_batch),
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
        
        # K·∫øt n·ªëi DuckDB v√† th·ª±c hi·ªán ETL v·ªõi SCD Type 2
        with get_duckdb_connection(DUCKDB_PATH) as duck_conn:
            try:
                # B·∫Øt ƒë·∫ßu transaction ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
                duck_conn.execute("BEGIN TRANSACTION")
                
                try:
                    # Kh·ªüi t·∫°o c√°c handler
                    dim_handler = DimensionHandler(duck_conn)
                    fact_handler = FactHandler(duck_conn)
                    
                    # D·ªçn d·∫πp duplicate records hi·ªán c√≥
                    fact_handler.cleanup_duplicate_fact_records()
                    
                    # X·ª≠ l√Ω dimension tables
                    logger.info("X·ª≠ l√Ω dimension tables v·ªõi SCD Type 2...")
                    
                    # DimJob
                    job_stats = process_dimension_batch(dim_handler, staging_batch, 'DimJob', prepare_dim_job, 'job_id', 'job_sk', ['title_clean', 'job_url', 'skills', 'last_update', 'logo_url'])
                    
                    # DimCompany
                    company_stats = process_dimension_batch(dim_handler, staging_batch, 'DimCompany', prepare_dim_company, 'company_name_standardized', 'company_sk', ['company_url', 'verified_employer'])
                    
                    # DimLocation (x·ª≠ l√Ω ƒë·∫∑c bi·ªát v√¨ c√≥ composite key)
                    location_stats = process_dimension_batch(dim_handler, staging_batch, 'DimLocation', prepare_dim_location)
                    
                    # X·ª≠ l√Ω DimDate
                    logger.info("X·ª≠ l√Ω DimDate")
                    
                    # T√¨m ng√†y b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c t·ª´ staging data
                    min_date = datetime.now().date() - timedelta(days=30)  # M·∫∑c ƒë·ªãnh 30 ng√†y tr∆∞·ªõc
                    max_date = datetime.now().date() + timedelta(days=270)  # M·∫∑c ƒë·ªãnh 270 ng√†y sau
                    
                    # T·∫°o date range
                    date_range_df = generate_date_range(min_date, max_date)
                    
                    # Insert v√†o DimDate
                    date_records = []
                    for _, date_record in date_range_df.iterrows():
                        date_records.append(date_record.to_dict())
                    
                    date_count = batch_insert_records(
                        duck_conn, 
                        'DimDate', 
                        date_records, 
                        on_conflict='ON CONFLICT (date_id) DO NOTHING'
                    )
                    
                    # X·ª≠ l√Ω fact table
                    logger.info("X·ª≠ l√Ω FactJobPostingDaily v√† FactJobLocationBridge")
                    fact_records, _ = generate_fact_records_with_retry(fact_handler, staging_batch)

                    # REMOVED: processed_ids not used
                    
                    # T√≥m t·∫Øt k·∫øt qu·∫£
                    dim_stats = {
                        'DimJob': job_stats,
                        'DimCompany': company_stats,
                        'DimLocation': location_stats,
                        'DimDate': {'inserted': date_count, 'updated': 0, 'unchanged': 0}
                    }
                    
                    # Commit transaction
                    duck_conn.execute("COMMIT")
                    logger.info("Transaction ƒë√£ ƒë∆∞·ª£c commit th√†nh c√¥ng")
                    
                    # K·∫øt th√∫c
                    duration = (datetime.now() - start_time).total_seconds()
                    logger.info(f"‚úÖ ETL batch ho√†n th√†nh trong {duration:.2f} gi√¢y")
                    
                    return {
                        "success": True,
                        "message": f"ETL batch ho√†n th√†nh th√†nh c√¥ng trong {duration:.2f} gi√¢y",
                        "source_count": len(staging_batch),
                        "fact_count": len(fact_records),
                        "dim_stats": dim_stats,
                        "duration_seconds": duration
                    }
                
                except Exception as e:
                    # Rollback transaction n·∫øu c√≥ l·ªói
                    duck_conn.execute("ROLLBACK")
                    logger.error(f"L·ªói trong qu√° tr√¨nh ETL batch, ƒë√£ rollback: {e}", exc_info=True)
                    return {
                        "success": False,
                        "message": f"L·ªói trong qu√° tr√¨nh ETL batch: {str(e)}",
                        "source_count": len(staging_batch),
                        "fact_count": 0,
                        "duration_seconds": (datetime.now() - start_time).total_seconds()
                    }
                    
            except Exception as e:
                # ƒê·∫£m b·∫£o transaction ƒë∆∞·ª£c rollback n·∫øu c√≥ l·ªói
                try:
                    duck_conn.execute("ROLLBACK")
                except:
                    pass
                    
                logger.error(f"L·ªói khi x·ª≠ l√Ω transaction ETL batch: {e}", exc_info=True)
                return {
                    "success": False,
                    "message": f"L·ªói khi x·ª≠ l√Ω transaction ETL batch: {str(e)}",
                    "source_count": len(staging_batch),
                    "fact_count": 0,
                    "duration_seconds": (datetime.now() - start_time).total_seconds()
                }
                
    except Exception as e:
        logger.error(f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh ETL batch: {e}", exc_info=True)
        return {
            "success": False,
            "message": f"L·ªói nghi√™m tr·ªçng: {str(e)}",
            "source_count": len(staging_batch),
            "fact_count": 0,
            "duration_seconds": (datetime.now() - start_time).total_seconds()
        }

def main():
    """
    H√†m main ƒë·ªÉ ch·∫°y ETL t·ª´ command line
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='ETL t·ª´ Staging sang Data Warehouse')
    parser.add_argument('--days', type=int, default=7, help='S·ªë ng√†y d·ªØ li·ªáu c·∫ßn l·∫•y (m·∫∑c ƒë·ªãnh: 7)')
    args = parser.parse_args()
    
    # T√≠nh ng√†y b·∫Øt ƒë·∫ßu ETL
    last_etl_date = datetime.now() - timedelta(days=args.days)
    
    # Ch·∫°y ETL
    etl_result = run_staging_to_dwh_etl(last_etl_date)
    
    # Ki·ªÉm tra k·∫øt qu·∫£
    if etl_result.get("success", False):
        logger.info("‚úÖ ETL HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
        sys.exit(0)
    else:
        logger.error(f"‚ùå ETL TH·∫§T B·∫†I: {etl_result.get('message', 'Unknown error')}")
        sys.exit(1)

if __name__ == "__main__":
    main() 