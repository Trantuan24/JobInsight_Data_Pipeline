#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ETL module ch√≠nh cho vi·ªác chuy·ªÉn d·ªØ li·ªáu t·ª´ Staging sang Data Warehouse (DuckDB)
Phi√™n b·∫£n c·∫£i ti·∫øn v·ªõi c·∫•u tr√∫c module r√µ r√†ng
"""
import pandas as pd
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any

# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† logging
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
sys.path.insert(0, PROJECT_ROOT)

# ƒê·∫£m b·∫£o th∆∞ m·ª•c logs t·ªìn t·∫°i
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "etl.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Import c√°c module c·∫ßn thi·∫øt
from src.utils.config import DUCKDB_PATH, DWH_STAGING_SCHEMA, STAGING_JOBS_TABLE
from src.utils.db import get_dataframe, execute_query

from src.etl.etl_utils import get_duckdb_connection, setup_duckdb_schema, batch_insert_records
from src.etl.dimension_handler import DimensionHandler
from src.etl.fact_handler import FactHandler
from src.etl.partitioning import PartitionManager

try:
    from src.processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range
    )
except ImportError:
    from processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range
    )

def get_staging_batch(last_etl_date: datetime) -> pd.DataFrame:
    """
    L·∫•y batch d·ªØ li·ªáu t·ª´ staging jobs k·ªÉ t·ª´ l·∫ßn ETL g·∫ßn nh·∫•t
    
    Args:
        last_etl_date: Timestamp c·ªßa l·∫ßn ETL g·∫ßn nh·∫•t
        
    Returns:
        DataFrame ch·ª©a b·∫£n ghi c·∫ßn x·ª≠ l√Ω
    """
    try:
        logger.info(f"Truy v·∫•n d·ªØ li·ªáu t·ª´ b·∫£ng {STAGING_JOBS_TABLE}")
        
        query = f"""
            SELECT *
            FROM {STAGING_JOBS_TABLE}
            WHERE crawled_at >= %s
            OR
            (crawled_at IS NOT NULL AND %s IS NULL)
        """
        
        # Ki·ªÉm tra xem b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
        table_exists_query = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{DWH_STAGING_SCHEMA}' 
                AND table_name = 'staging_jobs'
            );
        """
        
        # Th·ª±c hi·ªán ki·ªÉm tra
        exists = execute_query(table_exists_query, fetch=True)
        
        if not exists or not exists[0].get('exists', False):
            logger.error(f"B·∫£ng {STAGING_JOBS_TABLE} kh√¥ng t·ªìn t·∫°i!")
            # Tr·∫£ v·ªÅ DataFrame r·ªóng n·∫øu b·∫£ng kh√¥ng t·ªìn t·∫°i
            return pd.DataFrame()
        
        df = get_dataframe(query, params=(last_etl_date, last_etl_date))
        logger.info(f"ƒê√£ l·∫•y {len(df)} b·∫£n ghi t·ª´ staging (t·ª´ {last_etl_date})")
        
        # Log c√°c c·ªôt ƒë·ªÉ debug
        if not df.empty:
            logger.info(f"C√°c c·ªôt c√≥ trong d·ªØ li·ªáu: {list(df.columns)}")
        
        return df
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y d·ªØ li·ªáu t·ª´ staging: {e}", exc_info=True)
        # Tr·∫£ v·ªÅ DataFrame r·ªóng trong tr∆∞·ªùng h·ª£p l·ªói
        return pd.DataFrame()

def verify_etl_integrity(staging_count: int, fact_count: int, threshold: float = 0.9) -> bool:
    """
    Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa qu√° tr√¨nh ETL Staging to DWH
    
    Args:
        staging_count: S·ªë b·∫£n ghi staging ƒë·∫ßu v√†o
        fact_count: S·ªë b·∫£n ghi fact ƒë√£ t·∫°o
        threshold: Ng∆∞·ª°ng ch·∫•p nh·∫≠n (% d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng)
        
    Returns:
        bool: True n·∫øu t·ª∑ l·ªá d·ªØ li·ªáu chuy·ªÉn ƒë·ªïi ƒë·∫°t threshold
    """
    if staging_count == 0:
        logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ngu·ªìn ƒë·ªÉ x·ª≠ l√Ω")
        return True
    
    # M·ªói staging record c√≥ th·ªÉ t·∫°o ra nhi·ªÅu fact record (m·ªói ng√†y m·ªôt record)
    # N√™n ki·ªÉm tra xem c√≥ fact records ƒë∆∞·ª£c t·∫°o kh√¥ng, kh√¥ng so s√°nh s·ªë l∆∞·ª£ng
    if fact_count == 0:
        logger.error("Kh√¥ng c√≥ fact record n√†o ƒë∆∞·ª£c t·∫°o t·ª´ staging data!")
        return False
    
    logger.info(f"ƒê√£ t·∫°o {fact_count} fact records t·ª´ {staging_count} staging records")
    return True

def run_staging_to_dwh_etl(last_etl_date: Optional[datetime] = None) -> Dict[str, Any]:
    """
    Ch·∫°y ETL t·ª´ staging sang DWH
    
    Args:
        last_etl_date: Ng√†y ch·∫°y ETL g·∫ßn nh·∫•t, l·∫•y d·ªØ li·ªáu t·ª´ ng√†y n√†y ƒë·∫øn hi·ªán t·∫°i
        
    Returns:
        Dict th√¥ng tin v·ªÅ k·∫øt qu·∫£ ETL
    """
    start_time = datetime.now()
    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu ETL Staging to DWH...")
    
    try:
        # 1. L·∫•y d·ªØ li·ªáu t·ª´ staging
        if last_etl_date is None:
            last_etl_date = datetime.now() - timedelta(days=7)
            
        logger.info(f"L·∫•y d·ªØ li·ªáu t·ª´ staging t·ª´ {last_etl_date}...")
        staging_batch = get_staging_batch(last_etl_date)
        
        if staging_batch.empty:
            logger.info("Kh√¥ng c√≥ b·∫£n ghi n√†o ƒë·ªÉ x·ª≠ l√Ω t·ª´ staging")
            return {
                "success": True,
                "message": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω",
                "source_count": 0,
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
            
        logger.info(f"ƒê√£ l·∫•y {len(staging_batch)} b·∫£n ghi t·ª´ staging")
        
        # 2. Ki·ªÉm tra file DuckDB
        if os.path.exists(DUCKDB_PATH):
            logger.info(f"üìÅ S·ª≠ d·ª•ng DuckDB hi·ªán c√≥: {DUCKDB_PATH}")
        else:
            logger.info(f"üÜï T·∫°o DuckDB m·ªõi: {DUCKDB_PATH}")
        
        # 3. Thi·∫øt l·∫≠p schema v√† b·∫£ng (gi·ªØ nguy√™n d·ªØ li·ªáu c≈©)
        if not setup_duckdb_schema():
            return {
                "success": False,
                "message": "Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema DuckDB",
                "source_count": len(staging_batch),
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
        
        # 4. K·∫øt n·ªëi DuckDB v√† th·ª±c hi·ªán ETL v·ªõi SCD Type 2
        with get_duckdb_connection(DUCKDB_PATH) as duck_conn:
            # Kh·ªüi t·∫°o c√°c handler
            dim_handler = DimensionHandler(duck_conn)
            fact_handler = FactHandler(duck_conn)
            partition_manager = PartitionManager(duck_conn)
            
            # Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu ban ƒë·∫ßu
            logger.info("üîç Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu ban ƒë·∫ßu...")
            initial_integrity_check = duck_conn.execute("""
                SELECT 
                    (SELECT COUNT(*) FROM FactJobPostingDaily) as total_facts,
                    (SELECT COUNT(*) FROM (SELECT DISTINCT job_sk, date_id FROM FactJobPostingDaily)) as unique_combinations
            """).fetchone()
            
            if initial_integrity_check and initial_integrity_check[0] != initial_integrity_check[1]:
                logger.warning(f"‚ö†Ô∏è Ph√°t hi·ªán v·∫•n ƒë·ªÅ tr√πng l·∫∑p d·ªØ li·ªáu: {initial_integrity_check[0]} facts, {initial_integrity_check[1]} unique combinations")
            
            # D·ªçn d·∫πp duplicate records hi·ªán c√≥
            logger.info("üßπ D·ªçn d·∫πp duplicate records hi·ªán c√≥...")
            cleanup_result = fact_handler.cleanup_duplicate_fact_records()
            if not cleanup_result.get('success', False):
                logger.warning(f"‚ö†Ô∏è C√≥ v·∫•n ƒë·ªÅ trong qu√° tr√¨nh d·ªçn d·∫πp: {cleanup_result.get('message', 'Unknown error')}")
            
            # Ki·ªÉm tra l·∫°i sau khi d·ªçn d·∫πp
            post_cleanup_check = duck_conn.execute("""
                SELECT COUNT(*) FROM (
                    SELECT job_sk, date_id, COUNT(*) 
                    FROM FactJobPostingDaily 
                    GROUP BY job_sk, date_id 
                    HAVING COUNT(*) > 1
                )
            """).fetchone()
            
            if post_cleanup_check and post_cleanup_check[0] > 0:
                logger.warning(f"‚ö†Ô∏è V·∫´n c√≤n {post_cleanup_check[0]} nh√≥m duplicate sau khi d·ªçn d·∫πp - c·∫ßn ph√¢n t√≠ch s√¢u h∆°n!")
                
                # Th·ª≠ s·ª≠a ch·ªØa b·∫±ng c√°ch force cleanup
                logger.info("Th·ª±c hi·ªán force cleanup ƒë·ªÉ s·ª≠a ch·ªØa d·ªØ li·ªáu...")
                duck_conn.execute("""
                    BEGIN TRANSACTION;
                    
                    -- T·∫°o b·∫£ng t·∫°m ƒë·ªÉ l∆∞u unique records
                    CREATE TEMP TABLE unique_facts AS
                    SELECT * FROM (
                        SELECT *, ROW_NUMBER() OVER (PARTITION BY job_sk, date_id ORDER BY fact_id) as rn
                        FROM FactJobPostingDaily
                    ) WHERE rn = 1;
                    
                    -- X√≥a h·∫øt bridge tables
                    DELETE FROM FactJobLocationBridge;
                    
                    -- X√≥a h·∫øt facts
                    DELETE FROM FactJobPostingDaily;
                    
                    -- Insert l·∫°i unique facts (b·ªè c·ªôt rn)
                    INSERT INTO FactJobPostingDaily 
                    SELECT fact_id, job_sk, company_sk, date_id, 
                           salary_min, salary_max, salary_type, 
                           due_date, time_remaining, verified_employer, 
                           posted_time, crawled_at, load_month
                    FROM unique_facts;
                    
                    -- X√≥a b·∫£ng t·∫°m
                    DROP TABLE unique_facts;
                    
                    COMMIT;
                """)
                
                # Ki·ªÉm tra l·∫°i sau khi force cleanup
                final_check = duck_conn.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM FactJobPostingDaily) as total_facts,
                        (SELECT COUNT(*) FROM (SELECT DISTINCT job_sk, date_id FROM FactJobPostingDaily)) as unique_combinations
                """).fetchone()
                
                if final_check and final_check[0] == final_check[1]:
                    logger.info(f"‚úÖ Force cleanup th√†nh c√¥ng: {final_check[0]} facts, {final_check[1]} unique combinations")
                else:
                    logger.error(f"‚ùå Force cleanup th·∫•t b·∫°i: {final_check[0]} facts, {final_check[1]} unique combinations")
            
            # 5. X·ª≠ l√Ω v√† insert d·ªØ li·ªáu v·ªõi SCD Type 2
            dim_stats = {}
            
            # 5.1 DimJob v·ªõi SCD Type 2
            dim_stats['DimJob'] = dim_handler.process_dimension_with_scd2(
                staging_batch, 'DimJob', prepare_dim_job,
                'job_id', 'job_sk', ['title_clean', 'skills', 'job_url']
            )
        
            # 5.2. DimCompany v·ªõi SCD Type 2
            dim_stats['DimCompany'] = dim_handler.process_dimension_with_scd2(
                staging_batch, 'DimCompany', prepare_dim_company,
                'company_name_standardized', 'company_sk', ['company_url', 'verified_employer']
            )
        
            # 5.3. DimLocation - x·ª≠ l√Ω ƒë·∫∑c bi·ªát v√¨ composite key
            dim_stats['DimLocation'] = dim_handler.process_location_dimension(
                staging_batch, prepare_dim_location
            )
        
            # 5.4. ƒê·∫£m b·∫£o b·∫£ng DimDate c√≥ ƒë·∫ßy ƒë·ªß c√°c ng√†y c·∫ßn thi·∫øt
            logger.info("X·ª≠ l√Ω DimDate")
            start_date = (datetime.now() - timedelta(days=60)).date()
            end_date = (datetime.now() + timedelta(days=240)).date()
            
            date_df = generate_date_range(start_date, end_date)
            
            # Filter out existing dates
            new_date_records = []
            for _, date_record in date_df.iterrows():
                date_dict = date_record.to_dict()
                exists = duck_conn.execute(f"SELECT 1 FROM DimDate WHERE date_id = ?", [date_dict['date_id']]).fetchone()
                if not exists:
                    new_date_records.append(date_dict)
            
            dim_stats['DimDate'] = {
                'inserted': batch_insert_records(duck_conn, 'DimDate', new_date_records),
                'updated': 0,
                'unchanged': len(date_df) - len(new_date_records)
            }
        
            # 5.5. Insert d·ªØ li·ªáu v√†o FactJobPostingDaily v√† FactJobLocationBridge
            logger.info("X·ª≠ l√Ω FactJobPostingDaily v√† FactJobLocationBridge")
            
            # Th·ª±c hi·ªán transaction ri√™ng cho vi·ªác t·∫°o fact records
            try:
                # T·∫°o fact records m√† kh√¥ng s·ª≠ d·ª•ng transaction n·ªôi b·ªô
                fact_records, bridge_records = fact_handler.generate_fact_records(staging_batch)
                logger.info(f"ƒê√£ x·ª≠ l√Ω {len(fact_records)} b·∫£n ghi fact v√† {len(bridge_records)} b·∫£n ghi bridge")
            except Exception as e:
                logger.error(f"L·ªói khi t·∫°o fact records: {str(e)}")
                raise
            
            # Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa d·ªØ li·ªáu
            if not verify_etl_integrity(len(staging_batch), len(fact_records)):
                logger.warning("‚ö†Ô∏è Ph√°t hi·ªán v·∫•n ƒë·ªÅ v·ªÅ t√≠nh to√†n v·∫πn d·ªØ li·ªáu trong qu√° tr√¨nh ETL!")
            
            # Ki·ªÉm tra duplicate m·ªôt l·∫ßn n·ªØa
            duplicate_check = duck_conn.execute("""
                SELECT COUNT(*) FROM (
                    SELECT job_sk, date_id, COUNT(*) 
                    FROM FactJobPostingDaily 
                    GROUP BY job_sk, date_id 
                    HAVING COUNT(*) > 1
                )
            """).fetchone()
            
            if duplicate_check and duplicate_check[0] > 0:
                logger.warning(f"‚ö†Ô∏è Ph√°t hi·ªán {duplicate_check[0]} nh√≥m duplicate sau khi insert - c·∫ßn ch·∫°y l·∫°i cleanup!")
                cleanup_result_2 = fact_handler.cleanup_duplicate_fact_records()
                if not cleanup_result_2.get('success', False):
                    logger.warning(f"‚ö†Ô∏è V·∫´n c√≥ v·∫•n ƒë·ªÅ trong cleanup: {cleanup_result_2.get('message', 'Unknown error')}")
            
            # 6. Qu·∫£n l√Ω partition v√† export sang Parquet
            logger.info("üìä Qu·∫£n l√Ω partition v√† export sang Parquet")
            partition_result = partition_manager.manage_partitions()
            
            # 7. T·ªïng k·∫øt ETL
            logger.info("="*60)
            logger.info("üìä T·ªîNG K·∫æT ETL STAGING TO DWH")
            logger.info("="*60)
            
            total_inserted = sum(stats.get('inserted', 0) for stats in dim_stats.values())
            total_updated = sum(stats.get('updated', 0) for stats in dim_stats.values())
            total_unchanged = sum(stats.get('unchanged', 0) for stats in dim_stats.values())
            
            for table, stats in dim_stats.items():
                logger.info(f"{table:15} - Insert: {stats['inserted']:5}, Update: {stats['updated']:5}, Unchanged: {stats['unchanged']:5}")
            
            logger.info("-"*60)
            logger.info(f"T·ªîNG DIM        - Insert: {total_inserted:5}, Update: {total_updated:5}, Unchanged: {total_unchanged:5}")
            
            # Log partition stats
            if partition_result.get('success', False):
                logger.info(f"PARTITION       - Th√†nh c√¥ng: {partition_result.get('message', '')}")
            else:
                logger.warning(f"PARTITION       - Th·∫•t b·∫°i: {partition_result.get('message', '')}")
            
            # 8. Validation v√† Data Quality Check
            logger.info("üîç B·∫Øt ƒë·∫ßu validation ETL...")
            validation_success = True
            validation_message = ""
            
            try:
                from src.utils.etl_validator import generate_etl_report, log_validation_results
                validation_results = generate_etl_report(duck_conn)
                log_validation_results(validation_results)
                
                # Ki·ªÉm tra c√°c v·∫•n ƒë·ªÅ nghi√™m tr·ªçng
                if validation_results.get('issues', {}).get('critical', 0) > 0:
                    validation_success = False
                    validation_message = f"Ph√°t hi·ªán {validation_results['issues']['critical']} v·∫•n ƒë·ªÅ nghi√™m tr·ªçng trong validation"
                    logger.error(validation_message)
            except ImportError:
                logger.warning("Kh√¥ng th·ªÉ import etl_validator - b·ªè qua validation")
            except Exception as e:
                validation_success = False
                validation_message = f"L·ªói khi th·ª±c hi·ªán validation: {str(e)}"
                logger.error(validation_message)
        
        # T√≠nh th·ªùi gian ch·∫°y
        duration = (datetime.now() - start_time).total_seconds()
        
        # T·ªïng k·∫øt
        logger.info("="*60)
        logger.info(f"‚úÖ ETL HO√ÄN TH√ÄNH TRONG {duration:.2f} GI√ÇY!")
        logger.info("="*60)
        
        # Th·ªëng k√™ k·∫øt qu·∫£ ETL
        etl_stats = {
            "success": True,
            "source_count": len(staging_batch),
            "fact_count": len(fact_records),
            "bridge_count": len(bridge_records),
            "dim_stats": dim_stats,
            "total_dim_inserted": total_inserted,
            "total_dim_updated": total_updated,
            "partition_success": partition_result.get('success', False),
            "duration_seconds": duration,
            "validation_success": validation_success,
            "validation_message": validation_message
        }
        
        return etl_stats
    
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        error_msg = f"L·ªói trong qu√° tr√¨nh ETL: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        return {
            "success": False,
            "message": error_msg,
            "duration_seconds": duration
        }

def main():
    """
    H√†m main ƒë·ªÉ ch·∫°y ETL t·ª´ command line
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='ETL t·ª´ Staging sang Data Warehouse')
    parser.add_argument('--days', type=int, default=7, help='S·ªë ng√†y d·ªØ li·ªáu c·∫ßn l·∫•y (m·∫∑c ƒë·ªãnh: 7)')
    args = parser.parse_args()
    
    # T√≠nh ng√†y b·∫Øt ƒë·∫ßu ETL
    last_etl_date = datetime.now() - timedelta(days=args.days)
    
    # Ch·∫°y ETL
    etl_result = run_staging_to_dwh_etl(last_etl_date)
    
    # Ki·ªÉm tra k·∫øt qu·∫£
    if etl_result.get("success", False):
        logger.info("‚úÖ ETL HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
        sys.exit(0)
    else:
        logger.error(f"‚ùå ETL TH·∫§T B·∫†I: {etl_result.get('message', 'Unknown error')}")
        sys.exit(1)

if __name__ == "__main__":
    main() 