#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ETL module cho vi·ªác chuy·ªÉn d·ªØ li·ªáu t·ª´ Staging sang Data Warehouse (DuckDB)
Fixed version v·ªõi logic parsing location m·ªõi
"""
import pandas as pd
import logging
import json
import os
import sys
import duckdb
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any

# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† logging
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
sys.path.insert(0, PROJECT_ROOT)

# ƒê·∫£m b·∫£o th∆∞ m·ª•c logs t·ªìn t·∫°i
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "etl.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

from src.utils.logger import get_logger
from src.utils.db import get_connection, get_dataframe, execute_query
from src.utils.config import DUCKDB_PATH, DWH_STAGING_SCHEMA, STAGING_JOBS_TABLE

try:
    from src.processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range, parse_single_location_item, parse_job_location,
        check_dimension_changes, apply_scd_type2_updates, 
        generate_daily_fact_records, calculate_load_month
    )
except ImportError:
    from processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range, parse_single_location_item, parse_job_location,
        check_dimension_changes, apply_scd_type2_updates, 
        generate_daily_fact_records, calculate_load_month
    )

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c SQL
SQL_DIR = os.path.join(PROJECT_ROOT, "sql")
if not os.path.exists(SQL_DIR):
    SQL_DIR = os.path.join(os.getcwd(), "sql")
    os.makedirs(SQL_DIR, exist_ok=True)

def get_duckdb_connection(duckdb_path: str = DUCKDB_PATH) -> duckdb.DuckDBPyConnection:
    """ K·∫øt n·ªëi ƒë·∫øn DuckDB """
    # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n l√† tuy·ªát ƒë·ªëi
    if not os.path.isabs(duckdb_path):
        duckdb_path = os.path.join(PROJECT_ROOT, duckdb_path)
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c cha t·ªìn t·∫°i
    parent_dir = os.path.dirname(duckdb_path)
    os.makedirs(parent_dir, exist_ok=True)
    
    logger.info(f"K·∫øt n·ªëi DuckDB t·∫°i: {duckdb_path}")
    
    return duckdb.connect(duckdb_path)

def execute_sql_file_duckdb(sql_file_path, conn=None):
    """Th·ª±c thi file SQL tr√™n DuckDB v·ªõi connection truy·ªÅn v√†o (ho·∫∑c t·ª± t·∫°o)"""
    try:
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_script = f.read()
        # N·∫øu kh√¥ng truy·ªÅn conn th√¨ t·ª± t·∫°o v√† t·ª± ƒë√≥ng, c√≤n truy·ªÅn v√†o th√¨ kh√¥ng ƒë√≥ng
        close_conn = False
        if conn is None:
            conn = get_duckdb_connection()
            close_conn = True
        conn.execute(sql_script)
        if close_conn:
            conn.close()
        logger.info(f"ƒê√£ th·ª±c thi file SQL: {sql_file_path}")
        return True
    except Exception as e:
        logger.error(f"L·ªói khi th·ª±c thi file SQL {sql_file_path}: {str(e)}")
        return False

def setup_duckdb_schema():
    """Thi·∫øt l·∫≠p schema v√† b·∫£ng cho DuckDB"""
    try:
        with get_duckdb_connection() as conn:
            # T·∫°o schema n·∫øu ch∆∞a c√≥ (DuckDB s·∫Ω kh√¥ng b√°o l·ªói n·∫øu ƒë√£ t·ªìn t·∫°i)
            conn.execute(f"CREATE SCHEMA IF NOT EXISTS {DWH_STAGING_SCHEMA};")
            logger.info(f"ƒê√£ ƒë·∫£m b·∫£o t·ªìn t·∫°i schema: {DWH_STAGING_SCHEMA}")

            # Th·ª±c thi file schema_dwh.sql ƒë·ªÉ t·∫°o b·∫£ng v√† ch·ªâ m·ª•c
            schema_dwh = os.path.join(SQL_DIR, "schema_dwh.sql")
            if os.path.exists(schema_dwh):
                if not execute_sql_file_duckdb(schema_dwh, conn=conn):
                    logger.error("Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema v√† b·∫£ng!")
                    return False
            else:
                logger.error(f"Kh√¥ng t√¨m th·∫•y file schema: {schema_dwh}")
                return False

            # Ki·ªÉm tra xem c√°c b·∫£ng ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng ch∆∞a
            tables_in_db = [row[0] for row in conn.execute("SHOW TABLES").fetchall()]
            for table in ['DimJob', 'DimCompany', 'DimLocation', 'DimDate', 'FactJobPostingDaily', 'FactJobLocationBridge']:
                if table in tables_in_db:
                    logger.info(f"‚úì B·∫£ng {table} ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")
                else:
                    logger.warning(f"‚úó B·∫£ng {table} KH√îNG ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")

        logger.info("ƒê√£ thi·∫øt l·∫≠p schema v√† b·∫£ng database th√†nh c√¥ng cho DuckDB!")
        return True
    except Exception as e:
        logger.error(f"L·ªói khi thi·∫øt l·∫≠p schema database v·ªõi DuckDB: {str(e)}")
        return False
    

def get_staging_batch(last_etl_date: datetime) -> pd.DataFrame:
    """
    L·∫•y batch d·ªØ li·ªáu t·ª´ staging jobs k·ªÉ t·ª´ l·∫ßn ETL g·∫ßn nh·∫•t
    
    Args:
        last_etl_date: Timestamp c·ªßa l·∫ßn ETL g·∫ßn nh·∫•t
        
    Returns:
        DataFrame ch·ª©a b·∫£n ghi c·∫ßn x·ª≠ l√Ω
    """
    try:
        logger.info(f"Truy v·∫•n d·ªØ li·ªáu t·ª´ b·∫£ng {STAGING_JOBS_TABLE}")
        
        query = f"""
            SELECT *
            FROM {STAGING_JOBS_TABLE}
            WHERE crawled_at >= %s
            OR
            (crawled_at IS NOT NULL AND %s IS NULL)
        """
        
        # Ki·ªÉm tra xem b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
        table_exists_query = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{DWH_STAGING_SCHEMA}' 
                AND table_name = 'staging_jobs'
            );
        """
        
        # Th·ª±c hi·ªán ki·ªÉm tra
        exists = execute_query(table_exists_query, fetch=True)
        
        if not exists or not exists[0].get('exists', False):
            logger.error(f"B·∫£ng {STAGING_JOBS_TABLE} kh√¥ng t·ªìn t·∫°i!")
            # Tr·∫£ v·ªÅ DataFrame r·ªóng n·∫øu b·∫£ng kh√¥ng t·ªìn t·∫°i
            return pd.DataFrame()
        
        df = get_dataframe(query, params=(last_etl_date, last_etl_date))
        logger.info(f"ƒê√£ l·∫•y {len(df)} b·∫£n ghi t·ª´ staging (t·ª´ {last_etl_date})")
        
        # Log c√°c c·ªôt ƒë·ªÉ debug
        if not df.empty:
            logger.info(f"C√°c c·ªôt c√≥ trong d·ªØ li·ªáu: {list(df.columns)}")
        
        return df
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y d·ªØ li·ªáu t·ª´ staging: {e}", exc_info=True)
        # Tr·∫£ v·ªÅ DataFrame r·ªóng trong tr∆∞·ªùng h·ª£p l·ªói
        return pd.DataFrame()
    
def lookup_location_key(
    duck_conn: duckdb.DuckDBPyConnection,
    province: str = None,
    city: str = None,
    district: str = None
) -> Optional[int]:
    """
    T√¨m location_sk t·ª´ b·∫£ng DimLocation d·ª±a tr√™n province, city, district
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        province: T√™n t·ªânh
        city: T√™n th√†nh ph·ªë
        district: T√™n qu·∫≠n/huy·ªán
    
    Returns:
        location_sk ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        # X√¢y d·ª±ng query ƒë·ªông d·ª±a tr√™n c√°c tham s·ªë c√≥ gi√° tr·ªã
        conditions = ["is_current = TRUE"]
        params = []
        
        if province is not None:
            conditions.append("province = ?")
            params.append(province)
        else:
            conditions.append("province IS NULL")
            
        if city is not None:
            conditions.append("city = ?")
            params.append(city)
        else:
            conditions.append("city IS NULL")
            
        if district is not None:
            conditions.append("district = ?")
            params.append(district)
        else:
            conditions.append("district IS NULL")
        
        query = f"""
            SELECT location_sk
            FROM DimLocation
            WHERE {' AND '.join(conditions)}
            LIMIT 1
        """
        
        result = duck_conn.execute(query, params).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"L·ªói khi t√¨m location_sk: {e}")
        return None

def lookup_dimension_key(
    duck_conn: duckdb.DuckDBPyConnection,
    dim_table: str,
    key_column: str,
    key_value: Any,
    surrogate_key_col: str
) -> Optional[int]:
    """
    T√¨m surrogate key t·ª´ b·∫£ng dimension
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        dim_table: T√™n b·∫£ng dimension
        key_column: T√™n c·ªôt d√πng ƒë·ªÉ t√¨m ki·∫øm
        key_value: Gi√° tr·ªã c·∫ßn t√¨m
        surrogate_key_col: T√™n c·ªôt surrogate key
    
    Returns:
        Surrogate key ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        query = f"""
            SELECT {surrogate_key_col}
            FROM {dim_table}
            WHERE {key_column} = ?
            AND is_current = TRUE
            LIMIT 1
        """
        
        result = duck_conn.execute(query, [key_value]).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"L·ªói khi t√¨m kh√≥a trong {dim_table}: {e}")
        return None

def batch_insert_records(duck_conn: duckdb.DuckDBPyConnection, table_name: str, records: List[Dict], batch_size: int = 1000, upsert_on_conflict: str = None):
    """
    Batch insert records ƒë·ªÉ t·ªëi ∆∞u performance
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        table_name: T√™n b·∫£ng
        records: List c√°c records c·∫ßn insert
        batch_size: K√≠ch th∆∞·ªõc batch
        upsert_on_conflict: Column(s) ƒë·ªÉ UPSERT (e.g., "(fact_id, location_sk)")
    """
    if not records:
        return 0
        
    inserted_count = 0
    
    # Chia records th√†nh c√°c batch
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        
        try:
            # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho FactJobLocationBridge v·ªõi duplicate check
            if table_name == 'FactJobLocationBridge':
                for record in batch:
                    fact_id = record['fact_id']
                    location_sk = record['location_sk']
                    
                    # Ki·ªÉm tra xem bridge record ƒë√£ t·ªìn t·∫°i ch∆∞a
                    check_query = """
                        SELECT 1 FROM FactJobLocationBridge 
                        WHERE fact_id = ? AND location_sk = ?
                    """
                    exists = duck_conn.execute(check_query, [fact_id, location_sk]).fetchone()
                    
                    if not exists:
                        insert_query = """
                            INSERT INTO FactJobLocationBridge (fact_id, location_sk)
                            VALUES (?, ?)
                        """
                        duck_conn.execute(insert_query, [fact_id, location_sk])
                        inserted_count += 1
                        
            else:
                # X·ª≠ l√Ω th√¥ng th∆∞·ªùng cho c√°c b·∫£ng kh√°c
                # Chu·∫©n b·ªã d·ªØ li·ªáu batch
                df_batch = pd.DataFrame(batch)
                
                # Handle JSON columns
                for col in df_batch.columns:
                    df_batch[col] = df_batch[col].apply(
                        lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x
                    )
                
                # Insert batch
                duck_conn.execute(f"INSERT INTO {table_name} SELECT * FROM df_batch")
                inserted_count += len(batch)
            
        except Exception as e:
            logger.warning(f"L·ªói khi batch insert v√†o {table_name}: {e}")
            # Fallback: insert t·ª´ng record
            for record in batch:
                try:
                    columns = list(record.keys())
                    placeholders = ', '.join(['?'] * len(columns))
                    values = [record[col] for col in columns]
                    
                    # Handle JSON values
                    for j, val in enumerate(values):
                        if isinstance(val, (dict, list)):
                            values[j] = json.dumps(val)
                    
                    # Ki·ªÉm tra n·∫øu c√≥ upsert_on_conflict
                    if upsert_on_conflict and table_name == 'FactJobLocationBridge':
                        # Skip n·∫øu ƒë√£ t·ªìn t·∫°i
                        check_query = f"SELECT 1 FROM {table_name} WHERE fact_id = ? AND location_sk = ?"
                        exists = duck_conn.execute(check_query, [values[0], values[1]]).fetchone()
                        if exists:
                            continue
                    
                    query = f"""
                        INSERT INTO {table_name} ({', '.join(columns)})
                        VALUES ({placeholders})
                    """
                    duck_conn.execute(query, values)
                    inserted_count += 1
                except Exception as e2:
                    logger.error(f"L·ªói khi insert single record v√†o {table_name}: {e2}")
    
    logger.info(f"ƒê√£ batch insert {inserted_count} records v√†o {table_name}")
    return inserted_count

def process_dimension_with_scd2(
    duck_conn: duckdb.DuckDBPyConnection,
    staging_records: pd.DataFrame,
    dim_table: str,
    prepare_function,
    natural_key: str,
    surrogate_key: str,
    compare_columns: List[str]
) -> Dict[str, int]:
    """
    X·ª≠ l√Ω dimension table v·ªõi SCD Type 2
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        staging_records: D·ªØ li·ªáu staging
        dim_table: T√™n b·∫£ng dimension
        prepare_function: Function chu·∫©n b·ªã d·ªØ li·ªáu
        natural_key: Natural key column
        surrogate_key: Surrogate key column
        compare_columns: Columns ƒë·ªÉ so s√°nh thay ƒë·ªïi
    
    Returns:
        Dict th·ªëng k√™ insert/update
    """
    logger.info(f"X·ª≠ l√Ω {dim_table} v·ªõi SCD Type 2")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    prepared_data = prepare_function(staging_records)
    
    if prepared_data.empty:
        logger.warning(f"Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω cho {dim_table}")
        return {'inserted': 0, 'updated': 0, 'unchanged': 0}
    
    # Ki·ªÉm tra thay ƒë·ªïi
    to_insert, to_update, unchanged = check_dimension_changes(
        duck_conn, prepared_data, dim_table, natural_key, compare_columns
    )
    
    stats = {
        'inserted': 0,
        'updated': 0,
        'unchanged': len(unchanged)
    }
    
    # √Åp d·ª•ng updates (SCD Type 2)
    if to_update:
        apply_scd_type2_updates(duck_conn, dim_table, surrogate_key, to_update)
        stats['updated'] = len(to_update)
    
    # Insert records m·ªõi
    if not to_insert.empty:
        insert_records = []
        for _, record in to_insert.iterrows():
            record_dict = record.to_dict()
            # Lo·∫°i b·ªè surrogate key
            if surrogate_key in record_dict:
                del record_dict[surrogate_key]
            insert_records.append(record_dict)
        
        stats['inserted'] = batch_insert_records(duck_conn, dim_table, insert_records)
    
    logger.info(f"{dim_table} - Inserted: {stats['inserted']}, Updated: {stats['updated']}, Unchanged: {stats['unchanged']}")
    return stats

def generate_fact_records(
    duck_conn: duckdb.DuckDBPyConnection,
    staging_records: pd.DataFrame
) -> Tuple[List[Dict], List[Dict]]:
    """
    T·∫°o b·∫£n ghi fact v√† bridge t·ª´ d·ªØ li·ªáu staging
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        staging_records: D·ªØ li·ªáu t·ª´ staging
    
    Returns:
        Tuple ch·ª©a (fact_records, bridge_records)
    """
    fact_records = []
    bridge_records = []
    
    for _, job in staging_records.iterrows():
        # Lookup dimension keys
        job_sk = lookup_dimension_key(
            duck_conn, 'DimJob', 'job_id', job.job_id, 'job_sk'
        )
        
        company_name = job.company_name_standardized if pd.notna(job.company_name_standardized) else job.company_name
        company_sk = lookup_dimension_key(
            duck_conn, 'DimCompany', 'company_name_standardized', company_name, 'company_sk'
        )
        
        # Check if required keys exist
        if not job_sk or not company_sk:
            logger.warning(f"B·ªè qua job_id={job.job_id}: Kh√¥ng t√¨m th·∫•y dimension key (job_sk={job_sk}, company_sk={company_sk})")
            continue
        
        # X·ª≠ l√Ω ng√†y
        due_date = pd.to_datetime(job.due_date) if pd.notna(job.due_date) else None
        posted_time = pd.to_datetime(job.posted_time) if pd.notna(job.posted_time) else None
        crawled_at = pd.to_datetime(job.crawled_at) if pd.notna(job.crawled_at) else datetime.now()
        
        # T·∫°o danh s√°ch c√°c ng√†y c·∫ßn t·∫°o fact records
        daily_dates = generate_daily_fact_records(posted_time, due_date)
        
        # T√≠nh load_month
        load_month = calculate_load_month(crawled_at)
        
        # T·∫°o fact records cho t·ª´ng ng√†y
        for date_id in daily_dates:
            try:
                # Ki·ªÉm tra xem fact record ƒë√£ t·ªìn t·∫°i ch∆∞a
                check_query = """
                    SELECT fact_id FROM FactJobPostingDaily 
                    WHERE job_sk = ? AND date_id = ?
                """
                existing_fact = duck_conn.execute(check_query, [job_sk, date_id]).fetchone()
                
                if existing_fact:
                    # ƒê√£ t·ªìn t·∫°i, ch·ªâ c·∫≠p nh·∫≠t m·ªôt s·ªë field quan tr·ªçng
                    fact_id = existing_fact[0]
                    
                    update_query = """
                        UPDATE FactJobPostingDaily 
                        SET 
                            time_remaining = ?,
                            crawled_at = ?,
                            load_month = ?
                        WHERE fact_id = ?
                    """
                    
                    update_values = [
                        job.time_remaining if pd.notna(job.time_remaining) else None,
                        crawled_at,
                        load_month,
                        fact_id
                    ]
                    
                    try:
                        duck_conn.execute(update_query, update_values)
                        logger.debug(f"Updated existing fact record: job_sk={job_sk}, date_id={date_id}, fact_id={fact_id}")
                    except Exception as e:
                        logger.error(f"L·ªói khi update fact record cho job_id={job.job_id}, date={date_id}: {e}")
                        continue
                        
                else:
                    # Ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi
                    fact_record = {
                        'job_sk': job_sk,
                        'company_sk': company_sk,
                        'date_id': date_id,
                        'salary_min': job.salary_min if pd.notna(job.salary_min) else None,
                        'salary_max': job.salary_max if pd.notna(job.salary_max) else None,
                        'salary_type': job.salary_type if pd.notna(job.salary_type) else None,
                        'due_date': due_date,
                        'time_remaining': job.time_remaining if pd.notna(job.time_remaining) else None,
                        'verified_employer': job.verified_employer if pd.notna(job.verified_employer) else False,
                        'posted_time': posted_time,
                        'crawled_at': crawled_at,
                        'load_month': load_month
                    }
                
                    # Insert fact record v√† l·∫•y fact_id
                    columns = ', '.join([k for k, v in fact_record.items() if v is not None])
                    placeholders = ', '.join(['?'] * len([v for v in fact_record.values() if v is not None]))
                    values = [v for v in fact_record.values() if v is not None]
                    
                    insert_query = f"""
                        INSERT INTO FactJobPostingDaily ({columns})
                        VALUES ({placeholders})
                        RETURNING fact_id
                    """
                    
                    try:
                        result = duck_conn.execute(insert_query, values).fetchone()
                        if not result:
                            logger.warning(f"Kh√¥ng th·ªÉ insert fact record cho job_id={job.job_id}, date={date_id}")
                            continue
                            
                        fact_id = result[0]
                        fact_records.append(fact_record)
                        logger.debug(f"Inserted new fact record: job_sk={job_sk}, date_id={date_id}, fact_id={fact_id}")
                    
                    except Exception as e:
                        logger.error(f"L·ªói khi insert fact record cho job_id={job.job_id}, date={date_id}: {e}")
                        continue
                    
                # X·ª≠ l√Ω locations v·ªõi c·∫•u tr√∫c m·ªõi (province, city, district)
                location_str = None
                
                # ∆Øu ti√™n s·ª≠ d·ª•ng location_pairs n·∫øu c√≥
                if hasattr(job, 'location_pairs'):
                    try:
                        location_pairs_value = getattr(job, 'location_pairs')
                        if location_pairs_value is not None and str(location_pairs_value).lower() not in ['nan', 'none', '']:
                            location_str = str(location_pairs_value)
                    except:
                        pass
                        
                # Fallback v·ªÅ location n·∫øu kh√¥ng c√≥ location_pairs
                if not location_str and hasattr(job, 'location'):
                    try:
                        location_value = getattr(job, 'location')
                        if location_value is not None and str(location_value).lower() not in ['nan', 'none', '']:
                            location_str = str(location_value)
                    except:
                        pass
                
                # X√≥a bridge records c≈© cho fact_id n√†y (n·∫øu update)
                if existing_fact:
                    duck_conn.execute("DELETE FROM FactJobLocationBridge WHERE fact_id = ?", [fact_id])
                
                if location_str:
                    # Parse location string th√†nh c√°c tuple (province, city, district)
                    logger.debug(f"Parsing location_str: {location_str}")
                    parsed_locations = parse_job_location(location_str)
                    logger.debug(f"Parsed locations: {parsed_locations}")
                    
                    location_sks_added = set()  # Tr√°nh duplicate locations cho c√πng 1 fact_id
                    
                    for province, city, district in parsed_locations:
                        location_sk = lookup_location_key(duck_conn, province, city, district)
                        
                        if location_sk and location_sk not in location_sks_added:
                            bridge_records.append({'fact_id': fact_id, 'location_sk': location_sk})
                            location_sks_added.add(location_sk)
                        elif not location_sk:
                            # N·∫øu kh√¥ng t√¨m th·∫•y exact match, th·ª≠ t√¨m Unknown
                            unknown_location_sk = lookup_location_key(duck_conn, None, 'Unknown', None)
                            if unknown_location_sk and unknown_location_sk not in location_sks_added:
                                bridge_records.append({'fact_id': fact_id, 'location_sk': unknown_location_sk})
                                location_sks_added.add(unknown_location_sk)
                            else:
                                logger.warning(f"Kh√¥ng t√¨m th·∫•y location_sk cho job_id={job.job_id}, location=({province}, {city}, {district})")
                else:
                    # Kh√¥ng c√≥ location, s·ª≠ d·ª•ng Unknown
                    unknown_location_sk = lookup_location_key(duck_conn, None, 'Unknown', None)
                    if unknown_location_sk:
                        bridge_records.append({'fact_id': fact_id, 'location_sk': unknown_location_sk})
                        
            except Exception as e:
                logger.error(f"L·ªói khi insert fact record cho job_id={job.job_id}, date={date_id}: {e}")
                continue
    
    return fact_records, bridge_records


def cleanup_duplicate_fact_records(duck_conn: duckdb.DuckDBPyConnection):
    """
    D·ªçn d·∫πp c√°c duplicate records trong FactJobPostingDaily v√† FactJobLocationBridge
    """
    logger.info("B·∫Øt ƒë·∫ßu d·ªçn d·∫πp duplicate fact records...")
    
    try:
        # 1. Backup bridge records tr∆∞·ªõc khi x√≥a
        logger.info("Backup bridge records...")
        backup_bridge_query = """
            CREATE OR REPLACE TEMP TABLE bridge_backup AS
            SELECT DISTINCT fact_id, location_sk 
            FROM FactJobLocationBridge
        """
        duck_conn.execute(backup_bridge_query)
        
        # 2. T√¨m v√† x√≥a duplicate fact records, gi·ªØ l·∫°i record c√≥ fact_id nh·ªè nh·∫•t
        logger.info("T√¨m duplicate fact records...")
        find_duplicates_query = """
            SELECT job_sk, date_id, COUNT(*) as count, MIN(fact_id) as keep_fact_id
            FROM FactJobPostingDaily
            GROUP BY job_sk, date_id
            HAVING COUNT(*) > 1
        """
        
        duplicates = duck_conn.execute(find_duplicates_query).fetchdf()
        
        if not duplicates.empty:
            logger.info(f"T√¨m th·∫•y {len(duplicates)} nh√≥m duplicate fact records")
            
            total_deleted = 0
            for _, dup in duplicates.iterrows():
                job_sk, date_id, count, keep_fact_id = dup['job_sk'], dup['date_id'], dup['count'], dup['keep_fact_id']
                
                # L·∫•y danh s√°ch fact_id c·∫ßn x√≥a (t·∫•t c·∫£ tr·ª´ keep_fact_id)
                get_delete_ids_query = """
                    SELECT fact_id FROM FactJobPostingDaily
                    WHERE job_sk = ? AND date_id = ? AND fact_id != ?
                """
                delete_ids = duck_conn.execute(get_delete_ids_query, [job_sk, date_id, keep_fact_id]).fetchall()
                
                if delete_ids:
                    fact_ids_to_delete = [row[0] for row in delete_ids]
                    placeholders = ','.join(['?'] * len(fact_ids_to_delete))
                    
                    # X√≥a bridge records tr∆∞·ªõc
                    delete_bridge_query = f"""
                        DELETE FROM FactJobLocationBridge 
                        WHERE fact_id IN ({placeholders})
                    """
                    duck_conn.execute(delete_bridge_query, fact_ids_to_delete)
                    
                    # X√≥a fact records
                    delete_fact_query = f"""
                        DELETE FROM FactJobPostingDaily 
                        WHERE fact_id IN ({placeholders})
                    """
                    duck_conn.execute(delete_fact_query, fact_ids_to_delete)
                    
                    total_deleted += len(fact_ids_to_delete)
                    logger.debug(f"ƒê√£ x√≥a {len(fact_ids_to_delete)} duplicate records cho job_sk={job_sk}, date_id={date_id}")
            
            logger.info(f"ƒê√£ x√≥a t·ªïng c·ªông {total_deleted} duplicate fact records")
            
            # 3. Restore bridge records cho c√°c fact_id c√≤n l·∫°i
            logger.info("Restore bridge records...")
            restore_bridge_query = """
                INSERT INTO FactJobLocationBridge (fact_id, location_sk)
                SELECT DISTINCT b.fact_id, b.location_sk
                FROM bridge_backup b
                JOIN FactJobPostingDaily f ON b.fact_id = f.fact_id
                WHERE NOT EXISTS (
                    SELECT 1 FROM FactJobLocationBridge fb 
                    WHERE fb.fact_id = b.fact_id AND fb.location_sk = b.location_sk
                )
            """
            duck_conn.execute(restore_bridge_query)
            
        else:
            logger.info("Kh√¥ng t√¨m th·∫•y duplicate fact records")
            
        # 4. Th·ªëng k√™ sau khi d·ªçn d·∫πp
        stats_query = """
            SELECT 
                COUNT(*) as total_facts,
                (SELECT COUNT(*) FROM (SELECT DISTINCT job_sk, date_id FROM FactJobPostingDaily)) as unique_combinations
            FROM FactJobPostingDaily
        """
        stats = duck_conn.execute(stats_query).fetchone()
        logger.info(f"Sau d·ªçn d·∫πp: {stats[0]} fact records, {stats[1]} unique combinations")
        
        if stats[0] != stats[1]:
            logger.warning(f"V·∫´n c√≤n {stats[0] - stats[1]} duplicate records!")
        else:
            logger.info("‚úÖ ƒê√£ d·ªçn d·∫πp th√†nh c√¥ng t·∫•t c·∫£ duplicate records")
            
    except Exception as e:
        logger.error(f"L·ªói khi d·ªçn d·∫πp duplicate records: {e}")
        raise

def verify_etl_integrity(staging_count: int, fact_count: int, threshold: float = 0.9) -> bool:
    """
    Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa qu√° tr√¨nh ETL Staging to DWH
    
    Args:
        staging_count: S·ªë b·∫£n ghi staging ƒë·∫ßu v√†o
        fact_count: S·ªë b·∫£n ghi fact ƒë√£ t·∫°o
        threshold: Ng∆∞·ª°ng ch·∫•p nh·∫≠n (% d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng)
        
    Returns:
        bool: True n·∫øu t·ª∑ l·ªá d·ªØ li·ªáu chuy·ªÉn ƒë·ªïi ƒë·∫°t threshold
    """
    if staging_count == 0:
        logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ngu·ªìn ƒë·ªÉ x·ª≠ l√Ω")
        return True
    
    # M·ªói staging record c√≥ th·ªÉ t·∫°o ra nhi·ªÅu fact record (m·ªói ng√†y m·ªôt record)
    # N√™n ki·ªÉm tra xem c√≥ fact records ƒë∆∞·ª£c t·∫°o kh√¥ng, kh√¥ng so s√°nh s·ªë l∆∞·ª£ng
    if fact_count == 0:
        logger.error("Kh√¥ng c√≥ fact record n√†o ƒë∆∞·ª£c t·∫°o t·ª´ staging data!")
        return False
    
    logger.info(f"ƒê√£ t·∫°o {fact_count} fact records t·ª´ {staging_count} staging records")
    return True

def run_staging_to_dwh_etl(last_etl_date: Optional[datetime] = None) -> Dict[str, Any]:
    """
    Th·ª±c hi·ªán quy tr√¨nh ETL chuy·ªÉn d·ªØ li·ªáu t·ª´ Staging sang Data Warehouse
    
    Args:
        last_etl_date: Timestamp c·ªßa l·∫ßn ETL g·∫ßn nh·∫•t, m·∫∑c ƒë·ªãnh l√† 7 ng√†y tr∆∞·ªõc
        
    Returns:
        Dict[str, Any]: K·∫øt qu·∫£ th·ªëng k√™ ETL
    """
    start_time = datetime.now()
    
    try:
        # Thi·∫øt l·∫≠p ng√†y ETL g·∫ßn nh·∫•t n·∫øu kh√¥ng c√≥
        if last_etl_date is None:
            last_etl_date = datetime.now() - timedelta(days=7)
        
        logger.info("="*60)
        logger.info(f"üöÄ B·∫ÆT ƒê·∫¶U ETL STAGING TO DWH - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"üïí L·∫•y d·ªØ li·ªáu t·ª´: {last_etl_date}")
        logger.info("="*60)
        
        # 1. L·∫•y d·ªØ li·ªáu t·ª´ staging
        staging_batch = get_staging_batch(last_etl_date)
        if staging_batch.empty:
            logger.info("Kh√¥ng c√≥ b·∫£n ghi n√†o ƒë·ªÉ x·ª≠ l√Ω t·ª´ staging")
            return {
                "success": True,
                "message": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω",
                "source_count": 0,
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
            
        logger.info(f"ƒê√£ l·∫•y {len(staging_batch)} b·∫£n ghi t·ª´ staging")
        
        # 2. Ki·ªÉm tra file DuckDB
        if os.path.exists(DUCKDB_PATH):
            logger.info(f"üìÅ S·ª≠ d·ª•ng DuckDB hi·ªán c√≥: {DUCKDB_PATH}")
        else:
            logger.info(f"üÜï T·∫°o DuckDB m·ªõi: {DUCKDB_PATH}")
        
        # 3. Thi·∫øt l·∫≠p schema v√† b·∫£ng (gi·ªØ nguy√™n d·ªØ li·ªáu c≈©)
        if not setup_duckdb_schema():
            return {
                "success": False,
                "message": "Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema DuckDB",
                "source_count": len(staging_batch),
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
        
        # 4. K·∫øt n·ªëi DuckDB v√† th·ª±c hi·ªán ETL v·ªõi SCD Type 2
        with get_duckdb_connection(DUCKDB_PATH) as duck_conn:
            # D·ªçn d·∫πp duplicate records hi·ªán c√≥ (ch·∫°y 1 l·∫ßn)
            logger.info("üßπ D·ªçn d·∫πp duplicate records hi·ªán c√≥...")
            cleanup_duplicate_fact_records(duck_conn)
            
            # 5. X·ª≠ l√Ω v√† insert d·ªØ li·ªáu v·ªõi SCD Type 2
            dim_stats = {}
            
            # 5.1 DimJob v·ªõi SCD Type 2
            dim_stats['DimJob'] = process_dimension_with_scd2(
                duck_conn, staging_batch, 'DimJob', prepare_dim_job,
                'job_id', 'job_sk', ['title_clean', 'skills', 'job_url']
            )
        
            # 5.2. DimCompany v·ªõi SCD Type 2
            dim_stats['DimCompany'] = process_dimension_with_scd2(
                duck_conn, staging_batch, 'DimCompany', prepare_dim_company,
                'company_name_standardized', 'company_sk', ['company_url', 'verified_employer']
            )
        
            # 5.3. DimLocation - x·ª≠ l√Ω ƒë·∫∑c bi·ªát v√¨ composite key
            logger.info("X·ª≠ l√Ω DimLocation v·ªõi composite key")
            dim_location_df = prepare_dim_location(staging_batch)
            if not dim_location_df.empty:
                location_records = []
                for _, location in dim_location_df.iterrows():
                    location_dict = location.to_dict()
                    if 'location_sk' in location_dict:
                        del location_dict['location_sk']
                    location_records.append(location_dict)
                
                dim_stats['DimLocation'] = {
                    'inserted': batch_insert_records(duck_conn, 'DimLocation', location_records),
                    'updated': 0,
                    'unchanged': 0
                }
            else:
                dim_stats['DimLocation'] = {'inserted': 0, 'updated': 0, 'unchanged': 0}
        
            # 5.4. ƒê·∫£m b·∫£o b·∫£ng DimDate c√≥ ƒë·∫ßy ƒë·ªß c√°c ng√†y c·∫ßn thi·∫øt
            logger.info("X·ª≠ l√Ω DimDate")
            start_date = (datetime.now() - timedelta(days=60)).date()
            end_date = (datetime.now() + timedelta(days=240)).date()
            
            date_df = generate_date_range(start_date, end_date)
            
            # Filter out existing dates
            new_date_records = []
            for _, date_record in date_df.iterrows():
                date_dict = date_record.to_dict()
                exists = duck_conn.execute(f"SELECT 1 FROM DimDate WHERE date_id = ?", [date_dict['date_id']]).fetchone()
                if not exists:
                    new_date_records.append(date_dict)
            
            dim_stats['DimDate'] = {
                'inserted': batch_insert_records(duck_conn, 'DimDate', new_date_records),
                'updated': 0,
                'unchanged': len(date_df) - len(new_date_records)
            }
        
            # 5.5. Insert d·ªØ li·ªáu v√†o FactJobPostingDaily v√† FactJobLocationBridge
            logger.info("X·ª≠ l√Ω FactJobPostingDaily v√† FactJobLocationBridge")
            fact_records, bridge_records = generate_fact_records(duck_conn, staging_batch)
            
            # Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa d·ªØ li·ªáu
            if not verify_etl_integrity(len(staging_batch), len(fact_records)):
                logger.warning("‚ö†Ô∏è Ph√°t hi·ªán v·∫•n ƒë·ªÅ v·ªÅ t√≠nh to√†n v·∫πn d·ªØ li·ªáu trong qu√° tr√¨nh ETL!")
                # V·∫´n ti·∫øp t·ª•c nh∆∞ng ƒë√£ c·∫£nh b√°o
            
            logger.info(f"ƒê√£ insert {len(fact_records)} b·∫£n ghi v√†o FactJobPostingDaily")
            logger.info(f"Chu·∫©n b·ªã insert {len(bridge_records)} b·∫£n ghi v√†o FactJobLocationBridge")
        
            # Batch insert bridge records v√†o FactJobLocationBridge
            bridge_inserted = batch_insert_records(duck_conn, 'FactJobLocationBridge', bridge_records)
            logger.info(f"ƒê√£ batch insert {bridge_inserted} b·∫£n ghi v√†o FactJobLocationBridge")
        
            # 6. T·ªïng k·∫øt ETL
            logger.info("="*60)
            logger.info("üìä T·ªîNG K·∫æT ETL STAGING TO DWH")
            logger.info("="*60)
            
            total_inserted = sum(stats.get('inserted', 0) for stats in dim_stats.values())
            total_updated = sum(stats.get('updated', 0) for stats in dim_stats.values())
            total_unchanged = sum(stats.get('unchanged', 0) for stats in dim_stats.values())
            
            for table, stats in dim_stats.items():
                logger.info(f"{table:15} - Insert: {stats['inserted']:5}, Update: {stats['updated']:5}, Unchanged: {stats['unchanged']:5}")
            
            logger.info(f"{'FACTS':15} - FactJobPostingDaily: {len(fact_records)} records")
            logger.info(f"{'BRIDGE':15} - FactJobLocationBridge: {bridge_inserted} records")
            logger.info("-"*60)
            logger.info(f"T·ªîNG DIM        - Insert: {total_inserted:5}, Update: {total_updated:5}, Unchanged: {total_unchanged:5}")
            logger.info(f"T·ªîNG FACT/BRIDGE- Records: {len(fact_records) + bridge_inserted}")
            
            # Log load_month stats
            load_months = set()
            if fact_records:
                load_months = set(record.get('load_month') for record in fact_records)
                logger.info(f"Partition load_month: {', '.join(sorted(load_months))}")
            
            # 7. Validation v√† Data Quality Check
            logger.info("üîç B·∫Øt ƒë·∫ßu validation ETL...")
            validation_success = True
            validation_message = ""
            
            try:
                from src.utils.etl_validator import generate_etl_report, log_validation_results
                validation_results = generate_etl_report(duck_conn)
                log_validation_results(validation_results)
                
                # Ki·ªÉm tra c√°c v·∫•n ƒë·ªÅ nghi√™m tr·ªçng
                if validation_results.get('issues', {}).get('critical', 0) > 0:
                    validation_success = False
                    validation_message = f"Ph√°t hi·ªán {validation_results['issues']['critical']} v·∫•n ƒë·ªÅ nghi√™m tr·ªçng trong validation"
                    logger.error(validation_message)
            except ImportError:
                logger.warning("Kh√¥ng th·ªÉ import etl_validator - b·ªè qua validation")
            except Exception as e:
                validation_success = False
                validation_message = f"L·ªói khi th·ª±c hi·ªán validation: {str(e)}"
                logger.error(validation_message)
        
        # T√≠nh th·ªùi gian ch·∫°y
        duration = (datetime.now() - start_time).total_seconds()
        
        # T·ªïng k·∫øt
        logger.info("="*60)
        logger.info(f"‚úÖ ETL HO√ÄN TH√ÄNH TRONG {duration:.2f} GI√ÇY!")
        logger.info("="*60)
        
        # Th·ªëng k√™ k·∫øt qu·∫£ ETL
        etl_stats = {
            "success": True,
            "source_count": len(staging_batch),
            "fact_count": len(fact_records),
            "bridge_count": bridge_inserted,
            "dim_stats": dim_stats,
            "total_dim_inserted": total_inserted,
            "total_dim_updated": total_updated,
            "load_months": list(load_months),
            "duration_seconds": duration,
            "validation_success": validation_success,
            "validation_message": validation_message
        }
        
        return etl_stats
    
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        error_msg = f"L·ªói trong qu√° tr√¨nh ETL: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        return {
            "success": False,
            "message": error_msg,
            "duration_seconds": duration
        }

if __name__ == "__main__":
    # Ch·∫°y ETL v·ªõi d·ªØ li·ªáu t·ª´ 7 ng√†y tr∆∞·ªõc
    etl_result = run_staging_to_dwh_etl()
    
    # Ki·ªÉm tra k·∫øt qu·∫£
    if etl_result.get("success", False):
        logger.info("‚úÖ ETL HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
        sys.exit(0)
    else:
        logger.error(f"‚ùå ETL TH·∫§T B·∫†I: {etl_result.get('message', 'Unknown error')}")
        sys.exit(1)
