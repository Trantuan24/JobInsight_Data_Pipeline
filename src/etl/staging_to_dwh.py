#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ETL module cho viá»‡c chuyá»ƒn dá»¯ liá»‡u tá»« Staging sang Data Warehouse (DuckDB)
Fixed version vá»›i logic parsing location má»›i
"""
# Standard library imports
import json
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any

# Third-party imports
import pandas as pd
import duckdb
import pyarrow as pa
import pyarrow.parquet as pq

# Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n vÃ  logging
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
sys.path.insert(0, PROJECT_ROOT)

# Äáº£m báº£o thÆ° má»¥c logs tá»“n táº¡i
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

# Thiáº¿t láº­p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "etl.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Local imports
from src.utils.db import get_connection, get_dataframe, execute_query
from src.utils.config import DUCKDB_PATH, DWH_STAGING_SCHEMA, STAGING_JOBS_TABLE
from src.processing.data_prepare import (
    prepare_dim_job, prepare_dim_company, prepare_dim_location, 
    generate_date_range, parse_single_location_item, parse_job_location,
    check_dimension_changes, apply_scd_type2_updates, 
    generate_daily_fact_records, calculate_load_month
)

# ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c SQL
SQL_DIR = os.path.join(PROJECT_ROOT, "sql")
if not os.path.exists(SQL_DIR):
    SQL_DIR = os.path.join(os.getcwd(), "sql")
    os.makedirs(SQL_DIR, exist_ok=True)

def get_duckdb_connection(duckdb_path: str = DUCKDB_PATH) -> duckdb.DuckDBPyConnection:
    """ Káº¿t ná»‘i Ä‘áº¿n DuckDB """
    # Äáº£m báº£o Ä‘Æ°á»ng dáº«n lÃ  tuyá»‡t Ä‘á»‘i
    if not os.path.isabs(duckdb_path):
        duckdb_path = os.path.join(PROJECT_ROOT, duckdb_path)
    
    # Äáº£m báº£o thÆ° má»¥c cha tá»“n táº¡i
    parent_dir = os.path.dirname(duckdb_path)
    os.makedirs(parent_dir, exist_ok=True)
    
    logger.info(f"Káº¿t ná»‘i DuckDB táº¡i: {duckdb_path}")
    
    return duckdb.connect(duckdb_path)

def execute_sql_file_duckdb(sql_file_path, conn=None):
    """Thá»±c thi file SQL trÃªn DuckDB vá»›i connection truyá»n vÃ o (hoáº·c tá»± táº¡o)"""
    try:
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_script = f.read()
        # Náº¿u khÃ´ng truyá»n conn thÃ¬ tá»± táº¡o vÃ  tá»± Ä‘Ã³ng, cÃ²n truyá»n vÃ o thÃ¬ khÃ´ng Ä‘Ã³ng
        close_conn = False
        if conn is None:
            conn = get_duckdb_connection()
            close_conn = True
        conn.execute(sql_script)
        if close_conn:
            conn.close()
        logger.info(f"ÄÃ£ thá»±c thi file SQL: {sql_file_path}")
        return True
    except Exception as e:
        logger.error(f"Lá»—i khi thá»±c thi file SQL {sql_file_path}: {str(e)}")
        return False

def setup_duckdb_schema():
    """Thiáº¿t láº­p schema vÃ  báº£ng cho DuckDB"""
    try:
        with get_duckdb_connection() as conn:
            # Táº¡o schema náº¿u chÆ°a cÃ³ (DuckDB sáº½ khÃ´ng bÃ¡o lá»—i náº¿u Ä‘Ã£ tá»“n táº¡i)
            conn.execute(f"CREATE SCHEMA IF NOT EXISTS {DWH_STAGING_SCHEMA};")
            logger.info(f"ÄÃ£ Ä‘áº£m báº£o tá»“n táº¡i schema: {DWH_STAGING_SCHEMA}")

            # Thá»±c thi file schema_dwh.sql Ä‘á»ƒ táº¡o báº£ng vÃ  chá»‰ má»¥c
            schema_dwh = os.path.join(SQL_DIR, "schema_dwh.sql")
            if os.path.exists(schema_dwh):
                if not execute_sql_file_duckdb(schema_dwh, conn=conn):
                    logger.error("KhÃ´ng thá»ƒ thiáº¿t láº­p schema vÃ  báº£ng!")
                    return False
            else:
                logger.error(f"KhÃ´ng tÃ¬m tháº¥y file schema: {schema_dwh}")
                return False

            # Kiá»ƒm tra xem cÃ¡c báº£ng Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng chÆ°a
            tables_in_db = [row[0] for row in conn.execute("SHOW TABLES").fetchall()]
            for table in ['DimJob', 'DimCompany', 'DimLocation', 'DimDate', 'FactJobPostingDaily', 'FactJobLocationBridge']:
                if table in tables_in_db:
                    logger.info(f"âœ“ Báº£ng {table} Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng")
                else:
                    logger.warning(f"âœ— Báº£ng {table} KHÃ”NG Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng")

        logger.info("ÄÃ£ thiáº¿t láº­p schema vÃ  báº£ng database thÃ nh cÃ´ng cho DuckDB!")
        return True
    except Exception as e:
        logger.error(f"Lá»—i khi thiáº¿t láº­p schema database vá»›i DuckDB: {str(e)}")
        return False
    

def get_staging_batch(last_etl_date: datetime) -> pd.DataFrame:
    """
    Láº¥y batch dá»¯ liá»‡u tá»« staging jobs ká»ƒ tá»« láº§n ETL gáº§n nháº¥t
    
    Args:
        last_etl_date: Timestamp cá»§a láº§n ETL gáº§n nháº¥t
        
    Returns:
        DataFrame chá»©a báº£n ghi cáº§n xá»­ lÃ½
    """
    try:
        logger.info(f"Truy váº¥n dá»¯ liá»‡u tá»« báº£ng {STAGING_JOBS_TABLE}")
        
        query = f"""
            SELECT *
            FROM {STAGING_JOBS_TABLE}
            WHERE crawled_at >= %s
            OR
            (crawled_at IS NOT NULL AND %s IS NULL)
        """
        
        # Kiá»ƒm tra xem báº£ng cÃ³ tá»“n táº¡i khÃ´ng
        table_exists_query = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{DWH_STAGING_SCHEMA}' 
                AND table_name = 'staging_jobs'
            );
        """
        
        # Thá»±c hiá»‡n kiá»ƒm tra
        exists = execute_query(table_exists_query, fetch=True)
        
        if not exists or not exists[0].get('exists', False):
            logger.error(f"Báº£ng {STAGING_JOBS_TABLE} khÃ´ng tá»“n táº¡i!")
            # Tráº£ vá» DataFrame rá»—ng náº¿u báº£ng khÃ´ng tá»“n táº¡i
            return pd.DataFrame()
        
        df = get_dataframe(query, params=(last_etl_date, last_etl_date))
        logger.info(f"ÄÃ£ láº¥y {len(df)} báº£n ghi tá»« staging (tá»« {last_etl_date})")
        
        # Log cÃ¡c cá»™t Ä‘á»ƒ debug
        if not df.empty:
            logger.info(f"CÃ¡c cá»™t cÃ³ trong dá»¯ liá»‡u: {list(df.columns)}")
        
        return df
    except Exception as e:
        logger.error(f"Lá»—i khi láº¥y dá»¯ liá»‡u tá»« staging: {e}", exc_info=True)
        # Tráº£ vá» DataFrame rá»—ng trong trÆ°á»ng há»£p lá»—i
        return pd.DataFrame()
    
def lookup_location_key(
    duck_conn: duckdb.DuckDBPyConnection,
    province: str = None,
    city: str = None,
    district: str = None
) -> Optional[int]:
    """
    TÃ¬m location_sk tá»« báº£ng DimLocation dá»±a trÃªn province, city, district
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        province: TÃªn tá»‰nh
        city: TÃªn thÃ nh phá»‘
        district: TÃªn quáº­n/huyá»‡n
    
    Returns:
        location_sk hoáº·c None náº¿u khÃ´ng tÃ¬m tháº¥y
    """
    try:
        # XÃ¢y dá»±ng query Ä‘á»™ng dá»±a trÃªn cÃ¡c tham sá»‘ cÃ³ giÃ¡ trá»‹
        conditions = ["is_current = TRUE"]
        params = []
        
        if province is not None:
            conditions.append("province = ?")
            params.append(province)
        else:
            conditions.append("province IS NULL")
            
        if city is not None:
            conditions.append("city = ?")
            params.append(city)
        else:
            conditions.append("city IS NULL")
            
        if district is not None:
            conditions.append("district = ?")
            params.append(district)
        else:
            conditions.append("district IS NULL")
        
        query = f"""
            SELECT location_sk
            FROM DimLocation
            WHERE {' AND '.join(conditions)}
            LIMIT 1
        """
        
        result = duck_conn.execute(query, params).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"Lá»—i khi tÃ¬m location_sk: {e}")
        return None

def lookup_dimension_key(
    duck_conn: duckdb.DuckDBPyConnection,
    dim_table: str,
    key_column: str,
    key_value: Any,
    surrogate_key_col: str
) -> Optional[int]:
    """
    TÃ¬m surrogate key tá»« báº£ng dimension
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        dim_table: TÃªn báº£ng dimension
        key_column: TÃªn cá»™t dÃ¹ng Ä‘á»ƒ tÃ¬m kiáº¿m
        key_value: GiÃ¡ trá»‹ cáº§n tÃ¬m
        surrogate_key_col: TÃªn cá»™t surrogate key
    
    Returns:
        Surrogate key hoáº·c None náº¿u khÃ´ng tÃ¬m tháº¥y
    """
    try:
        query = f"""
            SELECT {surrogate_key_col}
            FROM {dim_table}
            WHERE {key_column} = ?
            AND is_current = TRUE
            LIMIT 1
        """
        
        result = duck_conn.execute(query, [key_value]).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"Lá»—i khi tÃ¬m khÃ³a trong {dim_table}: {e}")
        return None

def batch_insert_records(duck_conn: duckdb.DuckDBPyConnection, table_name: str, records: List[Dict], batch_size: int = 1000, upsert_on_conflict: str = None):
    """
    Batch insert records Ä‘á»ƒ tá»‘i Æ°u performance
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        table_name: TÃªn báº£ng
        records: List cÃ¡c records cáº§n insert
        batch_size: KÃ­ch thÆ°á»›c batch
        upsert_on_conflict: Column(s) Ä‘á»ƒ UPSERT (e.g., "(fact_id, location_sk)")
    """
    if not records:
        return 0
        
    inserted_count = 0
    
    # Chia records thÃ nh cÃ¡c batch
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        
        try:
            # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho FactJobLocationBridge vá»›i duplicate check
            if table_name == 'FactJobLocationBridge':
                for record in batch:
                    fact_id = record['fact_id']
                    location_sk = record['location_sk']
                    
                    # Kiá»ƒm tra xem bridge record Ä‘Ã£ tá»“n táº¡i chÆ°a
                    check_query = """
                        SELECT 1 FROM FactJobLocationBridge 
                        WHERE fact_id = ? AND location_sk = ?
                    """
                    exists = duck_conn.execute(check_query, [fact_id, location_sk]).fetchone()
                    
                    if not exists:
                        insert_query = """
                            INSERT INTO FactJobLocationBridge (fact_id, location_sk)
                            VALUES (?, ?)
                        """
                        duck_conn.execute(insert_query, [fact_id, location_sk])
                        inserted_count += 1
                        
            else:
                # Xá»­ lÃ½ thÃ´ng thÆ°á»ng cho cÃ¡c báº£ng khÃ¡c
                # Chuáº©n bá»‹ dá»¯ liá»‡u batch
                df_batch = pd.DataFrame(batch)
                
                # Handle JSON columns
                for col in df_batch.columns:
                    df_batch[col] = df_batch[col].apply(
                        lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x
                    )
                
                # Insert batch
                duck_conn.execute(f"INSERT INTO {table_name} SELECT * FROM df_batch")
                inserted_count += len(batch)
            
        except Exception as e:
            logger.warning(f"Lá»—i khi batch insert vÃ o {table_name}: {e}")
            # Fallback: insert tá»«ng record
            for record in batch:
                try:
                    columns = list(record.keys())
                    placeholders = ', '.join(['?'] * len(columns))
                    values = [record[col] for col in columns]
                    
                    # Handle JSON values
                    for j, val in enumerate(values):
                        if isinstance(val, (dict, list)):
                            values[j] = json.dumps(val)
                    
                    # Kiá»ƒm tra náº¿u cÃ³ upsert_on_conflict
                    if upsert_on_conflict and table_name == 'FactJobLocationBridge':
                        # Skip náº¿u Ä‘Ã£ tá»“n táº¡i
                        check_query = f"SELECT 1 FROM {table_name} WHERE fact_id = ? AND location_sk = ?"
                        exists = duck_conn.execute(check_query, [values[0], values[1]]).fetchone()
                        if exists:
                            continue
                    
                    query = f"""
                        INSERT INTO {table_name} ({', '.join(columns)})
                        VALUES ({placeholders})
                    """
                    duck_conn.execute(query, values)
                    inserted_count += 1
                except Exception as e2:
                    logger.error(f"Lá»—i khi insert single record vÃ o {table_name}: {e2}")
    
    logger.info(f"ÄÃ£ batch insert {inserted_count} records vÃ o {table_name}")
    return inserted_count

def process_dimension_with_scd2(
    duck_conn: duckdb.DuckDBPyConnection,
    staging_records: pd.DataFrame,
    dim_table: str,
    prepare_function,
    natural_key: str,
    surrogate_key: str,
    compare_columns: List[str]
) -> Dict[str, int]:
    """
    Xá»­ lÃ½ dimension table vá»›i SCD Type 2
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        staging_records: Dá»¯ liá»‡u staging
        dim_table: TÃªn báº£ng dimension
        prepare_function: Function chuáº©n bá»‹ dá»¯ liá»‡u
        natural_key: Natural key column
        surrogate_key: Surrogate key column
        compare_columns: Columns Ä‘á»ƒ so sÃ¡nh thay Ä‘á»•i
    
    Returns:
        Dict thá»‘ng kÃª insert/update
    """
    logger.info(f"Xá»­ lÃ½ {dim_table} vá»›i SCD Type 2")
    
    # Chuáº©n bá»‹ dá»¯ liá»‡u
    prepared_data = prepare_function(staging_records)
    
    if prepared_data.empty:
        logger.warning(f"KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½ cho {dim_table}")
        return {'inserted': 0, 'updated': 0, 'unchanged': 0}
    
    # Kiá»ƒm tra thay Ä‘á»•i
    to_insert, to_update, unchanged = check_dimension_changes(
        duck_conn, prepared_data, dim_table, natural_key, compare_columns
    )
    
    stats = {
        'inserted': 0,
        'updated': 0,
        'unchanged': len(unchanged)
    }
    
    # Ãp dá»¥ng updates (SCD Type 2)
    if to_update:
        apply_scd_type2_updates(duck_conn, dim_table, surrogate_key, to_update)
        stats['updated'] = len(to_update)
    
    # Insert records má»›i
    if not to_insert.empty:
        insert_records = []
        for _, record in to_insert.iterrows():
            record_dict = record.to_dict()
            # Loáº¡i bá» surrogate key
            if surrogate_key in record_dict:
                del record_dict[surrogate_key]
            insert_records.append(record_dict)
        
        stats['inserted'] = batch_insert_records(duck_conn, dim_table, insert_records)
    
    logger.info(f"{dim_table} - Inserted: {stats['inserted']}, Updated: {stats['updated']}, Unchanged: {stats['unchanged']}")
    return stats

def generate_fact_records(
    duck_conn: duckdb.DuckDBPyConnection,
    staging_records: pd.DataFrame
) -> Tuple[List[Dict], List[Dict]]:
    """
    Táº¡o báº£n ghi fact vÃ  bridge tá»« dá»¯ liá»‡u staging
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        staging_records: Dá»¯ liá»‡u tá»« staging
    
    Returns:
        Tuple chá»©a (fact_records, bridge_records)
    """
    fact_records = []
    bridge_records = []
    
    # Thá»‘ng kÃª job_ids Ä‘á»ƒ log
    job_ids = set()
    date_counts = {}
    skipped_jobs = 0
    
    # Chuáº©n bá»‹ lookup cho Unknown location
    unknown_location_sk = lookup_location_key(duck_conn, None, 'Unknown', None)
    
    # Táº¡o cache cho location lookups Ä‘á»ƒ trÃ¡nh truy váº¥n láº·p láº¡i
    location_cache = {}
    
    for _, job in staging_records.iterrows():
        # Lookup dimension keys
        job_sk = lookup_dimension_key(
            duck_conn, 'DimJob', 'job_id', job.job_id, 'job_sk'
        )
        
        company_name = job.company_name_standardized if pd.notna(job.company_name_standardized) else job.company_name
        company_sk = lookup_dimension_key(
            duck_conn, 'DimCompany', 'company_name_standardized', company_name, 'company_sk'
        )
        
        # Check if required keys exist
        if not job_sk or not company_sk:
            logger.warning(f"Bá» qua job_id={job.job_id}: KhÃ´ng tÃ¬m tháº¥y dimension key (job_sk={job_sk}, company_sk={company_sk})")
            skipped_jobs += 1
            continue
        
        # Xá»­ lÃ½ ngÃ y
        due_date = pd.to_datetime(job.due_date) if pd.notna(job.due_date) else None
        posted_time = pd.to_datetime(job.posted_time) if pd.notna(job.posted_time) else None
        crawled_at = pd.to_datetime(job.crawled_at) if pd.notna(job.crawled_at) else datetime.now()
        
        # Táº¡o danh sÃ¡ch cÃ¡c ngÃ y cáº§n táº¡o fact records
        daily_dates = generate_daily_fact_records(posted_time, due_date)
        
        # TÃ­nh load_month
        load_month = calculate_load_month(crawled_at)
        
        # Thá»‘ng kÃª Ä‘á»ƒ log
        job_ids.add(job.job_id)
        date_counts[job.job_id] = len(daily_dates)
        
        # Parse location string - Di chuyá»ƒn ra ngoÃ i vÃ²ng láº·p ngÃ y
        location_str = None
        if 'location_pairs' in job and pd.notna(job.location_pairs):
            location_str = str(job.location_pairs)
        elif 'location' in job and pd.notna(job.location):
            location_str = str(job.location)
            
        # Parse location thÃ nh cÃ¡c tuple (province, city, district)
        parsed_locations = parse_job_location(location_str) if location_str else []
        
        # TÃ¬m location_sk cho táº¥t cáº£ locations cá»§a job nÃ y trÆ°á»›c
        location_sks = set()
        if parsed_locations:
            for province, city, district in parsed_locations:
                # Táº¡o cache key
                cache_key = f"{province}:{city}:{district}"
                
                if cache_key in location_cache:
                    location_sk = location_cache[cache_key]
                else:
                    # Thá»­ lookup vá»›i Ä‘áº§y Ä‘á»§ thÃ´ng tin trÆ°á»›c
                    location_sk = lookup_location_key(duck_conn, province, city, district)
                    
                    if not location_sk and city:
                        # Thá»­ vá»›i chá»‰ province + city
                        location_sk = lookup_location_key(duck_conn, province, city, None)
                        
                    if not location_sk and city:
                        # Thá»­ vá»›i chá»‰ city
                        location_sk = lookup_location_key(duck_conn, None, city, None)
                    
                    # LÆ°u vÃ o cache
                    location_cache[cache_key] = location_sk
                
                if location_sk:
                    location_sks.add(location_sk)
        
        # Náº¿u khÃ´ng tÃ¬m Ä‘Æ°á»£c location nÃ o, dÃ¹ng Unknown
        if not location_sks and unknown_location_sk:
            location_sks.add(unknown_location_sk)
        
        # Táº¡o fact records cho tá»«ng ngÃ y
        for date_id in daily_dates:
            try:
                # Kiá»ƒm tra xem fact record Ä‘Ã£ tá»“n táº¡i chÆ°a
                check_query = """
                    SELECT fact_id FROM FactJobPostingDaily 
                    WHERE job_sk = ? AND date_id = ?
                """
                existing_fact = duck_conn.execute(check_query, [job_sk, date_id]).fetchone()
                
                # Chuáº©n bá»‹ common values cho cáº£ insert vÃ  update
                common_values = {
                    'job_sk': job_sk,
                    'company_sk': company_sk,
                    'date_id': date_id,
                    'salary_min': job.salary_min if pd.notna(job.salary_min) else None,
                    'salary_max': job.salary_max if pd.notna(job.salary_max) else None,
                    'salary_type': job.salary_type if pd.notna(job.salary_type) else None,
                    'due_date': due_date,
                    'time_remaining': job.time_remaining if pd.notna(job.time_remaining) else None,
                    'verified_employer': job.verified_employer if pd.notna(job.verified_employer) else False,
                    'posted_time': posted_time,
                    'crawled_at': crawled_at,
                    'load_month': load_month
                }
                
                fact_id = None
                if existing_fact:
                    # ÄÃ£ tá»“n táº¡i, cáº­p nháº­t
                    fact_id = existing_fact[0]
                    
                    # Lá»c ra cÃ¡c giÃ¡ trá»‹ khÃ´ng null Ä‘á»ƒ update
                    non_null_items = {k: v for k, v in common_values.items() if v is not None}
                    set_clauses = [f"{k} = ?" for k in non_null_items.keys()]
                    values = list(non_null_items.values()) + [fact_id]
                    
                    update_query = f"""
                        UPDATE FactJobPostingDaily 
                        SET {', '.join(set_clauses)}
                        WHERE fact_id = ?
                    """
                    
                    try:
                        duck_conn.execute(update_query, values)
                        logger.debug(f"Updated existing fact record: job_sk={job_sk}, date_id={date_id}, fact_id={fact_id}")
                    except Exception as e:
                        logger.error(f"Lá»—i khi update fact record cho job_id={job.job_id}, date={date_id}: {e}")
                        continue
                        
                else:
                    # ChÆ°a tá»“n táº¡i, táº¡o má»›i
                    # Lá»c ra cÃ¡c giÃ¡ trá»‹ khÃ´ng null Ä‘á»ƒ insert
                    non_null_items = {k: v for k, v in common_values.items() if v is not None}
                    columns = ', '.join(non_null_items.keys())
                    placeholders = ', '.join(['?'] * len(non_null_items))
                    values = list(non_null_items.values())
                    
                    insert_query = f"""
                        INSERT INTO FactJobPostingDaily ({columns})
                        VALUES ({placeholders})
                        RETURNING fact_id
                    """
                    
                    try:
                        result = duck_conn.execute(insert_query, values).fetchone()
                        if not result:
                            logger.warning(f"KhÃ´ng thá»ƒ insert fact record cho job_id={job.job_id}, date={date_id}")
                            continue
                            
                        fact_id = result[0]
                        fact_records.append(common_values)
                        logger.debug(f"Inserted new fact record: job_sk={job_sk}, date_id={date_id}, fact_id={fact_id}")
                    
                    except Exception as e:
                        logger.error(f"Lá»—i khi insert fact record cho job_id={job.job_id}, date={date_id}: {e}")
                        continue
                
                # XÃ³a bridge records cÅ© cho fact_id nÃ y
                duck_conn.execute("DELETE FROM FactJobLocationBridge WHERE fact_id = ?", [fact_id])
                
                # Táº¡o bridge records cho táº¥t cáº£ locations Ä‘Ã£ tÃ¬m tháº¥y
                for location_sk in location_sks:
                    bridge_records.append({'fact_id': fact_id, 'location_sk': location_sk})
                        
            except Exception as e:
                logger.error(f"Lá»—i khi xá»­ lÃ½ fact record cho job_id={job.job_id}, date={date_id}: {e}", exc_info=True)
                continue
    
    # Log káº¿t quáº£
    logger.info(f"ÄÃ£ táº¡o {len(fact_records)} fact records cho {len(job_ids)} jobs")
    logger.info(f"ÄÃ£ táº¡o {len(bridge_records)} bridge records")
    logger.info(f"Sá»‘ jobs bá»‹ bá» qua: {skipped_jobs}")
    
    # Log phÃ¢n bá»‘ sá»‘ fact records trÃªn má»—i job
    if date_counts:
        avg_dates = sum(date_counts.values()) / len(date_counts)
        max_dates = max(date_counts.values() if date_counts else [0])
        logger.info(f"Trung bÃ¬nh {avg_dates:.1f} ngÃ y/job, tá»‘i Ä‘a {max_dates} ngÃ y/job")
    
    return fact_records, bridge_records


def cleanup_duplicate_fact_records(duck_conn: duckdb.DuckDBPyConnection):
    """
    Dá»n dáº¹p cÃ¡c duplicate records trong FactJobPostingDaily vÃ  FactJobLocationBridge
    """
    logger.info("Báº¯t Ä‘áº§u dá»n dáº¹p duplicate fact records...")
    
    try:
        # 1. Backup bridge records trÆ°á»›c khi xÃ³a
        logger.info("Backup bridge records...")
        backup_bridge_query = """
            CREATE OR REPLACE TEMP TABLE bridge_backup AS
            SELECT DISTINCT fact_id, location_sk 
            FROM FactJobLocationBridge
        """
        duck_conn.execute(backup_bridge_query)
        
        # 2. TÃ¬m vÃ  xÃ³a duplicate fact records, giá»¯ láº¡i record cÃ³ fact_id nhá» nháº¥t
        logger.info("TÃ¬m duplicate fact records...")
        find_duplicates_query = """
            SELECT job_sk, date_id, COUNT(*) as count, MIN(fact_id) as keep_fact_id
            FROM FactJobPostingDaily
            GROUP BY job_sk, date_id
            HAVING COUNT(*) > 1
        """
        
        duplicates = duck_conn.execute(find_duplicates_query).fetchdf()
        
        if not duplicates.empty:
            logger.info(f"TÃ¬m tháº¥y {len(duplicates)} nhÃ³m duplicate fact records")
            
            total_deleted = 0
            for _, dup in duplicates.iterrows():
                job_sk, date_id, count, keep_fact_id = dup['job_sk'], dup['date_id'], dup['count'], dup['keep_fact_id']
                
                # Láº¥y danh sÃ¡ch fact_id cáº§n xÃ³a (táº¥t cáº£ trá»« keep_fact_id)
                get_delete_ids_query = """
                    SELECT fact_id FROM FactJobPostingDaily
                    WHERE job_sk = ? AND date_id = ? AND fact_id != ?
                """
                delete_ids = duck_conn.execute(get_delete_ids_query, [job_sk, date_id, keep_fact_id]).fetchall()
                
                if delete_ids:
                    fact_ids_to_delete = [row[0] for row in delete_ids]
                    placeholders = ','.join(['?'] * len(fact_ids_to_delete))
                    
                    # XÃ³a bridge records trÆ°á»›c
                    delete_bridge_query = f"""
                        DELETE FROM FactJobLocationBridge 
                        WHERE fact_id IN ({placeholders})
                    """
                    duck_conn.execute(delete_bridge_query, fact_ids_to_delete)
                    
                    # XÃ³a fact records
                    delete_fact_query = f"""
                        DELETE FROM FactJobPostingDaily 
                        WHERE fact_id IN ({placeholders})
                    """
                    duck_conn.execute(delete_fact_query, fact_ids_to_delete)
                    
                    total_deleted += len(fact_ids_to_delete)
                    logger.debug(f"ÄÃ£ xÃ³a {len(fact_ids_to_delete)} duplicate records cho job_sk={job_sk}, date_id={date_id}")
            
            logger.info(f"ÄÃ£ xÃ³a tá»•ng cá»™ng {total_deleted} duplicate fact records")
            
            # 3. Restore bridge records cho cÃ¡c fact_id cÃ²n láº¡i
            logger.info("Restore bridge records...")
            restore_bridge_query = """
                INSERT INTO FactJobLocationBridge (fact_id, location_sk)
                SELECT DISTINCT b.fact_id, b.location_sk
                FROM bridge_backup b
                JOIN FactJobPostingDaily f ON b.fact_id = f.fact_id
                WHERE NOT EXISTS (
                    SELECT 1 FROM FactJobLocationBridge fb 
                    WHERE fb.fact_id = b.fact_id AND fb.location_sk = b.location_sk
                )
            """
            duck_conn.execute(restore_bridge_query)
            
        else:
            logger.info("KhÃ´ng tÃ¬m tháº¥y duplicate fact records")
            
        # 4. Thá»‘ng kÃª sau khi dá»n dáº¹p
        stats_query = """
            SELECT 
                COUNT(*) as total_facts,
                (SELECT COUNT(*) FROM (SELECT DISTINCT job_sk, date_id FROM FactJobPostingDaily)) as unique_combinations
            FROM FactJobPostingDaily
        """
        stats = duck_conn.execute(stats_query).fetchone()
        logger.info(f"Sau dá»n dáº¹p: {stats[0]} fact records, {stats[1]} unique combinations")
        
        if stats[0] != stats[1]:
            logger.warning(f"Váº«n cÃ²n {stats[0] - stats[1]} duplicate records!")
        else:
            logger.info("âœ… ÄÃ£ dá»n dáº¹p thÃ nh cÃ´ng táº¥t cáº£ duplicate records")
            
    except Exception as e:
        logger.error(f"Lá»—i khi dá»n dáº¹p duplicate records: {e}")
        raise

def verify_etl_integrity(staging_count: int, fact_count: int, threshold: float = 0.9) -> bool:
    """
    Kiá»ƒm tra tÃ­nh toÃ n váº¹n cá»§a quÃ¡ trÃ¬nh ETL Staging to DWH
    
    Args:
        staging_count: Sá»‘ báº£n ghi staging Ä‘áº§u vÃ o
        fact_count: Sá»‘ báº£n ghi fact Ä‘Ã£ táº¡o
        threshold: NgÆ°á»¡ng cháº¥p nháº­n (% dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng)
        
    Returns:
        bool: True náº¿u tá»· lá»‡ dá»¯ liá»‡u chuyá»ƒn Ä‘á»•i Ä‘áº¡t threshold
    """
    if staging_count == 0:
        logger.warning("KhÃ´ng cÃ³ dá»¯ liá»‡u nguá»“n Ä‘á»ƒ xá»­ lÃ½")
        return True
    
    # Má»—i staging record cÃ³ thá»ƒ táº¡o ra nhiá»u fact record (má»—i ngÃ y má»™t record)
    # NÃªn kiá»ƒm tra xem cÃ³ fact records Ä‘Æ°á»£c táº¡o khÃ´ng, khÃ´ng so sÃ¡nh sá»‘ lÆ°á»£ng
    if fact_count == 0:
        logger.error("KhÃ´ng cÃ³ fact record nÃ o Ä‘Æ°á»£c táº¡o tá»« staging data!")
        return False
    
    logger.info(f"ÄÃ£ táº¡o {fact_count} fact records tá»« {staging_count} staging records")
    return True

def run_staging_to_dwh_etl(last_etl_date: Optional[datetime] = None) -> Dict[str, Any]:
    """
    Thá»±c hiá»‡n quy trÃ¬nh ETL chuyá»ƒn dá»¯ liá»‡u tá»« Staging sang Data Warehouse
    
    Args:
        last_etl_date: Timestamp cá»§a láº§n ETL gáº§n nháº¥t, máº·c Ä‘á»‹nh lÃ  7 ngÃ y trÆ°á»›c
        
    Returns:
        Dict[str, Any]: Káº¿t quáº£ thá»‘ng kÃª ETL
    """
    start_time = datetime.now()
    
    try:
        # Thiáº¿t láº­p ngÃ y ETL gáº§n nháº¥t náº¿u khÃ´ng cÃ³
        if last_etl_date is None:
            last_etl_date = datetime.now() - timedelta(days=7)
        
        logger.info("="*60)
        logger.info(f"ğŸš€ Báº®T Äáº¦U ETL STAGING TO DWH - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ğŸ•’ Láº¥y dá»¯ liá»‡u tá»«: {last_etl_date}")
        logger.info("="*60)
        
        # 1. Láº¥y dá»¯ liá»‡u tá»« staging
        staging_batch = get_staging_batch(last_etl_date)
        if staging_batch.empty:
            logger.info("KhÃ´ng cÃ³ báº£n ghi nÃ o Ä‘á»ƒ xá»­ lÃ½ tá»« staging")
            return {
                "success": True,
                "message": "KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½",
                "source_count": 0,
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
            
        logger.info(f"ÄÃ£ láº¥y {len(staging_batch)} báº£n ghi tá»« staging")
        
        # 2. Kiá»ƒm tra file DuckDB
        if os.path.exists(DUCKDB_PATH):
            logger.info(f"ğŸ“ Sá»­ dá»¥ng DuckDB hiá»‡n cÃ³: {DUCKDB_PATH}")
        else:
            logger.info(f"ğŸ†• Táº¡o DuckDB má»›i: {DUCKDB_PATH}")
        
        # 3. Thiáº¿t láº­p schema vÃ  báº£ng (giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©)
        if not setup_duckdb_schema():
            return {
                "success": False,
                "message": "KhÃ´ng thá»ƒ thiáº¿t láº­p schema DuckDB",
                "source_count": len(staging_batch),
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
        
        # 4. Káº¿t ná»‘i DuckDB vÃ  thá»±c hiá»‡n ETL vá»›i SCD Type 2
        with get_duckdb_connection(DUCKDB_PATH) as duck_conn:
            # Dá»n dáº¹p duplicate records hiá»‡n cÃ³ (cháº¡y 1 láº§n)
            logger.info("ğŸ§¹ Dá»n dáº¹p duplicate records hiá»‡n cÃ³...")
            cleanup_duplicate_fact_records(duck_conn)
            
            # 5. Xá»­ lÃ½ vÃ  insert dá»¯ liá»‡u vá»›i SCD Type 2
            dim_stats = {}
            
            # 5.1 DimJob vá»›i SCD Type 2
            dim_stats['DimJob'] = process_dimension_with_scd2(
                duck_conn, staging_batch, 'DimJob', prepare_dim_job,
                'job_id', 'job_sk', ['title_clean', 'skills', 'job_url']
            )
        
            # 5.2. DimCompany vá»›i SCD Type 2
            dim_stats['DimCompany'] = process_dimension_with_scd2(
                duck_conn, staging_batch, 'DimCompany', prepare_dim_company,
                'company_name_standardized', 'company_sk', ['company_url', 'verified_employer']
            )
        
            # 5.3. DimLocation - xá»­ lÃ½ Ä‘áº·c biá»‡t vÃ¬ composite key
            logger.info("Xá»­ lÃ½ DimLocation vá»›i composite key")
            dim_location_df = prepare_dim_location(staging_batch)
            if not dim_location_df.empty:
                location_records = []
                for _, location in dim_location_df.iterrows():
                    location_dict = location.to_dict()
                    if 'location_sk' in location_dict:
                        del location_dict['location_sk']
                    location_records.append(location_dict)
                
                dim_stats['DimLocation'] = {
                    'inserted': batch_insert_records(duck_conn, 'DimLocation', location_records),
                    'updated': 0,
                    'unchanged': 0
                }
            else:
                dim_stats['DimLocation'] = {'inserted': 0, 'updated': 0, 'unchanged': 0}
        
            # 5.4. Äáº£m báº£o báº£ng DimDate cÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c ngÃ y cáº§n thiáº¿t
            logger.info("Xá»­ lÃ½ DimDate")
            start_date = (datetime.now() - timedelta(days=60)).date()
            end_date = (datetime.now() + timedelta(days=240)).date()
            
            date_df = generate_date_range(start_date, end_date)
            
            # Filter out existing dates
            new_date_records = []
            for _, date_record in date_df.iterrows():
                date_dict = date_record.to_dict()
                exists = duck_conn.execute(f"SELECT 1 FROM DimDate WHERE date_id = ?", [date_dict['date_id']]).fetchone()
                if not exists:
                    new_date_records.append(date_dict)
            
            dim_stats['DimDate'] = {
                'inserted': batch_insert_records(duck_conn, 'DimDate', new_date_records),
                'updated': 0,
                'unchanged': len(date_df) - len(new_date_records)
            }
        
            # 5.5. Insert dá»¯ liá»‡u vÃ o FactJobPostingDaily vÃ  FactJobLocationBridge
            logger.info("Xá»­ lÃ½ FactJobPostingDaily vÃ  FactJobLocationBridge")
            fact_records, bridge_records = generate_fact_records(duck_conn, staging_batch)
            
            # Kiá»ƒm tra tÃ­nh toÃ n váº¹n cá»§a dá»¯ liá»‡u
            if not verify_etl_integrity(len(staging_batch), len(fact_records)):
                logger.warning("âš ï¸ PhÃ¡t hiá»‡n váº¥n Ä‘á» vá» tÃ­nh toÃ n váº¹n dá»¯ liá»‡u trong quÃ¡ trÃ¬nh ETL!")
                # Váº«n tiáº¿p tá»¥c nhÆ°ng Ä‘Ã£ cáº£nh bÃ¡o
            
            logger.info(f"ÄÃ£ insert {len(fact_records)} báº£n ghi vÃ o FactJobPostingDaily")
            logger.info(f"Chuáº©n bá»‹ insert {len(bridge_records)} báº£n ghi vÃ o FactJobLocationBridge")
        
            # Batch insert bridge records vÃ o FactJobLocationBridge
            bridge_inserted = batch_insert_records(duck_conn, 'FactJobLocationBridge', bridge_records)
            logger.info(f"ÄÃ£ batch insert {bridge_inserted} báº£n ghi vÃ o FactJobLocationBridge")
        
            # 6. Tá»•ng káº¿t ETL
            logger.info("="*60)
            logger.info("ğŸ“Š Tá»”NG Káº¾T ETL STAGING TO DWH")
            logger.info("="*60)
            
            total_inserted = sum(stats.get('inserted', 0) for stats in dim_stats.values())
            total_updated = sum(stats.get('updated', 0) for stats in dim_stats.values())
            total_unchanged = sum(stats.get('unchanged', 0) for stats in dim_stats.values())
            
            for table, stats in dim_stats.items():
                logger.info(f"{table:15} - Insert: {stats['inserted']:5}, Update: {stats['updated']:5}, Unchanged: {stats['unchanged']:5}")
            
            logger.info(f"{'FACTS':15} - FactJobPostingDaily: {len(fact_records)} records")
            logger.info(f"{'BRIDGE':15} - FactJobLocationBridge: {bridge_inserted} records")
            logger.info("-"*60)
            logger.info(f"Tá»”NG DIM        - Insert: {total_inserted:5}, Update: {total_updated:5}, Unchanged: {total_unchanged:5}")
            logger.info(f"Tá»”NG FACT/BRIDGE- Records: {len(fact_records) + bridge_inserted}")
            
            # Log load_month stats
            load_months = set()
            if fact_records:
                load_months = set(record.get('load_month') for record in fact_records if record.get('load_month'))
                logger.info(f"Partition load_month: {', '.join(sorted(load_months))}")
            
            # 7. Validation vÃ  Data Quality Check
            logger.info("ğŸ” Báº¯t Ä‘áº§u validation ETL...")
            validation_success = True
            validation_message = ""
            
            try:
                from src.utils.etl_validator import generate_etl_report, log_validation_results
                validation_results = generate_etl_report(duck_conn)
                log_validation_results(validation_results)
                
                # Kiá»ƒm tra cÃ¡c váº¥n Ä‘á» nghiÃªm trá»ng
                if validation_results.get('issues', {}).get('critical', 0) > 0:
                    validation_success = False
                    validation_message = f"PhÃ¡t hiá»‡n {validation_results['issues']['critical']} váº¥n Ä‘á» nghiÃªm trá»ng trong validation"
                    logger.error(validation_message)
            except ImportError:
                logger.warning("KhÃ´ng thá»ƒ import etl_validator - bá» qua validation")
            except Exception as e:
                validation_success = False
                validation_message = f"Lá»—i khi thá»±c hiá»‡n validation: {str(e)}"
                logger.error(validation_message)
            
            # 8. Export dá»¯ liá»‡u ra Parquet theo load_month
            logger.info("ğŸ“¦ Báº¯t Ä‘áº§u export dá»¯ liá»‡u ra Parquet...")
            export_success = False
            export_message = ""
            export_stats = {}
            
            try:
                # Chuyá»ƒn tá»« set sang list Ä‘á»ƒ export
                load_months_list = list(load_months) if load_months else None
                
                # Chá»‰ export náº¿u cÃ³ load_months má»›i
                if load_months_list:
                    export_results = export_to_parquet(duck_conn, load_months_list)
                    export_success = export_results.get('success', False)
                    export_stats = export_results
                    
                    if export_success:
                        export_message = f"ÄÃ£ export dá»¯ liá»‡u cho {len(load_months_list)} load_month"
                        logger.info(f"âœ… {export_message}")
                    else:
                        export_message = f"CÃ³ lá»—i khi export dá»¯ liá»‡u: {export_results.get('message', 'Unknown error')}"
                        logger.warning(f"âš ï¸ {export_message}")
                else:
                    export_message = "KhÃ´ng cÃ³ load_month nÃ o Ä‘á»ƒ export"
                    logger.info(export_message)
            except Exception as e:
                export_success = False
                export_message = f"Lá»—i khi export dá»¯ liá»‡u ra Parquet: {str(e)}"
                logger.error(export_message, exc_info=True)
        
        # TÃ­nh thá»i gian cháº¡y
        duration = (datetime.now() - start_time).total_seconds()
        
        # Tá»•ng káº¿t
        logger.info("="*60)
        logger.info(f"âœ… ETL HOÃ€N THÃ€NH TRONG {duration:.2f} GIÃ‚Y!")
        logger.info("="*60)
        
        # Thá»‘ng kÃª káº¿t quáº£ ETL
        etl_stats = {
            "success": True,
            "source_count": len(staging_batch),
            "fact_count": len(fact_records),
            "bridge_count": bridge_inserted,
            "dim_stats": dim_stats,
            "total_dim_inserted": total_inserted,
            "total_dim_updated": total_updated,
            "load_months": list(load_months),
            "duration_seconds": duration,
            "validation_success": validation_success,
            "validation_message": validation_message,
            "export_success": export_success,
            "export_message": export_message,
            "export_stats": export_stats
        }
        
        return etl_stats
    
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        error_msg = f"Lá»—i trong quÃ¡ trÃ¬nh ETL: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        return {
            "success": False,
            "message": error_msg,
            "duration_seconds": duration
        }

if __name__ == "__main__":
    # Cháº¡y ETL vá»›i dá»¯ liá»‡u tá»« 7 ngÃ y trÆ°á»›c
    etl_result = run_staging_to_dwh_etl()
    
    # Kiá»ƒm tra káº¿t quáº£
    if etl_result.get("success", False):
        logger.info("âœ… ETL HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
        sys.exit(0)
    else:
        logger.error(f"âŒ ETL THáº¤T Báº I: {etl_result.get('message', 'Unknown error')}")
        sys.exit(1)

def export_to_parquet(duck_conn: duckdb.DuckDBPyConnection, load_months: List[str] = None) -> Dict[str, Any]:
    """
    Export dá»¯ liá»‡u tá»« DWH ra file Parquet theo load_month
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        load_months: List cÃ¡c load_month cáº§n export, náº¿u None thÃ¬ export táº¥t cáº£
        
    Returns:
        ThÃ´ng tin vá» quÃ¡ trÃ¬nh export
    """
    try:
        # Táº¡o thÆ° má»¥c export náº¿u chÆ°a cÃ³
        export_dir = os.path.join(PROJECT_ROOT, "export", "dwh")
        os.makedirs(export_dir, exist_ok=True)
        
        # Náº¿u khÃ´ng chá»‰ Ä‘á»‹nh load_months, láº¥y táº¥t cáº£ load_months tá»« fact table
        if not load_months:
            query = "SELECT DISTINCT load_month FROM FactJobPostingDaily ORDER BY load_month"
            load_months_result = duck_conn.execute(query).fetchall()
            load_months = [row[0] for row in load_months_result]
        
        if not load_months:
            logger.warning("KhÃ´ng cÃ³ load_month nÃ o Ä‘á»ƒ export!")
            return {"success": False, "message": "No load_months found"}
        
        # Thá»‘ng kÃª
        stats = {
            "load_months": load_months,
            "exports": {},
            "success": True,
            "timestamp": datetime.now().isoformat()
        }
        
        # Chuáº©n bá»‹ queries
        queries = {
            "facts": """
                SELECT f.*, j.title_clean, j.job_id, c.company_name_standardized
                FROM FactJobPostingDaily f
                JOIN DimJob j ON f.job_sk = j.job_sk
                JOIN DimCompany c ON f.company_sk = c.company_sk
                WHERE f.load_month = '{}'
            """,
            "locations": """
                SELECT f.fact_id, f.job_sk, f.date_id, j.job_id, j.title_clean,
                       l.province, l.city, l.district
                FROM FactJobPostingDaily f
                JOIN DimJob j ON f.job_sk = j.job_sk
                JOIN FactJobLocationBridge b ON f.fact_id = b.fact_id
                JOIN DimLocation l ON b.location_sk = l.location_sk
                WHERE f.load_month = '{}'
            """,
            "analytics": """
                SELECT j.title_clean, j.job_id, c.company_name_standardized, 
                       f.date_id, f.salary_min, f.salary_max, f.salary_type,
                       f.due_date, f.posted_time, f.verified_employer
                FROM FactJobPostingDaily f
                JOIN DimJob j ON f.job_sk = j.job_sk
                JOIN DimCompany c ON f.company_sk = c.company_sk
                WHERE f.load_month = '{}'
            """
        }
        
        # Export dá»¯ liá»‡u cho má»—i load_month
        total_records = 0
        for load_month in load_months:
            logger.info(f"Báº¯t Ä‘áº§u export dá»¯ liá»‡u cho load_month: {load_month}")
            
            # Táº¡o thÆ° má»¥c cho load_month
            month_dir = os.path.join(export_dir, load_month)
            os.makedirs(month_dir, exist_ok=True)
            
            try:
                export_files = {}
                record_counts = {}
                month_total = 0
                
                # Export tá»«ng loáº¡i dá»¯ liá»‡u
                for export_type, query_template in queries.items():
                    query = query_template.format(load_month)
                    df = duck_conn.execute(query).fetchdf()
                    
                    if not df.empty:
                        file_name = f"job_{export_type}_{load_month}.parquet"
                        file_path = os.path.join(month_dir, file_name)
                        df.to_parquet(file_path, index=False)
                        
                        export_files[export_type] = file_name
                        record_counts[export_type] = len(df)
                        stats["exports"][f"{export_type}_{load_month}"] = len(df)
                        month_total += len(df)
                    else:
                        export_files[export_type] = None
                        record_counts[export_type] = 0
                        stats["exports"][f"{export_type}_{load_month}"] = 0
                
                # Export metadata file
                meta_data = {
                    "load_month": load_month,
                    "export_time": datetime.now().isoformat(),
                    "record_counts": record_counts,
                    "files": [f for f in export_files.values() if f]
                }
                
                meta_file = os.path.join(month_dir, f"metadata_{load_month}.json")
                with open(meta_file, 'w', encoding='utf-8') as f:
                    json.dump(meta_data, f, indent=2)
                
                # Log tá»•ng há»£p
                logger.info(f"âœ… ÄÃ£ export {month_total} records cho load_month {load_month}")
                total_records += month_total
                
            except Exception as e:
                error_msg = f"Lá»—i khi export dá»¯ liá»‡u cho load_month {load_month}: {str(e)}"
                logger.error(error_msg, exc_info=True)
                stats["exports"][f"error_{load_month}"] = error_msg
                stats["success"] = False
        
        # Táº¡o file index cho táº¥t cáº£ load_months
        try:
            index_data = {
                "load_months": load_months,
                "export_time": datetime.now().isoformat(),
                "export_count": len(load_months),
                "total_records": total_records
            }
            
            index_file = os.path.join(export_dir, "index.json")
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, indent=2)
        except Exception as e:
            logger.error(f"Lá»—i khi táº¡o index file: {str(e)}")
        
        # Log tá»•ng káº¿t
        logger.info(f"âœ… HoÃ n thÃ nh export {total_records} records cho {len(load_months)} load_months")
        stats["total_records"] = total_records
        
        return stats
    
    except Exception as e:
        logger.error(f"Lá»—i khi export dá»¯ liá»‡u ra Parquet: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": str(e)
        }

def run_etl(last_etl_date: Optional[datetime] = None) -> Dict[str, Any]:
    """
    Alias cá»§a run_staging_to_dwh_etl Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n giá»¯a cÃ¡c module
    
    Args:
        last_etl_date: Timestamp cá»§a láº§n ETL gáº§n nháº¥t, máº·c Ä‘á»‹nh lÃ  7 ngÃ y trÆ°á»›c
        
    Returns:
        Dict[str, Any]: Káº¿t quáº£ thá»‘ng kÃª ETL
    """
    return run_staging_to_dwh_etl(last_etl_date)
