#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ETL module cho viá»‡c chuyá»ƒn dá»¯ liá»‡u tá»« Staging sang Data Warehouse (DuckDB)
Fixed version vá»›i logic parsing location má»›i
"""
# Standard library imports
import json
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any

# Third-party imports
import pandas as pd
import duckdb
import pyarrow as pa
import pyarrow.parquet as pq

# Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n vÃ  logging
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))

# Äáº£m báº£o thÆ° má»¥c logs tá»“n táº¡i
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

# Thiáº¿t láº­p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "etl.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Local imports
from src.utils.db import get_connection, get_dataframe, execute_query
from src.utils.config import DUCKDB_PATH, DWH_STAGING_SCHEMA, STAGING_JOBS_TABLE
from src.processing.data_prepare import (
    prepare_dim_job, prepare_dim_company, prepare_dim_location, 
    generate_date_range, parse_single_location_item, parse_job_location,
    check_dimension_changes, apply_scd_type2_updates, 
    generate_daily_fact_records, calculate_load_month
)

# ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c SQL
SQL_DIR = os.path.join(PROJECT_ROOT, "sql")
if not os.path.exists(SQL_DIR):
    SQL_DIR = os.path.join(os.getcwd(), "sql")
    os.makedirs(SQL_DIR, exist_ok=True)

def get_duckdb_connection(duckdb_path: str = DUCKDB_PATH) -> duckdb.DuckDBPyConnection:
    """ Káº¿t ná»‘i Ä‘áº¿n DuckDB """
    # Äáº£m báº£o Ä‘Æ°á»ng dáº«n lÃ  tuyá»‡t Ä‘á»‘i
    if not os.path.isabs(duckdb_path):
        duckdb_path = os.path.join(PROJECT_ROOT, duckdb_path)
    
    # Äáº£m báº£o thÆ° má»¥c cha tá»“n táº¡i
    parent_dir = os.path.dirname(duckdb_path)
    os.makedirs(parent_dir, exist_ok=True)
    
    logger.info(f"Káº¿t ná»‘i DuckDB táº¡i: {duckdb_path}")
    
    return duckdb.connect(duckdb_path)

def execute_sql_file_duckdb(sql_file_path, conn=None):
    """Thá»±c thi file SQL trÃªn DuckDB vá»›i connection truyá»n vÃ o (hoáº·c tá»± táº¡o)"""
    try:
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_script = f.read()
        # Náº¿u khÃ´ng truyá»n conn thÃ¬ tá»± táº¡o vÃ  tá»± Ä‘Ã³ng, cÃ²n truyá»n vÃ o thÃ¬ khÃ´ng Ä‘Ã³ng
        close_conn = False
        if conn is None:
            conn = get_duckdb_connection()
            close_conn = True
        conn.execute(sql_script)
        if close_conn:
            conn.close()
        logger.info(f"ÄÃ£ thá»±c thi file SQL: {sql_file_path}")
        return True
    except Exception as e:
        logger.error(f"Lá»—i khi thá»±c thi file SQL {sql_file_path}: {str(e)}")
        return False

def setup_duckdb_schema():
    """Thiáº¿t láº­p schema vÃ  báº£ng cho DuckDB"""
    try:
        with get_duckdb_connection() as conn:
            # Táº¡o schema náº¿u chÆ°a cÃ³ (DuckDB sáº½ khÃ´ng bÃ¡o lá»—i náº¿u Ä‘Ã£ tá»“n táº¡i)
            conn.execute(f"CREATE SCHEMA IF NOT EXISTS {DWH_STAGING_SCHEMA};")
            logger.info(f"ÄÃ£ Ä‘áº£m báº£o tá»“n táº¡i schema: {DWH_STAGING_SCHEMA}")

            # Thá»±c thi file schema_dwh.sql Ä‘á»ƒ táº¡o báº£ng vÃ  chá»‰ má»¥c
            schema_dwh = os.path.join(SQL_DIR, "schema_dwh.sql")
            if os.path.exists(schema_dwh):
                if not execute_sql_file_duckdb(schema_dwh, conn=conn):
                    logger.error("KhÃ´ng thá»ƒ thiáº¿t láº­p schema vÃ  báº£ng!")
                    return False
            else:
                logger.error(f"KhÃ´ng tÃ¬m tháº¥y file schema: {schema_dwh}")
                return False

            # Kiá»ƒm tra xem cÃ¡c báº£ng Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng chÆ°a
            tables_in_db = [row[0] for row in conn.execute("SHOW TABLES").fetchall()]
            for table in ['DimJob', 'DimCompany', 'DimLocation', 'DimDate', 'FactJobPostingDaily', 'FactJobLocationBridge']:
                if table in tables_in_db:
                    logger.info(f"âœ“ Báº£ng {table} Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng")
                else:
                    logger.warning(f"âœ— Báº£ng {table} KHÃ”NG Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng")

        logger.info("ÄÃ£ thiáº¿t láº­p schema vÃ  báº£ng database thÃ nh cÃ´ng cho DuckDB!")
        return True
    except Exception as e:
        logger.error(f"Lá»—i khi thiáº¿t láº­p schema database vá»›i DuckDB: {str(e)}")
        return False
    

def get_staging_batch(last_etl_date: datetime) -> pd.DataFrame:
    """
    Láº¥y batch dá»¯ liá»‡u tá»« staging jobs ká»ƒ tá»« láº§n ETL gáº§n nháº¥t
    
    Args:
        last_etl_date: Timestamp cá»§a láº§n ETL gáº§n nháº¥t
        
    Returns:
        DataFrame chá»©a báº£n ghi cáº§n xá»­ lÃ½
    """
    try:
        logger.info(f"Truy váº¥n dá»¯ liá»‡u tá»« báº£ng {STAGING_JOBS_TABLE}")
        
        query = f"""
            SELECT *
            FROM {STAGING_JOBS_TABLE}
            WHERE crawled_at >= %s
            OR
            (crawled_at IS NOT NULL AND %s IS NULL)
        """
        
        # Kiá»ƒm tra xem báº£ng cÃ³ tá»“n táº¡i khÃ´ng
        table_exists_query = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{DWH_STAGING_SCHEMA}' 
                AND table_name = 'staging_jobs'
            );
        """
        
        # Thá»±c hiá»‡n kiá»ƒm tra
        exists = execute_query(table_exists_query, fetch=True)
        
        if not exists or not exists[0].get('exists', False):
            logger.error(f"Báº£ng {STAGING_JOBS_TABLE} khÃ´ng tá»“n táº¡i!")
            # Tráº£ vá» DataFrame rá»—ng náº¿u báº£ng khÃ´ng tá»“n táº¡i
            return pd.DataFrame()
        
        df = get_dataframe(query, params=(last_etl_date, last_etl_date))
        logger.info(f"ÄÃ£ láº¥y {len(df)} báº£n ghi tá»« staging (tá»« {last_etl_date})")
        
        # REMOVED: Debug column logging - not needed in production
        
        return df
    except Exception as e:
        logger.error(f"Lá»—i khi láº¥y dá»¯ liá»‡u tá»« staging: {e}", exc_info=True)
        # Tráº£ vá» DataFrame rá»—ng trong trÆ°á»ng há»£p lá»—i
        return pd.DataFrame()
    
def lookup_location_key(
    duck_conn: duckdb.DuckDBPyConnection,
    province: str = None,
    city: str = None,
    district: str = None
) -> Optional[int]:
    """
    TÃ¬m location_sk tá»« báº£ng DimLocation dá»±a trÃªn province, city, district
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        province: TÃªn tá»‰nh
        city: TÃªn thÃ nh phá»‘
        district: TÃªn quáº­n/huyá»‡n
    
    Returns:
        location_sk hoáº·c None náº¿u khÃ´ng tÃ¬m tháº¥y
    """
    try:
        # XÃ¢y dá»±ng query Ä‘á»™ng dá»±a trÃªn cÃ¡c tham sá»‘ cÃ³ giÃ¡ trá»‹
        conditions = ["is_current = TRUE"]
        params = []
        
        if province is not None:
            conditions.append("province = ?")
            params.append(province)
        else:
            conditions.append("province IS NULL")
            
        if city is not None:
            conditions.append("city = ?")
            params.append(city)
        else:
            conditions.append("city IS NULL")
            
        if district is not None:
            conditions.append("district = ?")
            params.append(district)
        else:
            conditions.append("district IS NULL")
        
        query = f"""
            SELECT location_sk
            FROM DimLocation
            WHERE {' AND '.join(conditions)}
            LIMIT 1
        """
        
        result = duck_conn.execute(query, params).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"Lá»—i khi tÃ¬m location_sk: {e}")
        return None

def lookup_dimension_key(
    duck_conn: duckdb.DuckDBPyConnection,
    dim_table: str,
    key_column: str,
    key_value: Any,
    surrogate_key_col: str
) -> Optional[int]:
    """
    TÃ¬m surrogate key tá»« báº£ng dimension
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        dim_table: TÃªn báº£ng dimension
        key_column: TÃªn cá»™t dÃ¹ng Ä‘á»ƒ tÃ¬m kiáº¿m
        key_value: GiÃ¡ trá»‹ cáº§n tÃ¬m
        surrogate_key_col: TÃªn cá»™t surrogate key
    
    Returns:
        Surrogate key hoáº·c None náº¿u khÃ´ng tÃ¬m tháº¥y
    """
    try:
        query = f"""
            SELECT {surrogate_key_col}
            FROM {dim_table}
            WHERE {key_column} = ?
            AND is_current = TRUE
            LIMIT 1
        """
        
        result = duck_conn.execute(query, [key_value]).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"Lá»—i khi tÃ¬m khÃ³a trong {dim_table}: {e}")
        return None

def batch_insert_records(duck_conn: duckdb.DuckDBPyConnection, table_name: str, records: List[Dict], batch_size: int = 1000, upsert_on_conflict: str = None):
    """
    Batch insert records Ä‘á»ƒ tá»‘i Æ°u performance
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        table_name: TÃªn báº£ng
        records: List cÃ¡c records cáº§n insert
        batch_size: KÃ­ch thÆ°á»›c batch
        upsert_on_conflict: Column(s) Ä‘á»ƒ UPSERT (e.g., "(fact_id, location_sk)")
    """
    if not records:
        return 0
        
    inserted_count = 0
    
    # Chia records thÃ nh cÃ¡c batch
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        
        try:
            # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho FactJobLocationBridge vá»›i duplicate check
            if table_name == 'FactJobLocationBridge':
                for record in batch:
                    fact_id = record['fact_id']
                    location_sk = record['location_sk']
                    
                    # Kiá»ƒm tra xem bridge record Ä‘Ã£ tá»“n táº¡i chÆ°a
                    check_query = """
                        SELECT 1 FROM FactJobLocationBridge 
                        WHERE fact_id = ? AND location_sk = ?
                    """
                    exists = duck_conn.execute(check_query, [fact_id, location_sk]).fetchone()
                    
                    if not exists:
                        insert_query = """
                            INSERT INTO FactJobLocationBridge (fact_id, location_sk)
                            VALUES (?, ?)
                        """
                        duck_conn.execute(insert_query, [fact_id, location_sk])
                        inserted_count += 1
                        
            else:
                # Xá»­ lÃ½ thÃ´ng thÆ°á»ng cho cÃ¡c báº£ng khÃ¡c
                # Chuáº©n bá»‹ dá»¯ liá»‡u batch
                df_batch = pd.DataFrame(batch)
                
                # Handle JSON columns
                for col in df_batch.columns:
                    df_batch[col] = df_batch[col].apply(
                        lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x
                    )
                
                # Insert batch
                duck_conn.execute(f"INSERT INTO {table_name} SELECT * FROM df_batch")
                inserted_count += len(batch)
            
        except Exception as e:
            logger.warning(f"Lá»—i khi batch insert vÃ o {table_name}: {e}")
            # Fallback: insert tá»«ng record
            for record in batch:
                try:
                    columns = list(record.keys())
                    placeholders = ', '.join(['?'] * len(columns))
                    values = [record[col] for col in columns]
                    
                    # Handle JSON values
                    for j, val in enumerate(values):
                        if isinstance(val, (dict, list)):
                            values[j] = json.dumps(val)
                    
                    # Kiá»ƒm tra náº¿u cÃ³ upsert_on_conflict
                    if upsert_on_conflict and table_name == 'FactJobLocationBridge':
                        # Skip náº¿u Ä‘Ã£ tá»“n táº¡i
                        check_query = f"SELECT 1 FROM {table_name} WHERE fact_id = ? AND location_sk = ?"
                        exists = duck_conn.execute(check_query, [values[0], values[1]]).fetchone()
                        if exists:
                            continue
                    
                    query = f"""
                        INSERT INTO {table_name} ({', '.join(columns)})
                        VALUES ({placeholders})
                    """
                    duck_conn.execute(query, values)
                    inserted_count += 1
                except Exception as e2:
                    logger.error(f"Lá»—i khi insert single record vÃ o {table_name}: {e2}")
    
    logger.info(f"ÄÃ£ batch insert {inserted_count} records vÃ o {table_name}")
    return inserted_count

# REMOVED: Redundant process_dimension_with_scd2 function
# This functionality is now handled by DimensionHandler.process_dimension_with_scd2() in dimension_handler.py

# REMOVED: Redundant generate_fact_records function
# This functionality is now handled by FactHandler.generate_fact_records() in fact_handler.py


# REMOVED: Redundant cleanup_duplicate_fact_records function
# This functionality is now handled by FactHandler.cleanup_duplicate_fact_records() in fact_handler.py

def verify_etl_integrity(staging_count: int, fact_count: int, threshold: float = 0.9) -> bool:
    """
    Kiá»ƒm tra tÃ­nh toÃ n váº¹n cá»§a quÃ¡ trÃ¬nh ETL Staging to DWH
    
    Args:
        staging_count: Sá»‘ báº£n ghi staging Ä‘áº§u vÃ o
        fact_count: Sá»‘ báº£n ghi fact Ä‘Ã£ táº¡o
        threshold: NgÆ°á»¡ng cháº¥p nháº­n (% dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng)
        
    Returns:
        bool: True náº¿u tá»· lá»‡ dá»¯ liá»‡u chuyá»ƒn Ä‘á»•i Ä‘áº¡t threshold
    """
    if staging_count == 0:
        logger.warning("KhÃ´ng cÃ³ dá»¯ liá»‡u nguá»“n Ä‘á»ƒ xá»­ lÃ½")
        return True
    
    # Má»—i staging record cÃ³ thá»ƒ táº¡o ra nhiá»u fact record (má»—i ngÃ y má»™t record)
    # NÃªn kiá»ƒm tra xem cÃ³ fact records Ä‘Æ°á»£c táº¡o khÃ´ng, khÃ´ng so sÃ¡nh sá»‘ lÆ°á»£ng
    if fact_count == 0:
        logger.error("KhÃ´ng cÃ³ fact record nÃ o Ä‘Æ°á»£c táº¡o tá»« staging data!")
        return False
    
    logger.info(f"ÄÃ£ táº¡o {fact_count} fact records tá»« {staging_count} staging records")
    return True

def run_staging_to_dwh_etl(last_etl_date: Optional[datetime] = None) -> Dict[str, Any]:
    """
    Thá»±c hiá»‡n quy trÃ¬nh ETL chuyá»ƒn dá»¯ liá»‡u tá»« Staging sang Data Warehouse
    
    Args:
        last_etl_date: Timestamp cá»§a láº§n ETL gáº§n nháº¥t, máº·c Ä‘á»‹nh lÃ  7 ngÃ y trÆ°á»›c
        
    Returns:
        Dict[str, Any]: Káº¿t quáº£ thá»‘ng kÃª ETL
    """
    start_time = datetime.now()
    
    try:
        # Thiáº¿t láº­p ngÃ y ETL gáº§n nháº¥t náº¿u khÃ´ng cÃ³
        if last_etl_date is None:
            last_etl_date = datetime.now() - timedelta(days=7)
        
        logger.info("="*60)
        logger.info(f"ğŸš€ Báº®T Äáº¦U ETL STAGING TO DWH - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ğŸ•’ Láº¥y dá»¯ liá»‡u tá»«: {last_etl_date}")
        logger.info("="*60)
        
        # 1. Láº¥y dá»¯ liá»‡u tá»« staging
        staging_batch = get_staging_batch(last_etl_date)
        if staging_batch.empty:
            logger.info("KhÃ´ng cÃ³ báº£n ghi nÃ o Ä‘á»ƒ xá»­ lÃ½ tá»« staging")
            return {
                "success": True,
                "message": "KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½",
                "source_count": 0,
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
            
        logger.info(f"ÄÃ£ láº¥y {len(staging_batch)} báº£n ghi tá»« staging")
        
        # 2. Kiá»ƒm tra file DuckDB
        if os.path.exists(DUCKDB_PATH):
            logger.info(f"ğŸ“ Sá»­ dá»¥ng DuckDB hiá»‡n cÃ³: {DUCKDB_PATH}")
        else:
            logger.info(f"ğŸ†• Táº¡o DuckDB má»›i: {DUCKDB_PATH}")
        
        # 3. Thiáº¿t láº­p schema vÃ  báº£ng (giá»¯ nguyÃªn dá»¯ liá»‡u cÅ©)
        if not setup_duckdb_schema():
            return {
                "success": False,
                "message": "KhÃ´ng thá»ƒ thiáº¿t láº­p schema DuckDB",
                "source_count": len(staging_batch),
                "fact_count": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
        
        # 4. Káº¿t ná»‘i DuckDB vÃ  thá»±c hiá»‡n ETL vá»›i SCD Type 2
        with get_duckdb_connection(DUCKDB_PATH) as duck_conn:
            # Dá»n dáº¹p duplicate records hiá»‡n cÃ³ (cháº¡y 1 láº§n)
            logger.info("ğŸ§¹ Dá»n dáº¹p duplicate records hiá»‡n cÃ³...")
            from src.etl.fact_handler import FactHandler
            fact_handler = FactHandler(duck_conn)
            cleanup_result = fact_handler.cleanup_duplicate_fact_records()
            logger.info(f"âœ… Cleanup result: {cleanup_result}")
            
            # 5. Xá»­ lÃ½ vÃ  insert dá»¯ liá»‡u vá»›i SCD Type 2
            from src.etl.dimension_handler import DimensionHandler
            dim_handler = DimensionHandler(duck_conn)
            dim_stats = {}

            # 5.1 DimJob vá»›i SCD Type 2
            dim_stats['DimJob'] = dim_handler.process_dimension_with_scd2(
                staging_batch, 'DimJob', prepare_dim_job,
                'job_id', 'job_sk', ['title_clean', 'skills', 'job_url']
            )

            # 5.2. DimCompany vá»›i SCD Type 2
            dim_stats['DimCompany'] = dim_handler.process_dimension_with_scd2(
                staging_batch, 'DimCompany', prepare_dim_company,
                'company_name_standardized', 'company_sk', ['company_url', 'verified_employer']
            )
        
            # 5.3. DimLocation - xá»­ lÃ½ Ä‘áº·c biá»‡t vÃ¬ composite key
            logger.info("Xá»­ lÃ½ DimLocation vá»›i composite key")
            dim_location_df = prepare_dim_location(staging_batch)
            if not dim_location_df.empty:
                location_records = []
                for _, location in dim_location_df.iterrows():
                    location_dict = location.to_dict()
                    if 'location_sk' in location_dict:
                        del location_dict['location_sk']
                    location_records.append(location_dict)
                
                dim_stats['DimLocation'] = {
                    'inserted': batch_insert_records(duck_conn, 'DimLocation', location_records),
                    'updated': 0,
                    'unchanged': 0
                }
            else:
                dim_stats['DimLocation'] = {'inserted': 0, 'updated': 0, 'unchanged': 0}
        
            # 5.4. Äáº£m báº£o báº£ng DimDate cÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c ngÃ y cáº§n thiáº¿t
            logger.info("Xá»­ lÃ½ DimDate")
            start_date = (datetime.now() - timedelta(days=60)).date()
            end_date = (datetime.now() + timedelta(days=240)).date()
            
            date_df = generate_date_range(start_date, end_date)
            
            # Filter out existing dates
            new_date_records = []
            for _, date_record in date_df.iterrows():
                date_dict = date_record.to_dict()
                exists = duck_conn.execute(f"SELECT 1 FROM DimDate WHERE date_id = ?", [date_dict['date_id']]).fetchone()
                if not exists:
                    new_date_records.append(date_dict)
            
            dim_stats['DimDate'] = {
                'inserted': batch_insert_records(duck_conn, 'DimDate', new_date_records),
                'updated': 0,
                'unchanged': len(date_df) - len(new_date_records)
            }
        
            # 5.5. Insert dá»¯ liá»‡u vÃ o FactJobPostingDaily vÃ  FactJobLocationBridge
            logger.info("Xá»­ lÃ½ FactJobPostingDaily vÃ  FactJobLocationBridge")
            fact_records, bridge_records = fact_handler.generate_fact_records(staging_batch)
            
            # Kiá»ƒm tra tÃ­nh toÃ n váº¹n cá»§a dá»¯ liá»‡u
            if not verify_etl_integrity(len(staging_batch), len(fact_records)):
                logger.warning("âš ï¸ PhÃ¡t hiá»‡n váº¥n Ä‘á» vá» tÃ­nh toÃ n váº¹n dá»¯ liá»‡u trong quÃ¡ trÃ¬nh ETL!")
                # Váº«n tiáº¿p tá»¥c nhÆ°ng Ä‘Ã£ cáº£nh bÃ¡o
            
            logger.info(f"ÄÃ£ insert {len(fact_records)} báº£n ghi vÃ o FactJobPostingDaily")
            logger.info(f"Chuáº©n bá»‹ insert {len(bridge_records)} báº£n ghi vÃ o FactJobLocationBridge")
        
            # Batch insert bridge records vÃ o FactJobLocationBridge
            bridge_inserted = batch_insert_records(duck_conn, 'FactJobLocationBridge', bridge_records)
            logger.info(f"ÄÃ£ batch insert {bridge_inserted} báº£n ghi vÃ o FactJobLocationBridge")
        
            # 6. Tá»•ng káº¿t ETL
            logger.info("="*60)
            logger.info("ğŸ“Š Tá»”NG Káº¾T ETL STAGING TO DWH")
            logger.info("="*60)
            
            total_inserted = sum(stats.get('inserted', 0) for stats in dim_stats.values())
            total_updated = sum(stats.get('updated', 0) for stats in dim_stats.values())
            total_unchanged = sum(stats.get('unchanged', 0) for stats in dim_stats.values())
            
            for table, stats in dim_stats.items():
                logger.info(f"{table:15} - Insert: {stats['inserted']:5}, Update: {stats['updated']:5}, Unchanged: {stats['unchanged']:5}")
            
            logger.info(f"{'FACTS':15} - FactJobPostingDaily: {len(fact_records)} records")
            logger.info(f"{'BRIDGE':15} - FactJobLocationBridge: {bridge_inserted} records")
            logger.info("-"*60)
            logger.info(f"Tá»”NG DIM        - Insert: {total_inserted:5}, Update: {total_updated:5}, Unchanged: {total_unchanged:5}")
            logger.info(f"Tá»”NG FACT/BRIDGE- Records: {len(fact_records) + bridge_inserted}")
            
            # Log load_month stats
            load_months = set()
            if fact_records:
                load_months = set(record.get('load_month') for record in fact_records if record.get('load_month'))
                logger.info(f"Partition load_month: {', '.join(sorted(load_months))}")
            
            # 7. Validation vÃ  Data Quality Check
            logger.info("ğŸ” Báº¯t Ä‘áº§u validation ETL...")
            validation_success = True
            validation_message = ""
            
            try:
                from src.utils.etl_validator import generate_etl_report, log_validation_results
                validation_results = generate_etl_report(duck_conn)
                log_validation_results(validation_results)
                
                # Kiá»ƒm tra cÃ¡c váº¥n Ä‘á» nghiÃªm trá»ng
                if validation_results.get('issues', {}).get('critical', 0) > 0:
                    validation_success = False
                    validation_message = f"PhÃ¡t hiá»‡n {validation_results['issues']['critical']} váº¥n Ä‘á» nghiÃªm trá»ng trong validation"
                    logger.error(validation_message)
            except ImportError:
                logger.warning("KhÃ´ng thá»ƒ import etl_validator - bá» qua validation")
            except Exception as e:
                validation_success = False
                validation_message = f"Lá»—i khi thá»±c hiá»‡n validation: {str(e)}"
                logger.error(validation_message)
            
            # 8. Export dá»¯ liá»‡u ra Parquet theo load_month
            logger.info("ğŸ“¦ Báº¯t Ä‘áº§u export dá»¯ liá»‡u ra Parquet...")
            export_success = False
            export_message = ""
            export_stats = {}
            
            try:
                # Chuyá»ƒn tá»« set sang list Ä‘á»ƒ export
                load_months_list = list(load_months) if load_months else None
                
                # Chá»‰ export náº¿u cÃ³ load_months má»›i
                if load_months_list:
                    export_results = export_to_parquet(duck_conn, load_months_list)
                    export_success = export_results.get('success', False)
                    export_stats = export_results
                    
                    if export_success:
                        export_message = f"ÄÃ£ export dá»¯ liá»‡u cho {len(load_months_list)} load_month"
                        logger.info(f"âœ… {export_message}")
                    else:
                        export_message = f"CÃ³ lá»—i khi export dá»¯ liá»‡u: {export_results.get('message', 'Unknown error')}"
                        logger.warning(f"âš ï¸ {export_message}")
                else:
                    export_message = "KhÃ´ng cÃ³ load_month nÃ o Ä‘á»ƒ export"
                    logger.info(export_message)
            except Exception as e:
                export_success = False
                export_message = f"Lá»—i khi export dá»¯ liá»‡u ra Parquet: {str(e)}"
                logger.error(export_message, exc_info=True)
        
        # TÃ­nh thá»i gian cháº¡y
        duration = (datetime.now() - start_time).total_seconds()
        
        # Tá»•ng káº¿t
        logger.info("="*60)
        logger.info(f"âœ… ETL HOÃ€N THÃ€NH TRONG {duration:.2f} GIÃ‚Y!")
        logger.info("="*60)
        
        # Thá»‘ng kÃª káº¿t quáº£ ETL
        etl_stats = {
            "success": True,
            "source_count": len(staging_batch),
            "fact_count": len(fact_records),
            "bridge_count": bridge_inserted,
            "dim_stats": dim_stats,
            "total_dim_inserted": total_inserted,
            "total_dim_updated": total_updated,
            "load_months": list(load_months),
            "duration_seconds": duration,
            "validation_success": validation_success,
            "validation_message": validation_message,
            "export_success": export_success,
            "export_message": export_message,
            "export_stats": export_stats
        }
        
        return etl_stats
    
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        error_msg = f"Lá»—i trong quÃ¡ trÃ¬nh ETL: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        return {
            "success": False,
            "message": error_msg,
            "duration_seconds": duration
        }

if __name__ == "__main__":
    # Cháº¡y ETL vá»›i dá»¯ liá»‡u tá»« 7 ngÃ y trÆ°á»›c
    etl_result = run_staging_to_dwh_etl()
    
    # Kiá»ƒm tra káº¿t quáº£
    if etl_result.get("success", False):
        logger.info("âœ… ETL HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
        sys.exit(0)
    else:
        logger.error(f"âŒ ETL THáº¤T Báº I: {etl_result.get('message', 'Unknown error')}")
        sys.exit(1)

def export_to_parquet(duck_conn: duckdb.DuckDBPyConnection, load_months: List[str] = None) -> Dict[str, Any]:
    """
    Export dá»¯ liá»‡u tá»« DWH ra file Parquet theo load_month
    
    Args:
        duck_conn: Káº¿t ná»‘i DuckDB
        load_months: List cÃ¡c load_month cáº§n export, náº¿u None thÃ¬ export táº¥t cáº£
        
    Returns:
        ThÃ´ng tin vá» quÃ¡ trÃ¬nh export
    """
    try:
        # Táº¡o thÆ° má»¥c export náº¿u chÆ°a cÃ³
        export_dir = os.path.join(PROJECT_ROOT, "export", "dwh")
        os.makedirs(export_dir, exist_ok=True)
        
        # Náº¿u khÃ´ng chá»‰ Ä‘á»‹nh load_months, láº¥y táº¥t cáº£ load_months tá»« fact table
        if not load_months:
            query = "SELECT DISTINCT load_month FROM FactJobPostingDaily ORDER BY load_month"
            load_months_result = duck_conn.execute(query).fetchall()
            load_months = [row[0] for row in load_months_result]
        
        if not load_months:
            logger.warning("KhÃ´ng cÃ³ load_month nÃ o Ä‘á»ƒ export!")
            return {"success": False, "message": "No load_months found"}
        
        # Thá»‘ng kÃª
        stats = {
            "load_months": load_months,
            "exports": {},
            "success": True,
            "timestamp": datetime.now().isoformat()
        }
        
        # Chuáº©n bá»‹ queries
        queries = {
            "facts": """
                SELECT f.*, j.title_clean, j.job_id, c.company_name_standardized
                FROM FactJobPostingDaily f
                JOIN DimJob j ON f.job_sk = j.job_sk
                JOIN DimCompany c ON f.company_sk = c.company_sk
                WHERE f.load_month = '{}'
            """,
            "locations": """
                SELECT f.fact_id, f.job_sk, f.date_id, j.job_id, j.title_clean,
                       l.province, l.city, l.district
                FROM FactJobPostingDaily f
                JOIN DimJob j ON f.job_sk = j.job_sk
                JOIN FactJobLocationBridge b ON f.fact_id = b.fact_id
                JOIN DimLocation l ON b.location_sk = l.location_sk
                WHERE f.load_month = '{}'
            """,
            "analytics": """
                SELECT j.title_clean, j.job_id, c.company_name_standardized, 
                       f.date_id, f.salary_min, f.salary_max, f.salary_type,
                       f.due_date, f.posted_time, f.verified_employer
                FROM FactJobPostingDaily f
                JOIN DimJob j ON f.job_sk = j.job_sk
                JOIN DimCompany c ON f.company_sk = c.company_sk
                WHERE f.load_month = '{}'
            """
        }
        
        # Export dá»¯ liá»‡u cho má»—i load_month
        total_records = 0
        for load_month in load_months:
            logger.info(f"Báº¯t Ä‘áº§u export dá»¯ liá»‡u cho load_month: {load_month}")
            
            # Táº¡o thÆ° má»¥c cho load_month
            month_dir = os.path.join(export_dir, load_month)
            os.makedirs(month_dir, exist_ok=True)
            
            try:
                export_files = {}
                record_counts = {}
                month_total = 0
                
                # Export tá»«ng loáº¡i dá»¯ liá»‡u
                for export_type, query_template in queries.items():
                    query = query_template.format(load_month)
                    df = duck_conn.execute(query).fetchdf()
                    
                    if not df.empty:
                        file_name = f"job_{export_type}_{load_month}.parquet"
                        file_path = os.path.join(month_dir, file_name)
                        df.to_parquet(file_path, index=False)
                        
                        export_files[export_type] = file_name
                        record_counts[export_type] = len(df)
                        stats["exports"][f"{export_type}_{load_month}"] = len(df)
                        month_total += len(df)
                    else:
                        export_files[export_type] = None
                        record_counts[export_type] = 0
                        stats["exports"][f"{export_type}_{load_month}"] = 0
                
                # Export metadata file
                meta_data = {
                    "load_month": load_month,
                    "export_time": datetime.now().isoformat(),
                    "record_counts": record_counts,
                    "files": [f for f in export_files.values() if f]
                }
                
                meta_file = os.path.join(month_dir, f"metadata_{load_month}.json")
                with open(meta_file, 'w', encoding='utf-8') as f:
                    json.dump(meta_data, f, indent=2)
                
                # Log tá»•ng há»£p
                logger.info(f"âœ… ÄÃ£ export {month_total} records cho load_month {load_month}")
                total_records += month_total
                
            except Exception as e:
                error_msg = f"Lá»—i khi export dá»¯ liá»‡u cho load_month {load_month}: {str(e)}"
                logger.error(error_msg, exc_info=True)
                stats["exports"][f"error_{load_month}"] = error_msg
                stats["success"] = False
        
        # Táº¡o file index cho táº¥t cáº£ load_months
        try:
            index_data = {
                "load_months": load_months,
                "export_time": datetime.now().isoformat(),
                "export_count": len(load_months),
                "total_records": total_records
            }
            
            index_file = os.path.join(export_dir, "index.json")
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, indent=2)
        except Exception as e:
            logger.error(f"Lá»—i khi táº¡o index file: {str(e)}")
        
        # Log tá»•ng káº¿t
        logger.info(f"âœ… HoÃ n thÃ nh export {total_records} records cho {len(load_months)} load_months")
        stats["total_records"] = total_records
        
        return stats
    
    except Exception as e:
        logger.error(f"Lá»—i khi export dá»¯ liá»‡u ra Parquet: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": str(e)
        }

def run_etl(last_etl_date: Optional[datetime] = None) -> Dict[str, Any]:
    """
    Alias cá»§a run_staging_to_dwh_etl Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n giá»¯a cÃ¡c module
    
    Args:
        last_etl_date: Timestamp cá»§a láº§n ETL gáº§n nháº¥t, máº·c Ä‘á»‹nh lÃ  7 ngÃ y trÆ°á»›c
        
    Returns:
        Dict[str, Any]: Káº¿t quáº£ thá»‘ng kÃª ETL
    """
    return run_staging_to_dwh_etl(last_etl_date)
