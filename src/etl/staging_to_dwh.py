#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ETL module cho vi·ªác chuy·ªÉn d·ªØ li·ªáu t·ª´ Staging sang Data Warehouse (DuckDB)
Fixed version v·ªõi logic parsing location m·ªõi
"""
import pandas as pd
import logging
import json
import os
import sys
import duckdb
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any

# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n v√† logging
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
sys.path.insert(0, PROJECT_ROOT)

# ƒê·∫£m b·∫£o th∆∞ m·ª•c logs t·ªìn t·∫°i
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "etl.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

from src.utils.logger import get_logger
from src.utils.db import get_connection, get_dataframe, execute_query
from src.utils.config import DUCKDB_PATH, DWH_STAGING_SCHEMA, STAGING_JOBS_TABLE

try:
    from src.processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range, parse_single_location_item, parse_job_location,
        check_dimension_changes, apply_scd_type2_updates, 
        generate_daily_fact_records, calculate_load_month
    )
except ImportError:
    from processing.data_prepare import (
        prepare_dim_job, prepare_dim_company, prepare_dim_location, 
        generate_date_range, parse_single_location_item, parse_job_location,
        check_dimension_changes, apply_scd_type2_updates, 
        generate_daily_fact_records, calculate_load_month
    )

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c SQL
SQL_DIR = os.path.join(PROJECT_ROOT, "sql")
if not os.path.exists(SQL_DIR):
    SQL_DIR = os.path.join(os.getcwd(), "sql")
    os.makedirs(SQL_DIR, exist_ok=True)

def get_duckdb_connection(duckdb_path: str = DUCKDB_PATH) -> duckdb.DuckDBPyConnection:
    """ K·∫øt n·ªëi ƒë·∫øn DuckDB """
    # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n l√† tuy·ªát ƒë·ªëi
    if not os.path.isabs(duckdb_path):
        duckdb_path = os.path.join(PROJECT_ROOT, duckdb_path)
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c cha t·ªìn t·∫°i
    parent_dir = os.path.dirname(duckdb_path)
    os.makedirs(parent_dir, exist_ok=True)
    
    logger.info(f"K·∫øt n·ªëi DuckDB t·∫°i: {duckdb_path}")
    
    return duckdb.connect(duckdb_path)

def execute_sql_file_duckdb(sql_file_path, conn=None):
    """Th·ª±c thi file SQL tr√™n DuckDB v·ªõi connection truy·ªÅn v√†o (ho·∫∑c t·ª± t·∫°o)"""
    try:
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_script = f.read()
        # N·∫øu kh√¥ng truy·ªÅn conn th√¨ t·ª± t·∫°o v√† t·ª± ƒë√≥ng, c√≤n truy·ªÅn v√†o th√¨ kh√¥ng ƒë√≥ng
        close_conn = False
        if conn is None:
            conn = get_duckdb_connection()
            close_conn = True
        conn.execute(sql_script)
        if close_conn:
            conn.close()
        logger.info(f"ƒê√£ th·ª±c thi file SQL: {sql_file_path}")
        return True
    except Exception as e:
        logger.error(f"L·ªói khi th·ª±c thi file SQL {sql_file_path}: {str(e)}")
        return False

def setup_duckdb_schema():
    """Thi·∫øt l·∫≠p schema v√† b·∫£ng cho DuckDB"""
    try:
        with get_duckdb_connection() as conn:
            # T·∫°o schema n·∫øu ch∆∞a c√≥ (DuckDB s·∫Ω kh√¥ng b√°o l·ªói n·∫øu ƒë√£ t·ªìn t·∫°i)
            conn.execute(f"CREATE SCHEMA IF NOT EXISTS {DWH_STAGING_SCHEMA};")
            logger.info(f"ƒê√£ ƒë·∫£m b·∫£o t·ªìn t·∫°i schema: {DWH_STAGING_SCHEMA}")

            # Th·ª±c thi file schema_dwh.sql ƒë·ªÉ t·∫°o b·∫£ng v√† ch·ªâ m·ª•c
            schema_dwh = os.path.join(SQL_DIR, "schema_dwh.sql")
            if os.path.exists(schema_dwh):
                if not execute_sql_file_duckdb(schema_dwh, conn=conn):
                    logger.error("Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema v√† b·∫£ng!")
                    return False
            else:
                logger.error(f"Kh√¥ng t√¨m th·∫•y file schema: {schema_dwh}")
                return False

            # Ki·ªÉm tra xem c√°c b·∫£ng ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng ch∆∞a
            tables_in_db = [row[0] for row in conn.execute("SHOW TABLES").fetchall()]
            for table in ['DimJob', 'DimCompany', 'DimLocation', 'DimDate', 'FactJobPostingDaily', 'FactJobLocationBridge']:
                if table in tables_in_db:
                    logger.info(f"‚úì B·∫£ng {table} ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")
                else:
                    logger.warning(f"‚úó B·∫£ng {table} KH√îNG ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")

        logger.info("ƒê√£ thi·∫øt l·∫≠p schema v√† b·∫£ng database th√†nh c√¥ng cho DuckDB!")
        return True
    except Exception as e:
        logger.error(f"L·ªói khi thi·∫øt l·∫≠p schema database v·ªõi DuckDB: {str(e)}")
        return False
    

def get_staging_batch(last_etl_date: datetime) -> pd.DataFrame:
    """
    L·∫•y batch d·ªØ li·ªáu t·ª´ staging jobs k·ªÉ t·ª´ l·∫ßn ETL g·∫ßn nh·∫•t
    
    Args:
        last_etl_date: Timestamp c·ªßa l·∫ßn ETL g·∫ßn nh·∫•t
        
    Returns:
        DataFrame ch·ª©a b·∫£n ghi c·∫ßn x·ª≠ l√Ω
    """
    try:
        logger.info(f"Truy v·∫•n d·ªØ li·ªáu t·ª´ b·∫£ng {STAGING_JOBS_TABLE}")
        
        query = f"""
            SELECT *
            FROM {STAGING_JOBS_TABLE}
            WHERE crawled_at >= %s
            OR
            (crawled_at IS NOT NULL AND %s IS NULL)
        """
        
        # Ki·ªÉm tra xem b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
        table_exists_query = f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{DWH_STAGING_SCHEMA}' 
                AND table_name = 'staging_jobs'
            );
        """
        
        # Th·ª±c hi·ªán ki·ªÉm tra
        exists = execute_query(table_exists_query, fetch=True)
        
        if not exists or not exists[0].get('exists', False):
            logger.error(f"B·∫£ng {STAGING_JOBS_TABLE} kh√¥ng t·ªìn t·∫°i!")
            # Tr·∫£ v·ªÅ DataFrame r·ªóng n·∫øu b·∫£ng kh√¥ng t·ªìn t·∫°i
            return pd.DataFrame()
        
        df = get_dataframe(query, params=(last_etl_date, last_etl_date))
        logger.info(f"ƒê√£ l·∫•y {len(df)} b·∫£n ghi t·ª´ staging (t·ª´ {last_etl_date})")
        
        # Log c√°c c·ªôt ƒë·ªÉ debug
        if not df.empty:
            logger.info(f"C√°c c·ªôt c√≥ trong d·ªØ li·ªáu: {list(df.columns)}")
        
        return df
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y d·ªØ li·ªáu t·ª´ staging: {e}", exc_info=True)
        # Tr·∫£ v·ªÅ DataFrame r·ªóng trong tr∆∞·ªùng h·ª£p l·ªói
        return pd.DataFrame()
    
def lookup_location_key(
    duck_conn: duckdb.DuckDBPyConnection,
    province: str = None,
    city: str = None,
    district: str = None
) -> Optional[int]:
    """
    T√¨m location_sk t·ª´ b·∫£ng DimLocation d·ª±a tr√™n province, city, district
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        province: T√™n t·ªânh
        city: T√™n th√†nh ph·ªë
        district: T√™n qu·∫≠n/huy·ªán
    
    Returns:
        location_sk ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        # X√¢y d·ª±ng query ƒë·ªông d·ª±a tr√™n c√°c tham s·ªë c√≥ gi√° tr·ªã
        conditions = ["is_current = TRUE"]
        params = []
        
        if province is not None:
            conditions.append("province = ?")
            params.append(province)
        else:
            conditions.append("province IS NULL")
            
        if city is not None:
            conditions.append("city = ?")
            params.append(city)
        else:
            conditions.append("city IS NULL")
            
        if district is not None:
            conditions.append("district = ?")
            params.append(district)
        else:
            conditions.append("district IS NULL")
        
        query = f"""
            SELECT location_sk
            FROM DimLocation
            WHERE {' AND '.join(conditions)}
            LIMIT 1
        """
        
        result = duck_conn.execute(query, params).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"L·ªói khi t√¨m location_sk: {e}")
        return None

def lookup_dimension_key(
    duck_conn: duckdb.DuckDBPyConnection,
    dim_table: str,
    key_column: str,
    key_value: Any,
    surrogate_key_col: str
) -> Optional[int]:
    """
    T√¨m surrogate key t·ª´ b·∫£ng dimension
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        dim_table: T√™n b·∫£ng dimension
        key_column: T√™n c·ªôt d√πng ƒë·ªÉ t√¨m ki·∫øm
        key_value: Gi√° tr·ªã c·∫ßn t√¨m
        surrogate_key_col: T√™n c·ªôt surrogate key
    
    Returns:
        Surrogate key ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        query = f"""
            SELECT {surrogate_key_col}
            FROM {dim_table}
            WHERE {key_column} = ?
            AND is_current = TRUE
            LIMIT 1
        """
        
        result = duck_conn.execute(query, [key_value]).fetchone()
        if result:
            return result[0]
        return None
    except Exception as e:
        logger.error(f"L·ªói khi t√¨m kh√≥a trong {dim_table}: {e}")
        return None

def batch_insert_records(duck_conn: duckdb.DuckDBPyConnection, table_name: str, records: List[Dict], batch_size: int = 1000):
    """
    Batch insert records ƒë·ªÉ t·ªëi ∆∞u performance
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        table_name: T√™n b·∫£ng
        records: List c√°c records c·∫ßn insert
        batch_size: K√≠ch th∆∞·ªõc batch
    """
    if not records:
        return 0
        
    inserted_count = 0
    
    # Chia records th√†nh c√°c batch
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        
        try:
            # Chu·∫©n b·ªã d·ªØ li·ªáu batch
            df_batch = pd.DataFrame(batch)
            
            # Handle JSON columns
            for col in df_batch.columns:
                df_batch[col] = df_batch[col].apply(
                    lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x
                )
            
            # Insert batch
            duck_conn.execute(f"INSERT INTO {table_name} SELECT * FROM df_batch")
            inserted_count += len(batch)
            
        except Exception as e:
            logger.warning(f"L·ªói khi batch insert v√†o {table_name}: {e}")
            # Fallback: insert t·ª´ng record
            for record in batch:
                try:
                    columns = list(record.keys())
                    placeholders = ', '.join(['?'] * len(columns))
                    values = [record[col] for col in columns]
                    
                    # Handle JSON values
                    for j, val in enumerate(values):
                        if isinstance(val, (dict, list)):
                            values[j] = json.dumps(val)
                    
                    query = f"""
                        INSERT INTO {table_name} ({', '.join(columns)})
                        VALUES ({placeholders})
                    """
                    duck_conn.execute(query, values)
                    inserted_count += 1
                except Exception as e2:
                    logger.error(f"L·ªói khi insert single record v√†o {table_name}: {e2}")
    
    logger.info(f"ƒê√£ batch insert {inserted_count} records v√†o {table_name}")
    return inserted_count

def process_dimension_with_scd2(
    duck_conn: duckdb.DuckDBPyConnection,
    staging_records: pd.DataFrame,
    dim_table: str,
    prepare_function,
    natural_key: str,
    surrogate_key: str,
    compare_columns: List[str]
) -> Dict[str, int]:
    """
    X·ª≠ l√Ω dimension table v·ªõi SCD Type 2
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        staging_records: D·ªØ li·ªáu staging
        dim_table: T√™n b·∫£ng dimension
        prepare_function: Function chu·∫©n b·ªã d·ªØ li·ªáu
        natural_key: Natural key column
        surrogate_key: Surrogate key column
        compare_columns: Columns ƒë·ªÉ so s√°nh thay ƒë·ªïi
    
    Returns:
        Dict th·ªëng k√™ insert/update
    """
    logger.info(f"X·ª≠ l√Ω {dim_table} v·ªõi SCD Type 2")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    prepared_data = prepare_function(staging_records)
    
    if prepared_data.empty:
        logger.warning(f"Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω cho {dim_table}")
        return {'inserted': 0, 'updated': 0, 'unchanged': 0}
    
    # Ki·ªÉm tra thay ƒë·ªïi
    to_insert, to_update, unchanged = check_dimension_changes(
        duck_conn, prepared_data, dim_table, natural_key, compare_columns
    )
    
    stats = {
        'inserted': 0,
        'updated': 0,
        'unchanged': len(unchanged)
    }
    
    # √Åp d·ª•ng updates (SCD Type 2)
    if to_update:
        apply_scd_type2_updates(duck_conn, dim_table, surrogate_key, to_update)
        stats['updated'] = len(to_update)
    
    # Insert records m·ªõi
    if not to_insert.empty:
        insert_records = []
        for _, record in to_insert.iterrows():
            record_dict = record.to_dict()
            # Lo·∫°i b·ªè surrogate key
            if surrogate_key in record_dict:
                del record_dict[surrogate_key]
            insert_records.append(record_dict)
        
        stats['inserted'] = batch_insert_records(duck_conn, dim_table, insert_records)
    
    logger.info(f"{dim_table} - Inserted: {stats['inserted']}, Updated: {stats['updated']}, Unchanged: {stats['unchanged']}")
    return stats

def generate_fact_records(
    duck_conn: duckdb.DuckDBPyConnection,
    staging_records: pd.DataFrame
) -> Tuple[List[Dict], List[Dict]]:
    """
    T·∫°o b·∫£n ghi fact v√† bridge t·ª´ d·ªØ li·ªáu staging
    
    Args:
        duck_conn: K·∫øt n·ªëi DuckDB
        staging_records: D·ªØ li·ªáu t·ª´ staging
    
    Returns:
        Tuple ch·ª©a (fact_records, bridge_records)
    """
    fact_records = []
    bridge_records = []
    
    for _, job in staging_records.iterrows():
        # Lookup dimension keys
        job_sk = lookup_dimension_key(
            duck_conn, 'DimJob', 'job_id', job.job_id, 'job_sk'
        )
        
        company_name = job.company_name_standardized if pd.notna(job.company_name_standardized) else job.company_name
        company_sk = lookup_dimension_key(
            duck_conn, 'DimCompany', 'company_name_standardized', company_name, 'company_sk'
        )
        
        # Check if required keys exist
        if not job_sk or not company_sk:
            logger.warning(f"B·ªè qua job_id={job.job_id}: Kh√¥ng t√¨m th·∫•y dimension key (job_sk={job_sk}, company_sk={company_sk})")
            continue
        
        # X·ª≠ l√Ω ng√†y
        due_date = pd.to_datetime(job.due_date) if pd.notna(job.due_date) else None
        posted_time = pd.to_datetime(job.posted_time) if pd.notna(job.posted_time) else None
        crawled_at = pd.to_datetime(job.crawled_at) if pd.notna(job.crawled_at) else datetime.now()
        
        # T·∫°o danh s√°ch c√°c ng√†y c·∫ßn t·∫°o fact records
        daily_dates = generate_daily_fact_records(posted_time, due_date)
        
        # T√≠nh load_month
        load_month = calculate_load_month(crawled_at)
        
        # T·∫°o fact records cho t·ª´ng ng√†y
        for date_id in daily_dates:
            fact_record = {
                'job_sk': job_sk,
                'company_sk': company_sk,
                'date_id': date_id,
                'salary_min': job.salary_min if pd.notna(job.salary_min) else None,
                'salary_max': job.salary_max if pd.notna(job.salary_max) else None,
                'salary_type': job.salary_type if pd.notna(job.salary_type) else None,
                'due_date': due_date,
                'time_remaining': job.time_remaining if pd.notna(job.time_remaining) else None,
                'verified_employer': job.verified_employer if pd.notna(job.verified_employer) else False,
                'posted_time': posted_time,
                'crawled_at': crawled_at,
                'load_month': load_month
            }
        
            # Insert fact record v√† l·∫•y fact_id
            columns = ', '.join([k for k, v in fact_record.items() if v is not None])
            placeholders = ', '.join(['?'] * len([v for v in fact_record.values() if v is not None]))
            values = [v for v in fact_record.values() if v is not None]
            
            insert_query = f"""
                INSERT INTO FactJobPostingDaily ({columns})
                VALUES ({placeholders})
                RETURNING fact_id
            """
            
            try:
                result = duck_conn.execute(insert_query, values).fetchone()
                if not result:
                    logger.warning(f"Kh√¥ng th·ªÉ insert fact record cho job_id={job.job_id}, date={date_id}")
                    continue
                    
                fact_id = result[0]
                fact_records.append(fact_record)
                
                # X·ª≠ l√Ω locations v·ªõi c·∫•u tr√∫c m·ªõi (province, city, district)
                location_str = None
                
                # ∆Øu ti√™n s·ª≠ d·ª•ng location_pairs n·∫øu c√≥
                if hasattr(job, 'location_pairs'):
                    try:
                        location_pairs_value = getattr(job, 'location_pairs')
                        if location_pairs_value is not None and str(location_pairs_value).lower() not in ['nan', 'none', '']:
                            location_str = str(location_pairs_value)
                    except:
                        pass
                        
                # Fallback v·ªÅ location n·∫øu kh√¥ng c√≥ location_pairs
                if not location_str and hasattr(job, 'location'):
                    try:
                        location_value = getattr(job, 'location')
                        if location_value is not None and str(location_value).lower() not in ['nan', 'none', '']:
                            location_str = str(location_value)
                    except:
                        pass
                
                if location_str:
                    # Parse location string th√†nh c√°c tuple (province, city, district)
                    logger.info(f"Parsing location_str: {location_str}")
                    parsed_locations = parse_job_location(location_str)
                    logger.info(f"Parsed locations: {parsed_locations}")
                    
                    for province, city, district in parsed_locations:
                        location_sk = lookup_location_key(duck_conn, province, city, district)
                        
                        if location_sk:
                            bridge_records.append({'fact_id': fact_id, 'location_sk': location_sk})
                        else:
                            # N·∫øu kh√¥ng t√¨m th·∫•y exact match, th·ª≠ t√¨m Unknown
                            unknown_location_sk = lookup_location_key(duck_conn, None, 'Unknown', None)
                            if unknown_location_sk:
                                bridge_records.append({'fact_id': fact_id, 'location_sk': unknown_location_sk})
                            else:
                                logger.warning(f"Kh√¥ng t√¨m th·∫•y location_sk cho job_id={job.job_id}, location=({province}, {city}, {district})")
                else:
                    # Kh√¥ng c√≥ location, s·ª≠ d·ª•ng Unknown
                    unknown_location_sk = lookup_location_key(duck_conn, None, 'Unknown', None)
                    if unknown_location_sk:
                        bridge_records.append({'fact_id': fact_id, 'location_sk': unknown_location_sk})
                        
            except Exception as e:
                logger.error(f"L·ªói khi insert fact record cho job_id={job.job_id}, date={date_id}: {e}")
                continue
    
    return fact_records, bridge_records


if __name__ == "__main__":
    # 1. L·∫•y d·ªØ li·ªáu t·ª´ staging
    last_etl_date = datetime.now() - timedelta(days=7)
    staging_batch = get_staging_batch(last_etl_date)
    if not staging_batch.empty:
        logger.info(f"ƒê√£ l·∫•y {len(staging_batch)} b·∫£n ghi t·ª´ staging")
    else:
        logger.info("Kh√¥ng c√≥ b·∫£n ghi n√†o ƒë·ªÉ x·ª≠ l√Ω t·ª´ staging")
        sys.exit(0)  # Tho√°t s·ªõm n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu

    # 2. Kh√¥ng x√≥a file DuckDB ƒë·ªÉ gi·ªØ l·∫°i d·ªØ li·ªáu cho SCD Type 2
    # if os.path.exists(DUCKDB_PATH):
    #     os.remove(DUCKDB_PATH)
    #     logger.info(f"ƒê√£ x√≥a file DuckDB c≈©: {DUCKDB_PATH}")
    
    if os.path.exists(DUCKDB_PATH):
        logger.info(f"üìÅ S·ª≠ d·ª•ng DuckDB hi·ªán c√≥: {DUCKDB_PATH}")
    else:
        logger.info(f"üÜï T·∫°o DuckDB m·ªõi: {DUCKDB_PATH}")

    # 3. Thi·∫øt l·∫≠p schema v√† b·∫£ng (gi·ªØ nguy√™n d·ªØ li·ªáu c≈©)
    setup_duckdb_schema()

    # 4. K·∫øt n·ªëi DuckDB v√† th·ª±c hi·ªán ETL v·ªõi SCD Type 2
    duck_conn = get_duckdb_connection(DUCKDB_PATH)
    
    # 5. X·ª≠ l√Ω v√† insert d·ªØ li·ªáu v·ªõi SCD Type 2
    dim_stats = {}
    
    # 5.1 DimJob v·ªõi SCD Type 2
    dim_stats['DimJob'] = process_dimension_with_scd2(
        duck_conn, staging_batch, 'DimJob', prepare_dim_job,
        'job_id', 'job_sk', ['title_clean', 'skills', 'job_url']
    )

    # 5.2. DimCompany v·ªõi SCD Type 2
    dim_stats['DimCompany'] = process_dimension_with_scd2(
        duck_conn, staging_batch, 'DimCompany', prepare_dim_company,
        'company_name_standardized', 'company_sk', ['company_url', 'verified_employer']
    )

    # 5.3. DimLocation - x·ª≠ l√Ω ƒë·∫∑c bi·ªát v√¨ composite key
    logger.info("X·ª≠ l√Ω DimLocation v·ªõi composite key")
    dim_location_df = prepare_dim_location(staging_batch)
    if not dim_location_df.empty:
        location_records = []
        for _, location in dim_location_df.iterrows():
            location_dict = location.to_dict()
            if 'location_sk' in location_dict:
                del location_dict['location_sk']
            location_records.append(location_dict)
        
        dim_stats['DimLocation'] = {
            'inserted': batch_insert_records(duck_conn, 'DimLocation', location_records),
            'updated': 0,
            'unchanged': 0
        }
    else:
        dim_stats['DimLocation'] = {'inserted': 0, 'updated': 0, 'unchanged': 0}

    # 5.4. ƒê·∫£m b·∫£o b·∫£ng DimDate c√≥ ƒë·∫ßy ƒë·ªß c√°c ng√†y c·∫ßn thi·∫øt
    logger.info("X·ª≠ l√Ω DimDate")
    start_date = (datetime.now() - timedelta(days=60)).date()
    end_date = (datetime.now() + timedelta(days=240)).date()
    
    date_df = generate_date_range(start_date, end_date)
    
    # Filter out existing dates
    new_date_records = []
    for _, date_record in date_df.iterrows():
        date_dict = date_record.to_dict()
        exists = duck_conn.execute(f"SELECT 1 FROM DimDate WHERE date_id = ?", [date_dict['date_id']]).fetchone()
        if not exists:
            new_date_records.append(date_dict)
    
    dim_stats['DimDate'] = {
        'inserted': batch_insert_records(duck_conn, 'DimDate', new_date_records),
        'updated': 0,
        'unchanged': len(date_df) - len(new_date_records)
    }

    # 5.5. Insert d·ªØ li·ªáu v√†o FactJobPostingDaily v√† FactJobLocationBridge
    logger.info("X·ª≠ l√Ω FactJobPostingDaily v√† FactJobLocationBridge")
    fact_records, bridge_records = generate_fact_records(duck_conn, staging_batch)
    logger.info(f"ƒê√£ insert {len(fact_records)} b·∫£n ghi v√†o FactJobPostingDaily")
    logger.info(f"Chu·∫©n b·ªã insert {len(bridge_records)} b·∫£n ghi v√†o FactJobLocationBridge")

    # Batch insert bridge records v√†o FactJobLocationBridge
    bridge_inserted = batch_insert_records(duck_conn, 'FactJobLocationBridge', bridge_records)
    logger.info(f"ƒê√£ batch insert {bridge_inserted} b·∫£n ghi v√†o FactJobLocationBridge")

    # 6. T·ªïng k·∫øt ETL
    logger.info("="*60)
    logger.info("üìä T·ªîNG K·∫æT ETL STAGING TO DWH")
    logger.info("="*60)
    
    total_inserted = sum(stats.get('inserted', 0) for stats in dim_stats.values())
    total_updated = sum(stats.get('updated', 0) for stats in dim_stats.values())
    total_unchanged = sum(stats.get('unchanged', 0) for stats in dim_stats.values())
    
    for table, stats in dim_stats.items():
        logger.info(f"{table:15} - Insert: {stats['inserted']:5}, Update: {stats['updated']:5}, Unchanged: {stats['unchanged']:5}")
    
    logger.info(f"{'FACTS':15} - FactJobPostingDaily: {len(fact_records)} records")
    logger.info(f"{'BRIDGE':15} - FactJobLocationBridge: {bridge_inserted} records")
    logger.info("-"*60)
    logger.info(f"T·ªîNG DIM        - Insert: {total_inserted:5}, Update: {total_updated:5}, Unchanged: {total_unchanged:5}")
    logger.info(f"T·ªîNG FACT/BRIDGE- Records: {len(fact_records) + bridge_inserted}")
    
    # Log load_month stats
    if fact_records:
        load_months = set(record.get('load_month') for record in fact_records)
        logger.info(f"Partition load_month: {', '.join(sorted(load_months))}")
    
    logger.info("="*60)
    
    # 7. Validation v√† Data Quality Check
    logger.info("üîç B·∫Øt ƒë·∫ßu validation ETL...")
    try:
        from src.utils.etl_validator import generate_etl_report, log_validation_results
        validation_results = generate_etl_report(duck_conn)
        log_validation_results(validation_results)
    except ImportError:
        logger.warning("Kh√¥ng th·ªÉ import etl_validator - b·ªè qua validation")
    except Exception as e:
        logger.error(f"L·ªói khi th·ª±c hi·ªán validation: {e}")
    
    # 8. ƒê√≥ng k·∫øt n·ªëi
    duck_conn.close()
    logger.info("‚úÖ ETL HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
